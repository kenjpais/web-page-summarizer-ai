name: CI - Run Scraper Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    env:
      # General config
      DATA_DIR: data
      CONFIG_DIR: config
      DEBUG: "false"
      SUMMARIZE_ENABLED: "true"
      SOURCE_PAGE: https://amd64.origin.releases.ci.openshift.org/releasestream/4-scos-stable/release/4.19.0-okd-scos.0
      GITHUB_GRAPHQL_API_URL: https://api.github.com/graphql
      GITHUB_SERVER: https://github.com
      JIRA_SERVER: https://issues.redhat.com  
      LLM_MODEL: mistral
      LLM_API_URL: http://localhost:11434/api/generate
      CONFIG_FILE_PATH: config/filter.json
      SOURCES: '["JIRA", "GITHUB"]'
      
      # Secure token passed directly (not echoed, not saved in file)
      GH_API_TOKEN: ${{ secrets.GH_API_TOKEN }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install and start Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          # Kill any existing Ollama processes
          pkill -f "ollama serve" || true
          fuser -k 11434/tcp || true
          
          # Start Ollama in background
          nohup ollama serve &
          
          # Wait for Ollama to be ready
          echo "Waiting for Ollama to start..."
          for i in {1..30}; do
            if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "Ollama is ready!"
              break
            fi
            echo "Attempt $i: Ollama not ready yet, waiting..."
            sleep 2
          done
          
          # Verify Ollama is responding
          if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
            echo "Failed to start Ollama after 60 seconds"
            exit 1
          fi
          
          # Pull the model
          echo "Pulling model: ${{ env.LLM_MODEL }}"
          ollama pull ${{ env.LLM_MODEL }}
          
          # Verify model is available
          echo "Available models:"
          ollama list
          
      - name: Run tests
        run: |
          python -m unittest discover -s tests