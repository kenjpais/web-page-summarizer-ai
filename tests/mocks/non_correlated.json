[
    {
        "summary": "Document and support CNS volume migration using vCenter UI",
        "description": "Epic Goal We need to document and support CNS volume migration using native vCenter UI, so as customers can migrate volumes between datastores. Why is this important? (mandatory) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. Scenarios (mandatory) As an vCenter/Openshift admin, I want to migrate CNS volumes between datastores for existing vSphere CSI persistent volumes (PVs). This should cover attached and detached volumes. Special cases such as RWX, zonal or encrypted should also be tested to confirm is there is any limitation we should document. Dependencies (internal and external) (mandatory) This feature depends on VMware vCenter Server 7.0 Update 3o or vCenter Server 8.0 Update 2. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - STOR QE - STOR PX - Others - Acceptance Criteria (optional) This is mostly a testing / documentation epic, which will change current wording about unsupported CNS volume migration using vCenter UI. As part of this epic, we also want to remove the CLI tool we developed for from the payload. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
    },
    {
        "summary": "Upstream Beta Tracking: VolumeGroupSnapshot (TP)",
        "description": "Epic Goal Support upstream feature \"VolumeGroupSnapshot\"\" in OCP as -Beta- -GA- Beta, i.e. test it and have docs for it. Why is this important? We get this upstream feature through Kubernetes rebase. We should ensure it works well in OCP and we have docs for it. Upstream links Enhancement issue: KEP: Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) External: the feature is currently scheduled for GA in Kubernetes 1.32, i.e. OCP 4.19. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "OCP 4.19 release chores",
        "description": "Epic Goal Update all images that we ship with OpenShift to the latest upstream releases and libraries. Exact content of what needs to be updated will be determined as new images are released upstream, which is not known at the beginning of OCP development work. We don't know what new features will be included and should be tested and documented. Especially new CSI drivers releases may bring new, currently unknown features. We expect that the amount of work will be roughly the same as in the previous releases. Of course, QE or docs can reject an update if it's too close to deadline and/or looks too big. Traditionally we did these updates as bugfixes, because we did them after the feature freeze (FF). Why is this important? We want to ship the latest software that contains new features and bugfixes. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents."
    },
    {
        "summary": "Support for VolumeGroup Snapshots (TP)",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Add Volume Group Snapshots as Tech Preview. This is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. We will rely on the newly beta promoted feature. This feature is driver dependent. This will need a new external-snapshotter rebase + removal of the feature gate check in csi-snapshot-controller-operator. Freshly installed or upgraded from older release, will have group snapshot v1beta1 API enabled + enabled support for it in the snapshot-controller (+ ship corresponding external-snapshotter sidecar). No opt-in, no opt-out. OCP itself will not ship any CSI driver that supports it. Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? This is also a key requirement for backup and DR solutions specially for OCP virt. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. External snapshotter rebase to the upstream version that include the beta API. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR / ODF Documentation - STOR QE - STOR / ODF PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Since we don't ship any driver with OCP that support the feature we need to have testing with ODF Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. No risk, behind feature gate Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
    },
    {
        "summary": "Migrating CNS volumes between datastores via vSphere UI (GA)",
        "description": "Feature Overview (aka. Goal Summary) Allow customers to migrate CNS volumes (i.e vsphere CSI volumes) from one datastore to another. This operator relies on a new VMware CNS API and requires 8.0.2 or 7.0 Update 3o minimum versions In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. Goals (aka. expected user outcomes) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. As a first version, we developed a CLI tool that is shipped as part of the vsphere CSI operator. We keep this tooling internal for now, support can guide customers on a per request basis. This is to manage current urgent customer's requests, a CLI tool is easier and faster to develop it can also easily be used in previous OCP releases. After multiple discussion with VMware we now have confidence that we can rely on their built-in vSphere UI tool to migrate CNS volume from one datastore to another. This includes attached and detached volumes. Vmware confirmed they have confidence in this scenario and they fully support this operation for attached volumes. Requirements (aka. Acceptance Criteria): SInce the feature is external to OCP, it is mostly a matter of testing it works as expected with OCP but customers will be redirected to Vmware documentation as all the steps are done through the vSphere UI. Perform testing for attached and detached volumes + special cases such as RWX, zonal, encrypted. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YesHosted control planes YesConnected / Restricted Network x86Operator compatibility noUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCP on vsphere only| Use Cases (Optional): As a admin - want to migrate all my PVs or optional PVCs belonging to certain namespace to a different datastore within cluster without potentially requiring extended downtime. I want to move volumes to another datastore that has better performances I want to move volumes to another datastore current the current one is getting full I want to move all volumes to another datastore because the current one is being decommissioned. Questions to Answer (Optional): Get full support confirmation from vmware that their CNS volume migration feature Can be supported for OCP - YES is supported with attached volumes - YES Should detect if a volume is not migreable - YES Out of Scope Limited to what VMware supports. At the moment only one volume can be migrated at a time. Background We had a lot of requests to migrate volumes between datastore for multiple reason. Up until now it was not natively supported by VMware. In 8.0.2 they added a CNS API and a vsphere UI feature to perform volume migration. In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. This feature also includes the work needed to remove the CLI tool Customer Considerations Need to be explicit on requirements and limitations. Documentation Considerations Documented as part of the vsphere CSI OCP documentation. Specify min vsphere version. Document any limitation found during testing Redirect to vmware documentation. Announce removal of the CLI tool + update KB. Interoperability Considerations OCP on vSphere only"
    },
    {
        "summary": "Support for VolumeGroup Snapshots (GA)",
        "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as GA, have docs updated, testing as well as removing feature gate to enable it by default. Requirements (aka. Acceptance Criteria): Tests and CI must pass. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want my customers to benefit from the VolumeGroupSnapshot feature included in my CSI driver. As a backup/DR software vendor I want to use the VolumeGroupSnapshot feature. As a customer I want access to use VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs or use a backup/DR solution that leverages VolumeGroupSnapshot Out of Scope CSI drivers development/support for this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
    },
    {
        "summary": "vSphere - MachineSet - Support of more than one disk",
        "description": "Goal Support for more than one disk in machineset API for vSphere provider Feature description Customers using vSphere should be able to create machines with more than one disk. This is already available for other cloud and on-prem providers. Why do customers need this? To have Proper disk layout that better address their needs. Some examples are using the local storage operator or ODF. Affected packages or components RHCOS, Machine API, Cluster Infrastructure, CAPV."
    },
    {
        "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
        "description": "Feature Overview Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
    },
    {
        "summary": "GA 'oc adm upgrade status' command (and optionally status API)",
        "description": "As a customer of self managed OpenShift or an SRE managing a fleet of OpenShift clusters I should be able to determine the progress and state of an OCP upgrade and only be alerted if the cluster is unable to progress. Support a cli-status command and status-API which can be used by cluster-admin to monitor the progress. status command/API should also contain data to alert users about potential issues which can make the updates problematic. Feature Overview (aka. Goal Summary) {color:676767}_Show nodes where pod draining is taking more time._ _Customers have to dig deeper often to find the nodes for further debugging._ _The ask has been to bubble up this on the update progress window._{color} {color:676767}_oc update status ?_ _From the UI we can see the progress of the update. From oc cli we can see this from \"oc get cvo\"_ _But the ask is to show more details in a human-readable format._{color} _Know where the update has stopped. Consider adding at what run level it has stopped._ {code:java} oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.12.0 True True 16s Working towards 4.12.4: 9 of 829 done (1% complete) {code} Documentation Considerations {color:676767}_Update docs for UX and CLI changes_ _Reference :"
    },
    {
        "summary": "Tech Preview OLM v1: Manage operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes",
        "description": "Feature Overview (aka. Goal Summary) OLM v1 effectively manages operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Goals (aka. expected user outcomes) Users can rely on OLM v1 to manage operators packaged in registry+v1 bundle format, including those with OwnNamespace and SingleNamespace installModes. Operator authors can rely on OLM v1 to propagate the specified targetNamespaces to the operator deployment manifest during installation as the WATCH_NAMESPACE env vars, ensuring that the operator is scoped to the correct namespace without modifications Background Our Telco customers and ISV partners are eager to leverage OLM v1's ability to declare specific Operator versions for managed clusters using GitOps/ZTP workflows. By defining Operator versions directly in Git repositories, customers can ensure that only compatible versions are deployed to specific configurations. This GitOps process streamlines initial deployment and subsequent updates, ensuring alignment between Operator versions and managed cluster configurations. While the initial GA release of OLM v1 may have limitations, our goal is to support a broader range of operators, including those packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. This will enable us to meet the evolving needs of our Telco customers and protect our existing investments. By preserving compatibility with the current operator landscape, we can facilitate a smoother transition to the OLM v1. This not only secures existing workloads but also opens up new opportunities for growth within the OpenShift business. Requirements (aka. Acceptance Criteria) TargetNamespace propagation: OLM v1 can handle situations where the targetNamespace differs from or equals to the installNamespace when installing a registry+v1 bundle. OLM v1 can propagate the specified targetNamespace to the operator deployment manifest, ensuring correct namespace scoping for registry+v1 bundles. RBAC enforcement: OLM v1 enforces RBAC permissions based on the targetNamespace to prevent unauthorized access to cluster-wide resources Error handling and troubleshooting: OLM v1 provides warning and error logs to help users troubleshoot potential issues related to installing registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Customer Considerations Telco customers. Documentation Considerations A step-by-step guide on configuring and managing registry+v1 bundles with OwnNamespace and SingleNamespace install modes in OLM v1. Interoperability Considerations Existing Red Hat and certified operators packaged in registry+v1 bundles support OwnNamespace and SingleNamespace installModes."
    },
    {
        "summary": "Tech Preview OLM v1: Create a ServiceAccount with necessary permissions for managing cluster content lifecycle",
        "description": "Feature Overview (aka. Goal Summary) OLM v1 assists users in creating required ServiceAccounts with necessary permissions for managing cluster content lifecycle. Goals (aka. expected user outcomes) - Users can easily preview the required permissions before installing or upgrading an extension/operator. - Users can create a ServiceAccount with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Background By default, OLM v1 requires users to provide a service account for installing, upgrading, and deleting cluster content. This aligns with the least privilege principle, as OLM v1's default service account is limited to granted permissions and cannot easily perform actions on behalf of users with lower privileges. However, this requires cluster administrators or users with sufficient permissions to create a service account capable of creating, modifying, and deleting Kubernetes resources like Deployments, Services, and ConfigMaps, as needed by the extension/operator packages. To simplify this process, OLM v1 aims to assist users in determining and creating service accounts with appropriate permissions to manage cluster content. Requirements (aka. Acceptance Criteria) - Required permissions analysis: OLM v1 analyzes an extension/operator bundle to determine the required permissions for its lifecycle management. -- The required permissions include CRUD (Create, Read, Update, Delete) operations on all Kubernetes objects within the extension/operator bundle, as well as any permissions granted by included RBAC resources (if any) and resource dependencies. - Required permissions preview: OLM v1 provides a preview of required permissions for installing or upgrading operators/extensions to users. - Roles and RoleBindings creation: OLM v1 assists users in generating Role and RoleBinding objects based on the determined permissions, adhering to security best practices and least privilege principles. - ServiceAccount creation: OLM v1 assists users in creating a ServiceAccount and associate it with the generated RoleBindings with appropriate permissions for installing or upgrading extensions/operators. -- Provides an option for customizing the ServiceAccount name. - User Interaction: OLM v1 offers guidance and options for users to review and modify the generated Role/RoleBinding/ServiceAccount before creation. -- Provides an option for specifying custom permissions if needed. -- Handles errors during Role/RoleBinding/ServiceAccount creation with retry. -- Provides error messages for troubleshooting. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Open Questions: - How does this work for the contents packaged in Helm charts? -- Helm does not strictly require users to provide a ServiceAccount to create Kubernetes objects but can rely on the context in the Kubeconfig or the service account token mounted in the pod. Should we follow that pattern as one of the options to streamline the UX? Out of Scope __ your text here Documentation Considerations __ - The steps for previewing the required permissions before installing or upgrading an extension/operator. - The steps for creating a ServiceAccount and those associated Roles/Rolebindings with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Interoperability Considerations __ your text here"
    },
    {
        "summary": "Tech Preview (Phase 1) Next-gen OLM UX: Unifying workload management in the console",
        "description": "Feature Overview (aka. Goal Summary) : This ticket introduces the initial Tech Preview release of the next-generation OLM (OLM v1) user experience in the console. : This ticket focuses on enabling a unified catalog UX in the console. This will allow customers to manage layered capabilities delivered through operators and partners' workloads, including OpenShift certified Helm charts, using the next-generation OLM (OLM v1) in the OpenShift console. : This is enabled through the novel in-cluster efficient catalog content service designed in OCPSTRAT-1655 and delivered in the 4.18 timeframe. Goals (aka. expected user outcomes) In essence, customers can: discover collections of k8s extension/operator contents released in the FBC format with richer visibility into their release channels, versions, update graphs, and the deprecation information (if any) to make informed decisions about installation and/or update them. install a k8s extension/operator declaratively and potentially automate with GitOps to ensure predictable and reliable deployments. update a k8s extension/operator to a desired target version or keep it updated within a specific version range for security fixes without breaking changes. remove a k8s extension/operator declaratively and entirely including cleaning up its CRDs and other relevant on-cluster resources (with a way to opt out of this coming up in a later release). Requirements (aka. Acceptance Criteria): 1) Pre-installation: Both cluster-admins or non-privileged end-users can explore and discover the layered capabilities or workloads delivered by k8s extensions/operators or plain helm charts from a unified ecosystem catalog UI in the \u2018Administrator Perspective\u2019 in the console. Users can filter the available offerings based on the delivery mechanism/source type (i.e., operator-backed or plain helm charts), providers (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Users can discover all versions in all channels that an offering/package defines in a catalog, select a version from a channel, and see its detailed description, provided APIs, and other metadata before the installation. 2) Installation: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) using a ServiceAccount with sufficient permissions can install a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users can see the recommended installation namespace if provided by the package authors for installation. Users get notified through error messages from the OLM API whenever two conflicting k8s extensions/operators (will be) owning the same API objects, i.e., no conflicting ownership, after triggering the installation. During the installation, users can see the installation progress reported from the \u2018ClusterExtension\u2019 API object. After installed, users (who have access to OLM v1\u2019s user-facing \u2018ClusterExtension\u2019 API) can see can access the metadata of the installed k8s extension/operator to see essential information such as its provided APIs, example YAMLs of its provided APIs, descriptions, infrastructure features, valid subscriptions, etc. 3) Update: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) can see what updates are available for their k8s extension/operators in the form of immediate target versions and the associated update channels. Users can trigger the update of a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users get notified through error messages whenever a k8s extension/operator is prevented from updating to a newer version that has a backward incompatible CustomResourceDefinition (CRD) that will cause workload or k8s extension/operator breakage. During OpenShift cluster update, users get Informed when installed k8s extensions/operators do not support the next OpenShift version (when annotated by the package author/provider). Customers must update those k8s extensions/operators to a newer/compatible version before OLM unblocks the OpenShift cluster update. During the update, users can see the progress reported from the \u2018ClusterExtension\u2019 API object. 4) Uninstallation/Deletion: Users are made aware of OLM v1 by default cleanly remove an installed k8s extension/operator including deleting CustomResourceDefinitions (CRDs), custom resource objects (CRs) of the CRDs, and other relevant resources to revert the cluster to its original state before the installation. Users can see a list of resources that are relevant to the installed k8s extension/operator they are about to remove and then explicitly confirm the deletion. Questions to Answer (Optional): What impact will the console's \"perspective consolidation\" initiative have on this? Out of Scope __ your text here Background Our customers will experience a streamlined approach to managing layered capabilities and workloads delivered through operators, operators packaged in Helm charts, or even plain Helm charts. The next generation OLM will power this central distribution mechanism within the OpenShift in the future. Customers will be able to explore and discover the layered capabilities or workloads, and then install those offerings and make them available on their OpenShift clusters. Similar to the experience with the current OperatorHub, customers will be able to sort and filter the available offerings based on the delivery mechanism (i.e., operator-backed or plain helm charts), source type (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Once click on a specific offering, they see the details which include the description, usage, and requirements of the offering, the provided services in APIs, and the rest of the relevant metadata for making the decisions. The next-gen OLM aims to unify workload management. This includes operators packaged for current OLM, operators packaged in Helm charts, and even plain Helm charts for workloads. We want to leverage the current support for managing plain Helm charts within OpenShift and the console for leveraging our investment over the years. Documentation Considerations Refer to the \"Documentation Considerations\" section of the OLM v1 GA feature. Relevant documents Next-gen OLM UX: Unifying workload management Operator Framework F2F - PM Session OLM F2F Discussion - Roadmap"
    },
    {
        "summary": "Add all Dev only UI pages to the Admin Perspective",
        "description": "Feature Overview (aka. Goal Summary) __ your text here Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "BYOPKI for image verification in OCP - TP in 4.20",
        "description": "Feature Overview (aka. Goal Summary) Tech P : OCP 4.20 BYOPKI for image verification in OCP"
    },
    {
        "summary": "Migrate MAPI to Cluster API for AWS (TP) - Phase 1",
        "description": "Feature Overview (aka. Goal Summary) Implement Migration core for MAPI to CAPI for AWS This feature covers the design and implementation of converting from using the Machine API (MAPI) to Cluster API (CAPI) for AWS This Design investigates possible solutions for AWS Once AWS shim/sync layer is implemented use the architecture for other clouds in phase-2 & phase 3 Acceptance Criteria When customers use CAPI, There must be no negative effect to switching over to using CAPI . Seamless migration of Machine resources. the fields in MAPI/CAPI should reconcile from both CRDs."
    },
    {
        "summary": "Upstream OpenShift AutoScaler TechDebt (Phase 3)",
        "description": "Feature Overview This is a TechDebt and doesn't impact OpenShift Users. As the autoscaler has become a key feature of OpenShift, there is the requirement to continue to expand it's use bringing all the features to all the cloud platforms and contributing to the community upstream. This feature is to track the initiatives associated with the Autoscaler in OpenShift. Goals Scale from zero available on all cloud providers (where available) Required upstream work Work needed as a result of rebase to new kubernetes version Requirements RequirementNotesisMvp? Out of Scope n/a Background, and strategic fit Autoscaling is a key benefit of the Machine API and should be made available on all providers Assumptions Customer Considerations Documentation Considerations Target audience: cluster admins Updated content: update docs to mention any change to where the features are available."
    },
    {
        "summary": "Integrate Cluster API in standalone OCP-Phase 2",
        "description": "Feature Overview (aka. Goal Summary) Phase 2 Goal: Complete the design of the Cluster API (CAPI) architecture and build the core operator logic attach and detach of load balancers for internal and external load balancers for control plane machines on AWS, Azure, GCP and other relevant platforms manage the lifecycle of Cluster API components within OpenShift standalone clusters E2E tests for Phase-1, incorporating the assets from different repositories to simplify asset management. Background, and strategic fit Overarching Goal Move to using the upstream Cluster API (CAPI) in place of the current implementation of the Machine API for standalone Openshift. Phase 1 & 2 covers implementing base functionality for CAPI. Phase 2 also covers migrating MAPI resources to CAPI. Initially CAPI did not meet the requirements for cluster/machine management that OCP had the project has moved on, and CAPI is a better fit now and also has better community involvement. CAPI has much better community interaction than MAPI. Other projects are considering using CAPI and it would be cleaner to have one solution Long term it will allow us to add new features more easily in one place vs. doing this in multiple places. Acceptance Criteria There must be no negative effect to customers/users of the MAPI, this API must continue to be accessible to them though how it is implemented \"under the covers\" and if that implementation leverages CAPI is open"
    },
    {
        "summary": "GA Allow Custom machine names when using the CPMS feature",
        "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
    },
    {
        "summary": "GA Cert-manager support router to load secrets",
        "description": "Epic Goal Review design and development PRs that require feedback from NE team. Why is this important? Customer requires certificates to be managed by cert-manager on configured/newly added routes. Acceptance Criteria All PRs are reviewed and merged. Dependencies (internal and external) CFE team dependency for addressing review suggestions. Done Checklist DEV - All related PRs are merged."
    },
    {
        "summary": "Tech Preview Allow Custom machine names when using the CPMS feature",
        "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
    },
    {
        "summary": "Gateway API using Istio for Cluster Ingress - GA",
        "description": "Goal: Graduate to GA (full support) Gateway API with Istio to unify the management of cluster ingress with a common, open, expressive, and extensible API. Description: Gateway API is the evolution of upstream Kubernetes Ingress APIs. The upstream project is part of Kubernetes, working under SIG-NETWORK. OpenShift is contributing to the development, building a leadership position, and preparing OpenShift to support Gateway API, with Istio as our supported implementation. The plug-able nature of the implementation of Gateway API enables support for additional and optional 3rd-party Ingress technologies."
    },
    {
        "summary": "Enable HAProxy Dynamic Configuration Manager for OpenShift - Tech Preview",
        "description": "We need to do a lot of R&D and fix some known issues (e.g., see linked BZs). R&D targetted at 4.16 and productisation of this feature in 4.17"
    },
    {
        "summary": "GADisconnected Cluster Update and Boot without local image registry - phase 2",
        "description": "Feature Overview Note: This feature will be a TechPreview in 4.16 since the newly introduced API must graduate to v1. Overarching Goal Customers should be able to update and boot a cluster without a container registry in disconnected environments. This feature is for Baremetal disconnected cluster. Background For a single node cluster effectively cut off from all other networking, update the cluster despite the lack of access to image registries, local or remote. For multi-node clusters that could have a complete power outage, recover smoothly from that kind of disruption, despite the lack of access to image registries, local or remote. Allow cluster node(s) to boot without any access to a registry in case all the required images are pinned"
    },
    {
        "summary": "Updated boot images: Phase 4 (GCP, AWS to opt-out)",
        "description": "Feature Overview OCP 4 clusters still maintain pinned boot images. We have numerous clusters installed that have boot media pinned to first boot images as early as 4.1. In the future these boot images may not be certified by the OEM and may fail to boot on updated datacenter or cloud hardware platforms. These \"pinned\" boot images should be updateable so that customers can avoid this problem and better still scale out nodes with boot media that matches the running cluster version. In phase 1 provided tech preview for GCP. In phase 2, GCP support goes to GA and AWS goes to TP. In phase 3, AWS support goes to GA . In phase 4, AWS and GCP goes to opt-out. Requirements"
    },
    {
        "summary": "Exploitation of hardware based root volume LUKS encryption (IBM Z)",
        "description": "Feature Overview (aka. Goal Summary) As LUKS encryption is required for certain customer environments e.g. being PCI compliant and the current implementation with Network Based LUKS encryption are a) complex and b) not reliable and secure we need to support our Customers with an way to have the Root Device encrypted on a secure way with IBM HW based HSM to secure the LUKS Key. This is a kind of TPM approach to store the luks key but fence it from the user. Hardware based LUKS encryption requires injection of the read of secure keys in clevis during boot time. Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YHosted control planes YConnected / Restricted Network IBM ZOperator compatibility n/aUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "On Cluster Layering: Parity",
        "description": "Feature Overview (aka. Goal Summary) The original release of on cluster layering (OCL informally) came with known limitations and parity gaps. We need to close those gaps so everyone can have \"Image mode on OpenShift\". Goals (aka. expected user outcomes) OCL should work for as many OCP deployment patterns and footprints as possible. Ideally everywhere you can run OpenShift. Requirements (aka. Acceptance Criteria): Priority platform issues: Disconnected Single Node Two Node Hosted Control Planes Multi-arch (homogeneous arch cluster) Multi-arch (heterogeneous arch cluster) Priority feature issues: Node disruption policy Extensions"
    },
    {
        "summary": "On Cluster Layering: Phase 3 (GA)",
        "description": "Feature Overview This is Image mode on OpenShift. It uses the rpm-ostree native containers interface and not bootc but that is an implementation detail. In the initial delivery of CoreOS Layering, it is required that administrators provide their own build environment to customize RHCOS images. That could be a traditional RHEL environment or potentially an enterprising administrator with some knowledge of OCP Builds could set theirs up on-cluster. The primary virtue of an on-cluster build path is to continue using the cluster to manage the cluster. No external dependency, batteries-included. On-cluster, automated RHCOS Layering builds are important for multiple reasons: One-click/one-command upgrades of OCP are very popular. Many customers may want to make one or just a few customizations but also want to keep that simplified upgrade experience. Customers who only need to customize RHCOS temporarily (hotfix, driver test package, etc) will find off-cluster builds to be too much friction for one driver. One of OCP's virtues is that the platform and OS are developed, tested, and versioned together. Off-cluster building breaks that connection and leaves it up to the user to keep the OS up-to-date with the platform containers. We must make it easy for customers to add what they need and keep the OS image matched to the platform containers. Goals & Requirements The goal of this feature is primarily to bring the 4.14 progress (OCPSTRAT-35) to a Tech Preview or GA level of support. Customers should be able to specify a Containerfile with their customizations and \"forget it\" as long as the automated builds succeed. If they fail, the admin should be alerted and pointed to the logs from the failed build. The admin should then be able to correct the build and resume the upgrade. Intersect with the Custom Boot Images such that a required custom software component can be present on every boot of every node throughout the installation process including the bootstrap node sequence (example: out-of-box storage driver needed for root disk). Users can return a pool to an unmodified image easily. RHEL entitlements should be wired in or at least simple to set up (once). Parity with current features - including the current drain/reboot suppression list, CoreOS Extensions, and config drift monitoring."
    },
    {
        "summary": "Dev Preview AutoNode (Native Karpenter) with HCP",
        "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP -OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift) as APIs for Managed Services (ROSA) to consume. It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 (out of scope for Dev Preview) _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade (TBD for Dev Preview but potentially out of scope for Dev Preview, i.e. may slip to Tech Preview) Expose Karpenter metrics to Cluster Admin (out of scope for Dev Preview, Targeting Tech Preview) Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
    },
    {
        "summary": "Add a Mechanism to Label all Pods for a Hosted Cluster in the Control Plane Namespace",
        "description": "Background As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. Goal Implement Mechanism for Labeling Hosted Cluster Control Plane Pods Use-cases - Adherance to Microsoft 1p Resource Provider Requirements Components - Any pods that hypershift deploys or run in the hosted cluster namespace."
    },
    {
        "summary": "Azure - Remove not required permissions from the Nodes",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria) _The Installer only creates the minimum permissions required to deploy OpenShift on Azure_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background Customer Considerations _A KCS will be created for customers running previous OpenShift releases who want to remove this resource_ Documentation Considerations"
    },
    {
        "summary": "Support Private Google Access to GCP endpoints",
        "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
    },
    {
        "summary": "AWS - Allocate Load Balancers (API & Ingress) to Specific Subnets",
        "description": "Add ability to choose subnet while creating ingresscontroller of type LoadBalancerService. Checking ingresscontroller CRD could see that there is no such way to set subnet of load balancer. Why is this important? Currently, when deploying an IngressController instance , all the FrontendIPs will be in the same subnet. However, the LoadBalancer Service implementation allows specifying the target subnet through service annotation. Therefore the need to introduce an additional field to the ingresscontroller CRD, that allows to specify the target subnet. The value of this field is then used to annotate the created LoadBalancer Service from the beginning on, so the ingress controller immediately gets its FrontendIP into the right subnet. Scenarios If the cluster is spread across multiple subnets then its good to have a way to set subnet while creating ingresscontroller of type LoadBalancerService."
    },
    {
        "summary": "Remove Terraform from the Azure Stack Hub IPI installer",
        "description": "Feature Overview (aka. Goal Summary) As a result of Hashicorp's license change to BSL, Red Hat OpenShift needs to remove the use of Hashicorp's Terraform from the installer - specifically for IPI deployments which currently use Terraform for setting up the infrastructure. To avoid an increased support overhead once the license changes at the end of the year, we want to provision Azure Stack Hub infrastructure without the use of Terraform. Requirements (aka. Acceptance Criteria): _The Azure Stack Hub IPI Installer no longer contains or uses Terraform._ _The new provider should aim to provide the same results and have parity with the existing Azure Stack Hub Terraform provider. Specifically,_ we should aim for feature parity against the install config and the cluster it creates to minimize impact on existing customers' UX. Use Cases (Optional): __ Questions to Answer (Optional): __ Out of Scope __ Background __ Customer Considerations __ Documentation Considerations __ Interoperability Considerations __"
    },
    {
        "summary": "Remove ARO build-flag in openshift-installer (ARO fork removal - Phase II - Part 1)",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "Add support to enable boot diagnostics option at installation time in Azure (ARO fork removal - Phase II - Part 2)",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ Documentation Considerations Interoperability Considerations"
    },
    {
        "summary": "CAPI-based Installer technical debt",
        "description": "Feature Overview (aka. Goal Summary) _Review, refine and harden the CAPI-based Installer implementation introduced in 4.16_ Goals (aka. expected user outcomes) _From the implementation of the CAPI-based Installer started with OpenShift 4.16 there is some technical debt that needs to be reviewed and addressed to refine and harden this new installation architecture._ Requirements (aka. Acceptance Criteria): _Review existing implementation, refine as required and harden as possible to remove all the existing technical debt_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Documentation Considerations _There should not be any user-facing documentation required for this work_"
    },
    {
        "summary": "BGP for UDN GA On-prem",
        "description": "Feature Overview (aka. Goal Summary) OVN Kubernetes BGP support as a routing protocol for User Defined Network (Segmentation) pod and VM addressability. Goals (aka. expected user outcomes) OVN-Kubernetes BGP support enables the capability of dynamically exposing cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. OVN-Kubernetes currently has no native routing protocol integration, and relies on a Geneve overlay for east/west traffic, as well as third party operators to handle external network integration into the cluster. This enhancement adds support for BGP as a supported routing protocol with OVN-Kubernetes. The extent of this support will allow OVN-Kubernetes to integrate into different BGP user environments, enabling it to dynamically expose cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. In a follow-on release, this enhancement will provide support for EVPN, which is a common data center networking fabric that relies on BGP via OCPSTRAT-1744 Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Use Cases (Optional): Integration with 3rdparty load balancers that send packets directly to OpenShift nodes with the destination IP address of a targeted pod, without needing custom operators to detect which node a pod is scheduled to and then add routes into the load balancer to send the packet to the right node. Questions to Answer (Optional): Out of Scope Support of any other routing protocol Running separate BGP instances per VRF network Support for any other type of L3VPN with BGP, including MPLS Providing any type of API or operator to automatically connect two Kubernetes clusters via L3VPN Replacing the support that MetalLB provides today for advertising service IPs Asymmetric Integrated Routing and Bridging (IRB) with EVPN Background BGP Importing Routes from the Provider Network Today in OpenShift there is no API for a user to be able to configure routes into OVN. In order for a user to change how cluster traffic is routed egress into the cluster, the user leverages local gateway mode, which forces egress traffic to hop through the Linux host's networking stack, where a user can configure routes inside of the host via NM State. This manual configuration would need to be performed and maintained across nodes and VRFs within each node. Additionally, if a user chooses to not manage routes within the host and use local gateway mode, then by default traffic is always sent to the default gateway. The only other way to affect egress routing is by using the Multiple External Gateways (MEG) feature. With this feature the user may choose to have multiple different egress gateways per namespace to send traffic to. As an alternative, configuring BGP peers and which route-targets to import would eliminate the need to manually configure routes in the host, and would allow dynamic routing updates based on changes in the provider\u2019s network. Exporting Routes into the Provider Network There exists a need for provider networks to learn routes directly to services and pods today in Kubernetes. Metal LB is already one solution whereby load balancer IPs are advertised by BGP to provider networks, and this feature development does not intend to duplicate or replace the function of Metal LB. Metal LB should be able to interoperate with OVN-Kubernetes, and be responsible for advertising services to a provider\u2019s network. However, there is an alternative need to advertise pod IPs on the provider network. One use case is integration with 3rd party load balancers, where they terminate a load balancer and then send packets directly to OCP nodes with the destination IP address being the pod IP itself. Today these load balancers rely on custom operators to detect which node a pod is scheduled to and then add routes into its load balancer to send the packet to the right node. By integrating BGP and advertising the pod subnets/addresses directly on the provider network, load balancers and other entities on the network would be able to reach the pod IPs directly. EVPN Extending OVN-Kubernetes VRFs into the Provider Network This is the most powerful motivation for bringing support of EVPN into OVN-Kubernetes. A previous development effort enabled the ability to create a network per namespace (VRF) in OVN-Kubernetes, allowing users to create multiple isolated networks for namespaces of pods. However, the VRFs terminate at node egress, and routes are leaked from the default VRF so that traffic is able to route out of the OCP node. With EVPN, we can now extend the VRFs into the provider network using a VPN. This unlocks the ability to have L3VPNs that extend across the provider networks. Utilizing the EVPN Fabric as the Overlay for OVN-Kubernetes In addition to extending VRFs to the outside world for ingress and egress, we can also leverage EVPN to handle extending VRFs into the fabric for east/west traffic. This is useful in EVPN DC deployments where EVPN is already being used in the TOR network, and there is no need to use a Geneve overlay. In this use case, both layer 2 (MAC-VRFs) and layer 3 (IP-VRFs) can be advertised directly to the EVPN fabric. One advantage of doing this is that with Layer 2 networks, broadcast, unknown-unicast and multicast (BUM) traffic is suppressed across the EVPN fabric. Therefore the flooding domain in L2 networks for this type of traffic is limited to the node. Multi-homing, Link Redundancy, Fast Convergence Extending the EVPN fabric to OCP nodes brings other added benefits that are not present in OCP natively today. In this design there are at least 2 physical NICs and links leaving the OCP node to the EVPN leaves. This provides link redundancy, and when coupled with BFD and mass withdrawal, it can also provide fast failover. Additionally, the links can be used by the EVPN fabric to utilize ECMP routing. Customer Considerations For customers using MetalLB, it will continue to function correctly regardless of this development. Documentation Considerations Interoperability Considerations Multiple External Gateways (MEG) Egress IP Services Egress Service Egress Firewall Egress QoS"
    },
    {
        "summary": "OCP Console - Upgrade to PatternFly 6 (PF6)",
        "description": "Feature Overview (aka. Goal Summary) __ Upgrade the OCP console to Pattern Fly 6. Goals (aka. expected user outcomes) __ The core OCP Console should be upgraded to PF 6 and the Dynamic Plugin Framework should add support for PF6 and deprecate PF4. Requirements (aka. Acceptance Criteria): __ Console, Dynamic Plugin Framework, Dynamic Plugin Template, and Examples all should be upgraded to PF6 and all PF4 code should be removed. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background __ As a company we have all agreed to getting our products to look and feel the same. The current level is PF6. Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "TechDebt - OCP Console - Dependency Cleanup",
        "description": "Feature Overview (aka. Goal Summary) We need to maintain our dependencies across all the libraries we use in order to stay in compliance. Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "HCP KubeVirt VM Enhanced Topology Spread",
        "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
    },
    {
        "summary": "Phase 1: Cosign tag-based discovery oc-mirror v2: Discover and mirror SigStore-style attachments",
        "description": "Feature Overview (aka. Goal Summary) oc-mirror v2 can mirror images and their associated signatures and public keys, providing flexibility for both current \"SigStore/Cosign tag-based\" and the future \"OCI 1.1 referrer-based\" discovery modes. Goals (aka. expected user outcomes) oc-mirror v2 can discover and mirror image signatures alongside the images it mirrors, adhering to the Cosign tag convention. {color:00875a}(Out of scope for Phase 1, see reason in the comment Public Key Mirroring (or made available offline): Mirror public keys required for signature verification, including: Red Hat's public key for verifying Red Hat content. Rekor's public key for verifying signature transaction records for Red Hat content. To support enclave use cases, mirror public keys and store them locally to enable offline verification. This may require special permissions to access the /etc directory for storage. Out of Scope in Phase 1 __ OCI 1.1 referrer-based Discovery: The tool should be able to discover signatures referenced in the OCI 1.1 image manifest. SigStore-style Attachments Mirroring: SigStore-style attachments, such as SBOMs in formats like text/spdx or application/vnd.cyclonedx, should be optionally discoverable and mirrorable. Users can choose to enable this feature and specify the desired OCI media types. Flexible Signature Discovery - OCI 1.1 First, Cosign Second: oc-mirror v2 will prioritize discovering image signatures using the OCI 1.1 referrer-based approach, falling back to the SigStore/Cosign tag-based approach if necessary. Documentation Considerations __ your text here"
    },
    {
        "summary": "GA OC mirror v2",
        "description": "Feature description Oc-mirror v2 is focuses on major enhancements that include making oc-mirror faster and more robust and introduces caching as well as address more complex air-gapped scenarios. OC mirror v2 is a rewritten version with three goals: Manage complex air-gapped scenarios, providing support for the enclaves feature Faster and more robust: introduces caching, it doesn\u2019t rebuild catalogs from scratch Improves code maintainability, making it more reliable and easier to add features, and fixes, and including a feature plugin interface"
    },
    {
        "summary": "Configure containers to set readOnlyRootFilesystem to true starting in OCP 4.19",
        "description": "Red Hat Product Security recommends that pods be deployed with readOnlyRootFilesystem set to true in the SecurityContext, but does not require it because a successful attack can only be carried out with a combination of weaknesses and OpenShift runs with a variety of mitigating controls. However, customers are increasingly asking questions about why pods from Red Hat, and deployed as part of OpenShift, do not follow common hardening recommendations. Note that setting readOnlyRootFilesystem to true ensures that the container's root filesystem is mounted as read-only. This setting has nothing to do with host access. For more information, see Setting the readOnlyRootFilesystem flag to true reduces the attack surface of your containers, preventing an attacker from manipulating the contents of your container and its root file system. If your container needs to write temporary files, you can specify the ability to mount an emptyDir in the Security Context for your pod as described here. The following containers have been identified by customer scans as needing remediation. If your pod will not function with readOnlyRootFilesystem set to true, please document why so that we can document the reason for the exception. Service Mesh operator with sidecar-injector (this needs some additional investigation as we no longer ship the sidecar-injector with Service Mesh) S2I and Build operators: webhook tekton-pipelines-controller tekton-chains-controller openshift-pipelines-operator-cluster-operations tekton-operator-webhook openshift-pipelines-operator-lifecycle-event-listener Pac-webhook (part of Pipelines) Cluster ingress operator: serve-healthcheck-canary Node tuning operator: Tuned Machine Config Operator: Machine-config-daemon ACM Operator: Klusterlet-manifestwork-agent. This was fixed in ACM 2.10."
    },
    {
        "summary": "Tech Preview AutoNode (Native Karpenter) with ROSA-HCP",
        "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP - OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift). It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade Expose Karpenter metrics to Cluster Admin Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
    },
    {
        "summary": "Dev Preview Agent-Installer Installation UI for OpenShift Virtualization",
        "description": "Summary The installation process for the OpenShift Virtualization Engine (OVE) has been identified as a critical area for improvement to address customer concerns regarding its complexity compared to competitors like VMware, Nutanix, and Proxmox. Customers often struggle with disconnected environments, operator configuration, and managing external dependencies, making the initial deployment challenging and time-consuming. To resolve these issues, the goal is to deliver a streamlined, opinionated installation workflow that leverages existing tools like the Agent-Based Installer, the Assisted Installer, and the OpenShift Appliance (all sharing the same underlying technology) while pre-configuring essential operators and minimizing dependencies, especially the need for an image registry before installation. By focusing on enterprise customers, particularly VMware administrators working in isolated networks, this effort aims to provide a user-friendly, UI-based installation experience that simplifies cluster setup and ensures quick time-to-value. Objectives and Goals Primary Objectives Simplify the OpenShift Virtualization installation process to reduce complexity for enterprise customers coming from VMware vSphere. Enable installation in disconnected environments with minimal prerequisites. Eliminate the dependency on a pre-existing image registry in disconnected installations. Provide a user-friendly, UI-driven installation experience for users used to VMware vSphere. Goals Deliver an installation experience leveraging existing tools like the Agent-Based Installer, Assisted Installer, and OpenShift Appliance, i.e. the Assisted Service. Pre-configure essential operators for OVE and minimize external day 1 dependencies (see OCPSTRAT-1811 \"Agent Installer interface to install Operators\") Ensure successful installation in disconnected environments with standalone OpenShift, with minimal requirements and no pre-existing registry Personas Primary Audience VMware administrators transitioning to OpenShift Virtualization in isolated/disconnected environments. Pain Points Lack of UI-driven workflows; writing YAML files is a barrier for the target user (virtualization platforms admins) Complex setup requirements (e.g., image registries in disconnected environments). Difficulty in configuring network settings interactively. Lack of understanding when to use a specific installation method Hard time finding the relevant installation method (docs or at console.redhat.com) Technical Requirements Image Registry Simplification Eliminate the dependency on an existing external image registry for disconnected environments. Support a workflow similar to the OpenShift Appliance model, where users can deploy a cluster without external dependencies. Agent-Based Installer Enhancements Extend the existing UI to capture all essential data points (e.g., cluster details, network settings, storage configuration) without requiring YAML files. Install without a pre-existing registry in disconnected environment Install required operators for virtualization OpenShift Virtualization Reference Implementation Guide v1.0.2) doing a POC which was promising: User Interface (no configuration files) The type of users coming from VMware vSphere expect a UI. They aren't used to writing YAML files and this has been identified as a blocker for some of them. We must provide a simple UI to stand up a cluster. Proposed Workflow PRD and notes from regular meetings|"
    },
    {
        "summary": "Support for VolumeGroup Snapshots (TP)",
        "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. This feature tracks the Tech Preview implementation behind feature gate. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as tech preview have docs, testing as well as a feature gate to enable it in order for customers and partners to test it in advance. Requirements (aka. Acceptance Criteria): The feature should be graduated beta upstream to become TP in OCP. Tests and CI must pass and a feature gate should allow customers and partners to easily enable it. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs. Out of Scope CSI drivers development/support of this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
    },
    {
        "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
        "description": "Feature Overview Requirements Users can specify multiple NICs for the OpenShift VMs that will be created for the OpenShift cluster nodes with different subnets."
    },
    {
        "summary": "vSphere - Delete PV and PVCs when destroying a cluster",
        "description": "Goal Remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? When an OpenShift cluster on vSphere with CSI volumes is destroyed the volumes are not deleted, leaving behind multiple objects within vSphere. This leads to storage usage by orphan volumes that must be manually deleted. Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
    },
    {
        "summary": "Tech Preview OCP Update Precheck command to improve update experience",
        "description": "Feature Overview (aka. Goal Summary) As a cluster-admin I can use a single command to see all upgrade checklist before I trigger an update. Create a Update precheck command that is part of core openshift that helps customers identify potential issues before triggering an OpenShift cluster upgrade, without blocking the upgrade process. This tool aims to reduce upgrade failures and support tickets by surfacing common issues beforehand. Goals (aka. expected user outcomes) Enable users (especially those with limited OpenShift expertise) to identify potential upgrade issues before starting the upgrade Reduce the number of failed upgrades and support tickets Provide clear, actionable information about cluster state relevant to upgrades Help customers make informed decisions about when to initiate upgrades Requirements (aka. Acceptance Criteria): Check Pod Disruption Budgets (PDBs): Identify existing PDBs that might impact the upgrade Display information about PDBs in a way that's understandable to users with limited Kubernetes experience workaround - Check DVO PDB checks Image Registry Access Verification: Validate access to required image repositories Pre-check ability to pull images needed for the upgrade Verify connectivity to public registries or repository of choice workaround : Image pinning GA Node Health Verification: Check for unavailable nodes Identify nodes in maintenance mode Detect unscheduled nodes Verify overall node health status Core Platform Component Health: Verify health of control plane workloads Check core platform operators' health Alert Analysis: List any active critical alerts Display relevant warning alerts Focus on alerts that could impact upgrade success Version-Specific Checks: Include checks specific to the target upgrade version Verify requirements for new features or changes between versions Check networking-related requirements (e.g., SDN to OVN migrations) Output Requirements: Provide clear, understandable output for users without deep OpenShift knowledge Don't block upgrades even if issues are found Present information in an easily digestible format ============== {}New additions in 2025{} MCP status Check the maxUnavailable Compare maxUnavailable to the request level or current load level (if above request level) and determine if this is the correct setting Check to see if the MCPs are paused Make a note if etcd is backed up Other operators Note which operators are set to manual vs automatic update Check to determine the next update of all OLM based operators __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both standaloneHosted control planes AllConnected / Restricted Network AllOperator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope Blocking upgrade execution Checking entire cluster state Verifying non-platform workloads Automated issue resolution Comprehensive cluster health checking Extensive operator compatibility verification beyond core platform ACM integration Although Operations will use ACM for day 2 operations. Customer Engineering will use cli for patching, updating, precheck etc. Background __ your text here Customer Considerations Target users may have limited Kubernetes/OpenShift expertise Many users coming from VMware background Customers often don't have TAM or premium support Users may not be familiar with platform-specific concepts Need to accommodate users who prefer not to read extensive documentation Documentation Considerations Interoperability Considerations __ your text here",
        "epic_key": "OTA-1432"
    },
    {
        "summary": "Cluster-version operator version-pod failure accessability",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Simplify debugging when a cluster fails to update to a new target release image, when that release image is unsigned or otherwise fails to pull. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here - Kubelet/CRIO to verify RH images & release payload sigstore signatures - ART will add sigstore signatures to core OCP images __ These acceptance criteria are for all deployment flavors of OpenShift. List applicable specific needs (N/A = not applicable)Self-managed, managed, or both yesHosted control planes Connected / Restricted Network Operator compatibility none, Other (please specify) Documentation Considerations Add documentation for sigstore verification and gpg verification Interoperability Considerations For folks mirroring release images (e.g. disconnected/restricted-network): oc-mirror need to support sigstore mirroring (OCPSTRAT-1417). Customers using BYO image registries need to support hosting sigstore signatures."
    },
    {
        "summary": "Adding topology-awareness to Cinder CSI Driver",
        "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs| Since OpenStack does not provide a mechanism to map compute nodes to block storage AZs, the Cinder CSI driver treats the compute AZ as a block storage AZ, assuming that the operator has used the same naming convention across their deployment (that is, if there are three compute AZs, {{{}az-0{}}}, {{{}az-1{}}}, and {{{}az-2{}}}, then there will always be at least three block storage AZs with the same name and same semantic meaning (e.g. azN implies a particular rack, room, or data center for both the compute and block storage services). This is a reasonable position and is one the Nova project endorses, however, it isn't always true. Where a deployment is not doing and has divergent compute and block storage AZs, the Cinder CSI driver can end up requesting volumes with block storage AZs that don't exist. The way we have worked around this to date is to selectively enable or disable the topology feature flag provided to the external provisioner side car container, as deployed and managed by the Cinder CSI Driver Operator. This feature flag is being removed in a future release (when?), which means we can't rely on this long-term. We should therefore port the logic for determining whether or not to enable the topology feature from the Cinder CSI Driver Operator to the Cinder CSI Driver itself. Once this is done, we should remove the logic from the Operator since it should no longer be needed and will eventually not be supported. This epic tracks the above work. Why is this important? If we don't do this, we would lose the ability to disable the topology feature in environment where this is not supported (due to mismatched compute and block storage AZ sets). This will affect a number of customers."
    },
    {
        "summary": "OLMv1: Downstream Feature Gate Promotion Mechanics",
        "description": "Feature Overview (aka. Goal Summary) __ Support iterative development by enabling OLMv1 to work both with a _TechPreviewNoUpgrade_ feature set and a _GeneralAvailability_ feature set. Goals (aka. expected user outcomes) __ Users will be able to opt into testing out new features via _TechPreviewNoUpgrade_ Users will be able to use OLMv1 GA features without the risk of _TechPreview_ features Requirements (aka. Acceptance Criteria): __ All necessary infrastructure in place to enable the use of feature gates CI jobs to ensure that nothing from the _TechPreviewNoUpdate_ feature set breaks OLMv1 __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Out of Scope __ OLMv1 will soon need a mechanism for supporting alpha/beta fields in CRDs, but it has been decided to explore this as a separate effort. Background __ Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP which might span multiple releases to reach completeness and stability (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will need to work both with TechPreviewNoUpgrade and without it but will have a different set of features. Customer Considerations __ your text here Documentation Considerations __ Will need documentation on how to enable the _TechPreviewNoUpgrade_ feature set Interoperability Considerations __ your text here"
    },
    {
        "summary": "Remove Cgroup v1 from OCP in 4.19",
        "description": "Feature Overview (aka. Goal Summary) Cgroup V1 was deprecated in OCP 4.16 . RHEL will be removing support for cgroup v1 in RHEL 10 so we will remove it in OCP 4.19 Goal Upgrade Scenario For clusters running cgroup v1 on OpenShift 4.18 or earlier, upgrading to OpenShift 4.19 will be blocked. To proceed with the upgrade, clusters on OpenShift 4.18 must first switch from cgroup v1 to cgroup v2. Once this transition is complete, the cluster upgrade to OpenShift 4.19 can be performed."
    },
    {
        "summary": "GA User Name Space in OpenShift 4.20",
        "description": "Feature Overview (aka. Goal Summary) GA User Name Space in OpenShift 4.20 continue work from"
    },
    {
        "summary": "GA for sigstore API(clusterimagepolicy, imagepolicy)",
        "description": "GA for sigstore API(clusterimagepolicy, imagepolicy)"
    },
    {
        "summary": "BYOPKI for image verification in OCP - Dev P in 4.19",
        "description": "Feature Overview (aka. Goal Summary) BYOPKI for image verification in OCP"
    },
    {
        "summary": "Azure - Add support for Dxv6 machine series",
        "description": "Feature Overview (aka. Goal Summary) Dlsv6 Dsv6 Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Dldsv6 Ddsv6| Documentation Considerations Interoperability Considerations"
    },
    {
        "summary": "Support EndPort in MultiNetworkPolicy",
        "description": "Feature Overview (aka. Goal Summary) This Feature adds support for EndPort in MultiNetworkPolicy for customers migrating VM instances to OpenShift Virtualization, and with a requirement to specify a port-range without having to individually specify each port separately. Without this Feature, customers will have issues migrating specific VMs to OpenShift Virtualization. It is currently supported with NetworkPolicy, but not yet with MultiNetworkPolicy. Goals (aka. expected user outcomes) A port range can be specified in MultiNetworkPolicy, instead of having to specify each port individually. Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) NetworkPolicy product docs| Questions to Answer (Optional): Out of Scope Background Customer Considerations Documentation Considerations Interoperability Considerations"
    },
    {
        "summary": "vsphere Multi Disk Support",
        "description": "User Story: As an OpenShift administrator, I need to be able to configure my OpenShift cluster to have additional disks on each vSphere VM so that I can use the new data disks for various OS needs. Description: This goal of this epic is to be able to allow the cluster administrator to install and configure after install new machines with additional disks attached to each virtual machine for various OS needs. Required: Installer allows configuring additional disks for control plane and compute virtual machines Control Plane Machine Sets (CPMS) allows configuring control plane virtual machines with additional disks Machine API (MAPI) allows for configuring Machines and MachineSets with additional disks Cluster API (CAPI) allows for configuring Machines and MachineSets with additional disks Nice to Have: Acceptance Criteria: Notes:"
    },
    {
        "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
        "description": "Epic Goal Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements{} Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
    },
    {
        "summary": "Remove IPI/UPI support of Alibaba cloud from OpenShift",
        "description": "OCP/Telco Definition of Done Epic Goal We want to remove official support for UPI and IPI support for Alibaba Cloud provider. Going forward, we are recommending installations on Alibaba Cloud with either external platform installation method. Why is this important? __ Scenarios Impacted areas based on CI: alibaba-cloud-csi-driver/openshift-alibaba-cloud-csi-driver-release-4.16.yaml alibaba-disk-csi-driver-operator/openshift-alibaba-disk-csi-driver-operator-release-4.16.yaml cloud-provider-alibaba-cloud/openshift-cloud-provider-alibaba-cloud-release-4.16.yaml cluster-api-provider-alibaba/openshift-cluster-api-provider-alibaba-release-4.16.yaml cluster-cloud-controller-manager-operator/openshift-cluster-cloud-controller-manager-operator-release-4.16.yaml machine-config-operator/openshift-machine-config-operator-release-4.16.yaml Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI jobs are removed Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
        "description": "OCP/Telco Definition of Done Epic Goal In 4.18, support for multiple NICs was released as tech preview. The goal of this epic is to promote the feature to GA. Primarily, this involves proving the stability of the feature through supporting CI jobs and Sippy. Once proven, the feature gate and associated logic across impacted components are removed. Why is this important? Provides production/typical support for this feature. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "vSphere - Delete PV and PVCs on installer destroy",
        "description": "Epic Goal The goal of this epic is upon destroy to remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
    },
    {
        "summary": "Migrate UIPlugins to PF6",
        "description": "Description \"In order to keep plugins working with future versions of the OpenShift web console and match the new PF6 design, we as the Observability UI Team need to upgrade them to PF6.\" Goals & Outcomes +Engineering/Data Analytics Requirements:+ All UI plugins use PF6 dependency Documentation Patternfly support for plugins in the console:"
    },
    {
        "summary": "Status API for oc adm upgrade status command",
        "description": "Epic Goal Add a new command `oc adm upgrade status` command which is backed by an API. Please find the mock output of the command output attached in this card. Why is this important? (mandatory) From the UI we can see the progress of the update. Using OC CLI we can see some of the information using \"oc get clusterversion\" but the output is not readable and it is a lot of extra information to process. Customer as asking us to show more details in a human-readable format as well provide an API which they can use for automation. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
    },
    {
        "summary": "Reduce cluster version operator logging",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
    },
    {
        "summary": "Extend tech-preview 'oc adm upgrade recommend' to render relevant alerts",
        "description": "Epic Goal OCPSTRAT-1834 is requesting an oc precheck command that helps customers identify potential issues before triggering an OpenShift cluster upgrade. For 4.18, we'd built a tech-preview oc adm upgrade recommend (OTA-1270, product docs?\" space, and this Epic is about extending that subcommand with alerts to deliver the coverage requested by OCPSTRAT-1834. Why is this important? We currently document some manual checks for customer admins to run before launching an update. For example, RFE-5104 is up asking to automate whatever we're hoping customer are supposed to look for vs. critical alerts. But updating the production OpenShift Update Service is complicated, and it's easier to play around in a tech-preview oc subcommand, while we get a better idea of what information is helpful, and which presentation approaches are most accessible. 4.18's OTA-902 / cvo1907 and this Epic proposes to continue in that direction by retrieving update-relevant alerts and folding those in as additional client-side Conditional Update risks. Scenarios As a cluster administrator interested in launching an OCP update, I want to run an oc command that talks to me about my next-hop options, including any information related to known regressions with those target releases, and also including any information about things I should consider addressing in my current cluster state. Dependencies The initial implementation can be delivered unilaterally by the OTA updates team. The implementation may surface ambiguous or hard-to-actuate alert messages, and those messages will need to be improved by the component team responsible for maintaining that alert. Contributing Teams (and contacts) Development - OTA Documentation - no docs required QE - OTA PX - OTA Others - Acceptance Criteria OCPSTRAT-1834 customer is happy :) Drawbacks or Risk Client-side Conditional Update risks are helpful for cluster administrators who use that particular client. But admins who use older oc or who are using the in-cluster web-console and similar will not see risks known only to newer oc. If we can clearly tie a particular cluster state to update risk, declaring that risk via the OpenShift Update Service would put the information in front of all cluster administrators, regardless of which update interface they use. However, trialing update risks client-side in tech-preview oc and then possibly promoting them to risks served by the OpenShift Update Service in the future might help us identify cluster state that's only weakly coupled to update success but still interesting enough to display. Or help us find more accessible ways of displaying that context before putting the message in front of large chunks of the fleet. Done - Checklist CI Testing - Tests are merged and completing successfully Documentation - No docs. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
    },
    {
        "summary": "oc precheck command for oc cli",
        "description": "Epic Goal oc precheck is a command that the customer can run pre-update in 4.16 and above clusters to check for a limited number of conditions that might cause issues in upgrading the cluster. this command will create a report that the customer can read before they start the upgrade. the command output will NOT make any decisions regarding to proceed with the upgrade or not. That decision should be taken by the customer after reading the report. Why is this important? (mandatory) The customer requires a command that they can run on the cluster that prints a report of cluster conditions that might affect the upgrade process. Scenarios (mandatory) customer uses oc precheck command to check for cluster conditions that can hinder the upgrade process like strict pdb, etc Dependencies (internal and external) (mandatory) this is entirely an OTA team undertaking. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - OTA Documentation - OTA docs team QE - OTA PX - Others - Acceptance Criteria (optional) OTA team ships a `oc precheck` command to check for cluster conditions. Drawbacks or Risk (optional) We can check for frequent conditions that cause issues with upgrades, but we cannot guarantee that the cluster upgrade will go smoothly even after the customer reads the report of this command as every cluster is different and there are lot of unknowns. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
    },
    {
        "summary": "ClusterVersion status should include version-Pod error details",
        "description": "Epic Goal Currently the CVO launches a Job and waits for it to complete to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so the goal of this Epic is to drop it and create and monitor the Pod ourselves, so we can deliver better reporting of version-Pod state. Why is this important? When the version Pod fails to run, the cluster admin will likely need to take some action (clearing the update request, fixing a mirror registry, etc.). The more clearly we share the issues that the Pod is having with the cluster admin, the easier it will be for them to figure out their next steps. Scenarios oc adm upgrade and other ClusterVersion status UIs will be able to display Init:SignatureValidationFailed and other version-Pod failure modes directly. We don't expect to be able to give ClusterVersion consumers more detailed next-step advice, but hopefully the easier access to failure-mode context makes it easier for them to figure out next-steps on their own. Dependencies This change is purely and updates-team/OTA CVO pull request. No other dependencies. Contributing Teams Development - OTA Documentation - OTA QE - OTA Acceptance Criteria Definition of done: failure modes like unretrievable image digests (e.g. quay.io/openshift-release-dev/ocp-release@sha256:0000000000000000000000000000000000000000000000000000000000000000) or images with missing or unacceptable Sigstore signatures with OTA-1304's ClusterImagePolicy) have failure-mode details in ClusterVersion's RetrievePayload message, instead of the current Job was active longer than specified deadline. Drawbacks or Risk Limited audience, and failures like Init:SignatureValidationFailed are generic, while CVO version-Pod handling is pretty narrow. This may be redundant work if we end up getting nice generic init-Pod-issue handling like RFE-5627. But even if the work ends up being redundant, thinning the CVO stack by removing the Job controller is kind of nice. Done - Checklist The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
    },
    {
        "summary": "Sync CA bundle to credentials",
        "description": "Goal Add support for syncing CA bundle to the credentials generated by Cloud Credential Operator. Why is this important? It it generally necessary to provide a CA file to OpenStack clients in order to communicate with a cloud that uses self-signed certificates. The cloud-credential-operator syncs clouds.yaml files to various namespaces so that services running in those namespaces are able to communicate with the cloud, but it does not sync the CA file. Instead, this must be managed using another mechanism. This has led to some odd situations, such as the Cinder CSI driver operator inspecting cloud-provider configuration to pull out this file. We should start syncing not only the clouds.yaml file but also the CA file to anyone that requests it via a CredentialsRequest. Once we've done this, we can modify other components such as the Installer, CSI Driver Operator, Hypershift, and CCM Operator to pull the CA file from the same secrets that they pull the clouds.yaml from, rather than the litany of places they currently use. Scenarios As a deployer, I should be able to update all cloud credential-related information - including certificates - in one central place and see these rolled out to all components that require them. Acceptance Criteria The cloud-credential-operator is capable of consuming a CA cert from kube-system / openstack-credentials and rolling this out to the secrets in other namespaces The installer includes the CA cert in the root kube-system / openstack-credentials secret The UPI playbooks are modified to includes the CA cert in the root kube-system / openstack-credentials secret No regressions. Since we use self-signed certificates in many of our CI systems, we should see regressions early. Release notes and credential rotation documentation is updated to document this change Dependencies (internal and external) None. Previous Work (Optional): None. Open questions: None."
    },
    {
        "summary": "Add topology-awareness to Cinder CSI Driver",
        "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs to cover much of this and prevent regressions. We will need to manually test the negative case, where there is a mismatch between the set of Cinder AZs and set of Nova AZs, but this should be trivial to do. Open questions: None."
    },
    {
        "summary": "Own/SingleNamespace InstallMode Support",
        "description": ""
    },
    {
        "summary": "UPSTREAM Permission validation pre-flight check 988",
        "description": "From the WIP brieferror Permission Verification do the doc| Step 1 permission verification as well as escalate/bind checking Permissions and Validation Checks SelfSubjectRulesReview runner Testing unit test suites for each of the above two (2) new e2e for this work: happy path and common failure path"
    },
    {
        "summary": "UPSTREAM catalogd web interface performance improvements 451 TP",
        "description": "The RFC written for identified a desire to formalize the catalogd web API, and divided work into a set of v1.0-blocking changes to enable versioned web interfaces (phase 1( and non-blocking changes to express and extend a formalized API specification (phase 2). This epic is to track the design and implementation work associated with phase 2. During phase 1 RFC review we identified that we needed more work to capture the extensibility design but didn't want to slow progress on the v1.0 blocking changes so the first step should be an RFC to capture the design goals for phase 2, and then any implementation trackers we feel are necessary. Work here will be behind a feature gate. catalogd web api performance improvements RFC 1569 serve catalog content based on supplied parameters 1606 Downstreaming this feature We need to follow this guide to downstream this feature:"
    },
    {
        "summary": "EPIC - (Post-Monorepo Integration) - Optimize and Streamline Controller Operator and Catalogd",
        "description": "Note: This epic only tracks the phase one of the work listed in the RFC Consolidate catalogd and operator-controller Kustomize configs 1341 DOWNSTREAM: Changes in the kustomize configs might need downstream effort"
    },
    {
        "summary": "OLMv1 Downstream feature gate promotion mechanics",
        "description": "OCP/Telco Definition of Done Epic Goal To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time Why is this important? Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path or breaking customers. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will work both with TechPreviewNoUpgrade and without it but will have a different set of features. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Expose remaining Topology components and utils to openshift-console/dynamic-plugin-sdk",
        "description": "Description As a user, I want to use Topology components in the dynamic plugin Acceptance Criteria Should expose the below Topology components and utils to dynamic-plugin-sdk getModifyApplicationAction baseDataModelGetter getWorkloadResources contextMenuActions CreateConnector createConnectorCallback (e",
        "epic_key": "ODC-7716"
    },
    {
        "summary": "Merge Admin and Dev Perspectives",
        "description": "Epic Goal Base on user analytics many of customers switch back and fourth between perspectives, and average15 times per session. The following steps will be need: Surface all Dev specific Nav items in the Admin Console Disable the Dev perspective by default but allow admins to enable via console setting All quickstarts need to be updated to reflect the removal of the dev perspective Guided tour to show updated nav for merged perpspective Why is this important? We need to alleviate this pain point and improve the overall user experience for our users. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Automation enhancement for 4.19",
        "description": "Problem: This epic covers the scope of automation-related stories in ODC Goal: Automation enhancements for ODC Why is it important? Use cases: case Acceptance criteria: Automation enhancements as per the perspective merged Tests to be updated according to the default setting of having only Admin perspective Dependencies (External/Internal): Design Artifacts: Exploration: Note:"
    },
    {
        "summary": "OCP 4.20 - Console Dependencies & Tech Debt",
        "description": "Over time, our OpenShift Console has accumulated technical debt in its libraries, frameworks, and underlying infrastructure. This epic is focused on auditing, updating, and standardizing dependencies to the latest supported versions, while ensuring compatibility and minimizing user impact. Addressing this tech debt will: Reduce security vulnerabilities by patching known CVEs Improve performance through optimized libraries Streamline developer onboarding and maintenance Lay the groundwork for future features by aligning on current platform standards Goals & Objectives: Frontend Dependencies Audit third-party UI components (React, PatternFly, etc.) Upgrade to the latest stable major versions Refactor any deprecated APIs Backend Dependencies Update Go modules and middleware libraries Migrate from deprecated frameworks (if applicable) Ensure backward compatibility for REST/gRPC endpoints Infrastructure Harden CI/CD pipelines with up-to-date build agents Refresh container base images (e.g., Red Hat UBI versions) Align Kubernetes manifests with current API versions Acceptance Criteria: All frontend NPM package versions are updated to their latest non-breaking releases, with no failing unit or e2e tests. Backend Go modules have no outdated major versions; existing integration tests pass without regression. CI/CD pipeline definitions use current Docker image tags; all automated builds succeed in consumed staging clusters. No known high- or critical-severity vulnerabilities remain in project dependencies (scan report attached). Documentation updated to reflect new version requirements and rollback procedures."
    },
    {
        "summary": "Adopt PatternFly 6 and remove PatternFly 4",
        "description": "Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "OCP 4.19 - Console Dependencies & Tech Debt",
        "description": "An epic we can duplicate for each release to ensure we have a place to catch things we ought to be doing regularly but can tend to fall by the wayside."
    },
    {
        "summary": "Prepare to update OCP Console React dependency",
        "description": "Epic Goal Update the OCP Console frontend React dependency to a more recent version, as the current version has been end-of-life for over a year. Why is this important? The longer we wait to make this update, the harder it will be. It's important to stay current so that we can be more nimble with tech debt and dependencies. Scenarios As a developer, I am assigned a high-priority feature. I find that React must be updated as part of the feature. I also find that because there are many breaking changes between our version and the latest, an update would be out of scope for this story. The feature must be deferred until we can address the tech debt. A major issue is found in our current version of React, and an update is required. The scope of this update work has ballooned over the time since our last update. We have to drop other important work to prioritize this update and again, other important features or work are deferred. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. OCP Console React dependency and all other tangential dependencies have been updated to an agreed-upon recent version. Other stakeholders, like plugin consumers, should not be affected by this change. Previous Work (Optional): Open questions: Should this be accomplished as a swarm activity where we spend a sprint addressing all blockers? Should we take a cautious approach and resolve blockers over time until we reach a point where an update is feasible? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "OCP 4.19 - Address tech debt in frontend/public/components/secrets/create-secret.tsx",
        "description": "Epic Goal Migrate all components to functional components Remove all HOC patterns Break the file down into smaller files Improve type definitions Improve naming for better self-documentation Address any React anti-patterns like nested components, or mirroring props in state. Address issues with handling binary data Add unit tests to these components Acceptance Criteria Refactor secret forms Adding unit tests to these components. Fix Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Replace legacy custom components with PatternFly components",
        "description": "Epic Goal Goal is to locate and replace old custom components with PatternFly components. Why is this important? Custom components require supportive css to mimic the visual theme of PatternFly. Over time these supportive styles have grown and interspersed through the console codebase, which require ongoing efforts to carry along, update and maintain consistency across product areas and packages. Also, custom components can have varying behaviors that diverge from PatternFly components, causing bugs and create discordance across the product. Future PatternFly version upgrades will be more straightforward and require less work. Acceptance Criteria Identify custom components that have a PatternFly equivalent component. Create stories which will address those updates and fixes Update integration tests if necessary. Open questions: ..."
    },
    {
        "summary": "update or remove focus-trap-react",
        "description": "warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react@0.14.x ^15.0.0 ^16.0.0\". warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react-dom@0.14.x ^15.0.0 ^16.0.0\".",
        "epic_key": "CONSOLE-3945"
    },
    {
        "summary": "Add the ability to specify a second custom logo for PatternFly 6 api",
        "description": "In PatternFly 6, the colour of the masthead changes depending upon the mode (light vs dark). As a result, a single custom logo may not work for both cases (as is the case with the existing OKD and OpenShift logos as they assume the masthead background is always dark and include white text as a result). We need to add the ability to specify a second logo to account for this. See AC: propose chances writing an enhancement document which should cover the fullstack change - API, console-operator, console update console operator's config API with additional field for defining second custom logo for PF6 add unit tests for the API change",
        "epic_key": "CONSOLE-4325"
    },
    {
        "summary": "Update to NodeJS v22",
        "description": "Current version of NodeJS is in the maintainance mode and we need to update to the next version with long term support which is currently NodeJS v22. Acceptance Criteria: update console builder image update demo-dynamic-plugin base image fix any related build issues",
        "epic_key": "CONSOLE-4562"
    },
    {
        "summary": "update or remove react-tagsinput",
        "description": "warning \" react-tagsinput@3.19.0\" has incorrect peer dependency \"react@^16.0.0 ^15.0.0 ^0.14.0\".",
        "epic_key": "CONSOLE-3945"
    },
    {
        "summary": "Update login page",
        "epic_key": "CONSOLE-4325"
    },
    {
        "summary": "Address tech debt in KeyValueEntryForm component",
        "description": "The KeyValueEntryForm component needs to be refactored to address several tech debt issues: Rename to OpaqueSecretFormEntry Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
        "epic_key": "CONSOLE-4352"
    },
    {
        "summary": "Tech Preview Support BYOPKI for image verification in OCP",
        "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Remove Cgroup v1 from OCP in 4.19",
        "description": "OCP/Telco Definition of Done Epic Goal Remove the support for cgroup v1 in 4.19 Why is this important? Without dependant components like systemd, RHCOS moving away from cgroups v1 it is important for the node to make this move as well. Scenarios As a system administrator I would like to make sure my cluster doesn't use cgroup v1 from 4.19 onwards Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GA User Namespaces",
        "description": "Epic Goal Prepare user namespaces for GA by enhancing SCC support and testing Why is this important? Enable nested containers use cases and enhance security Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Move ClusterImagePolicy, ImagePolicy to v1",
        "description": "OCP/Telco Definition of Done Epic Goal This epic tracks the work needed to move the ClusterImagePolicy API from v1alpha1 to v1 in OCP 4.19. It does not include any new feature requests, which will be tracked by other epics-just the API upgrade process, CI jobs and related tasks. The workflow to move to GA: v1 types in o/api client-go to generate v1 Add v1 manifest to payload and update MCO to v1 APIs update featuregate to default enabled add the v1 manifest to the payload remove the v1alpha1 manifest from the payload Wait 1-2 weeks - feature promotion in o/api And do those all in order. For 3 here, those PRs that need to be simul-merged Why is this important? Moving the ClusterImagePolicy API to a stable version v1 and announcing that OpenShift now supports Sigstore verification are key steps in helping customers strengthen their software supply chain. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Dev Preview Support BYOPKI for image verification in OCP",
        "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "MAPI/CAPI Feature Parity (AWS) (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "MAPI/CAPI Feature Parity (Core) (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Implement Migration core for MAPI to CAPI (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal Create the core/common tooling needed to enable the migration designed in OCPCLOUD-1578 To allow providers to individually migrate from MAPI to CAPI Implementation plan in Why is this important? We need to build out the core so that development of the migration for individual providers can then happen in parallel Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Update autoscaling annotations to accommodate upstream keys",
        "description": "Epic Goal Update the scale from zero autoscaling annotations on MachineSets to conform with the upstream keys, while also continuing to accept the openshift specific keys that we have been using. Why is this important? This change makes our implementation of the cluster autoscaler conform to the API that is described in the upstream community. This reduces the mental overhead for someone that knows kubernetes but is new to openshift. This change also reduces the maintenance burden that we carry in the form of addition patches to the cluster autoscaler. By changing our controllers to understand the upstream annotations we are able to remove extra patches on our fork of the cluster autoscaler, making future maintenance easier and closer to the upstream source. Scenarios A user is debugging a cluster autoscaler issue by examining the related MachineSet objects, they see the scale from zero annotations and recognize them from the project documentation and from upstream discussions. The result is that the user is more easily able to find common issues and advice from the upstream community. An openshift maintainer is updating the cluster autoscaler for a new version of kubernetes, because the openshift controllers understand the upstream annotations, the maintainer does not need to carry or modify a patch to support multiple varieties of annotation. This in turn makes the task of updating the autoscaler simpler and reduces burden on the maintainer. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Scale from zero autoscaling must continue to work with both the old openshift annotations and the newer upstream annotations. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - OpenShift code and tests merged: link to meaningful PR or GitHub Issue DEV - OpenShift documentation merged: link to meaningful PR or GitHub Issue DEV - OpenShift build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - OpenShift documentation merged: link to meaningful PR please note, the changes described by this epic will happen in OpenShift controllers and as such there is no \"upstream\" relationship in the same sense as the Kubernetes-based controllers."
    },
    {
        "summary": "(Infrastructure) Cluster generation for Cluster API platforms",
        "description": "OCP/Telco Definition of Done Epic Goal To add support for generating Cluster and Infrastructure Cluster resources on Cluster API based clusters Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Rebasebot: lifecycle hooks",
        "description": "Epic Goal Rebasebot| needs to support customizable system for running repository specific tooling before/during/afeter rebase. This is primarily required for automatic rebases of CAPI provider repositories. Other uses for this feature are expected. Why is this important? Further automation of our rebase process will allow us to focus more on development instead of maintenance. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GA Ability to assign custom name formats to Control Plane Machines via CPMS",
        "description": "OCP/Telco Definition of Done Epic Goal Placeholder to track GA activities for OAPE-16 feature. Why is this important? ... Scenarios ... Acceptance Criteria Moving the feature to default feature set. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GA Support router to load secrets",
        "description": "Placeholder to track GA work for CFE-811"
    },
    {
        "summary": "TP Ability to assign custom name formats to Control Plane Machines via CPMS",
        "description": "Epic Goal Provide a new field to the CPMS that allows to define a Machine name prefix This prefix will supersede the current usage of the control plane label and role combination we use today The names must still continue to be suffixed with chars-idx as this is important to the operation of CPMS Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. DEV - Downstream code and tests merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Console plugin configuration and status panel",
        "description": "Epic Goal Create a new view in netobserv console plugin to configure FlowCollector and get some status information. This must address some of the limitations related to the generic OLM install form, which lacks of flexibility in organizing the UI elements to offer a good UX. This may also include a resource footprint calculator that would help users taking more informed decisions before installing the FlowCollector (nice to have - might be a follow-up and/or a separate tool) Finally, once the FlowCollector is installed, some basic status information will be provided such as: Current FC status (components readiness etc.) FC Status warnings (same warnings as in the validation webhook) Extra warnings / recommendations, such as recommending to install Kafka depending on the number of nodes LokiStack status / readiness when relevant A FlowMetrics section with list of installed metrics + estimated cardinality + actual cardinality Links to netobserv dashboards Why is this important? Make it easier, less intimidating, to configure: despite installation being well documented, having a good UX for guiding should make users happier. Current OLM form is too limited. Also, all the options offered are often seen as intimidating. Remove uncertainty before installation: many users are afraid of the potential cost in resource footprint and want to anticipate it. A calculator will help reduce/remove the uncertainty. Increase visibility of issues: some issues might remain unnoticed, such as configuration warnings or high metrics cardinality. We want to make them more visible."
    },
    {
        "summary": "Operator and Origin E2E tests for the gateway controller and CRD life-cycle management e2e testing automation",
        "description": "Use cases: As a developer I would like to test for unacceptable failures that exist in the Gateway API with Ingress product. This Epic is a place holder for stories regarding e2e and unit tests that are missing for old features and to determine whether OSSM 3.x TP2 bugs affect us before they are fixed in GA. There is already one epic for DNS and test cases should be added for any new features in the release. Write and run test cases that are currently missing."
    },
    {
        "summary": "Gateway controller implementation",
        "description": "Epic Goal Add Gateway API via Istio Gateway implementation as GA in future release Problem: As an administrator, I would like to securely expose cluster resources to remote clients and services while providing a self-service experience to application developers. GA: A feature is implemented as GA so that developers can issue an update to the Tech Preview MVP and: can no longer change APIs without following a deprecating or backwards compatibility process. are required to fix bugs customers uncover must support upgrading the cluster and your component provide docs provide education to CEE about the feature must also follow Red Hat's support policy for GA Why is this important? Reduces the burden on Red Hat developers to maintain IngressController and Route custom resources Brings OpenShift ingress configuration more in line with standard Kubernetes APIs Demonstrates Red Hat\u2019s leadership in the Kubernetes community. Scenarios ... Acceptance Criteria Gateway API and Istio Gateway are in an acceptable standing for GA Istio Gateway installation without sidecars enabled Decision completed on whether a new operator is required, especially for upgrade and status reports Decision completed on whether Ingress-Gateway (or Route-Gateway) translation is needed Enhancement Proposals, Migration details, Tech Enablement, and other input for QA and Docs as needed API server integration, Installation, CI, E2E tests, Upgrade details, Telemetry as needed TBD Dependencies (internal and external) OSSM release schedule aligned with OpenShift's cadence, or workaround designed ...tbd Previous Work (Optional): Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Bump to OSSM 3.0.0",
        "description": "Epic Goal The logic in cluster-ingress-operator that installs and configures OSSM 2.y should be updated to install and configure OSSM 3.y. Why is this important? The GA release of the OpenShift Gateway API feature will be based on OSSM 3.y. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (+) Priority is set by engineering. - (+) Epic must be Linked to a Parent Feature. - (-) Target version+ must be set. - (+) Assignee must be set. - (+) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown. - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) 1. OSSM 3.0 (currently in Tech Preview). Previous Work 1. NE-1105. Open questions 1. Will any further changes be required between OSSM 3.0 Tech Preview and OSSM 3.0 GA? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "CRD Lifecycle Management for Gateway API",
        "description": "Overview Gateway API in upstream Kubernetes. OpenShift Service Mesh (OSSM) Microshift and OpenShift AI OCP will be fully in charge of managing the life-cycle of the Gateway API enacts a process called \"CRD Management Succession\" to ensure the transfer of control occurs safely, which includes multiple pre-upgrade checks and CIO startup checks. Acceptance Criteria If not present the Gateway API CRDs should be deployed at the install-time of a cluster, and management thereafter handled by the platform Any existing CRDs not managed by the platform should be removed, or management and control transferred to the platform Only the platform can manage or make any changes to the Gateway API CRDs, others will be blocked Documentation about these APIs, and the process to upgrade to a version where they are being managed needs to be provided Cross-Team Coordination The organization as a whole needs to be made aware of this as new projects will continue to pop up with Gateway API support over the years. This includes (but is not limited to) OSSM Team (Istio) Connectivity Link Team (Kuadrant) MicroShift Team OpenShift AI Team (KServe) Importantly our cluster infrastructure work with Cluster API (CAPI) OCPCLOUD-2114 Investigate lifecycle of Cluster API APIs within OpenShift"
    },
    {
        "summary": "Pin OSSM Subscription to a Compatible Version for Gateway API Support",
        "description": "Template: Networking Definition of Planned Epic Goal: Guarantee a compatible OSSM version is installed for Gateway API Why is this important? The ingress operator manages OSSM operator and Istio configuration for Gateway API support, but these resources can change in future releases in potentially incompatible ways. In order to guarantee that the OSSM and Istio versions are compatible with the ingress operator in a given OpenShift release, the ingress operator should install a known good version, rather than the latest. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Needs to be OSSM 3.x.x+ so that we're more up to date with upstream Istio ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. NE-1326: Investigate OSSM subscription so we can minimize surprise interoperability issues Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Tech Debt Fix OWNERS files in openshift/origin",
        "description": "Add a NID alias to OWNERS_ALIASES and update the OWNERS file in test/extended/router and add OWNERS file to test/extended/dns"
    },
    {
        "summary": "Enable the dynamic config manager",
        "description": "Goal To make the current implementation of the HAProxy config manager should not be used to reduce the impact of the feature. - Limit dynamic server allocation -- Set the maximum number of dynamic servers| to a minimal value to prevent high resource consumption. - Provide customer opt-out -- Offer customers a handler to opt out of the default config manager implementation."
    },
    {
        "summary": "Operator E2E tests for gateway DNS management",
        "description": "Use cases: As a customer I would like to understand how and when DNS records are created for my Gateway API resources. As a developer I would like to fix any issues that are not acceptable, or document those that cannot be resolved. See Enhancement Proposal - new controller to manage dns records for gateway listeners and Write and run unit test cases to find answers to the following about the current Gateway API DNS reconciliation: Does it work with multiple Gateways? Create multiple Gateways that have listeners with same hostname, as well as with differing hostnames and test DNS flow. What happens for a Gateway with Listeners that don't have a hostname? -Does it ignore Services that are not associated with the Gateway controller?- What happens if a Gateway listener tries to claim a name (abc.apps.example.com) that would match a .apps.example.com ingress controller wildcard. Which endpoint tries to serves the request to abc.apps.example.com? Other important factors TBD. Acceptance Criteria: new unit test cases, documentation, and update to enhancement document if needed."
    },
    {
        "summary": "Update components to use Kubernetes 1.32 packages",
        "description": "Epic Goal Bump vendored Kubernetes packages (k8s.io/api, k8s.io/apimachinery, k8s.io/client-go, etc.) to v0.32.0 or newer version. Why is this important? Keep vendored packages up to date. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to ToDo status - (+) Priority+ is set by engineering - (x) Epic must be Linked to a Parent Feature - (-) Target version must be set - (+) Assignee must be set - (x) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated -Release Technical Enablement - Provide necessary release enablement details and documents.- Dependencies (internal and external) 1. Other vendored dependencies (such as openshift/api and controller-runtime) may also need to be updated to Kubernetes 1.32. Previous Work (Optional) 1. NE-1875. Open questions None. Done Checklist CI - CI is running, tests are automated and merged. -Release Enablement link to Feature Enablement Presentation- DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue -DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue- DEV - Downstream build attached to advisory: link to errata -QE - Test plans in Polarion: link or reference to Polarion- -QE - Automated tests merged: link or reference to automated tests- -DOC - Downstream documentation merged: link to meaningful PR-"
    },
    {
        "summary": "Graduate \"GatewayAPI\" featuregate to TechPreviewNoUpgrade",
        "description": "As a developer, I need a featuregate to develop behind so that the Gateway API work does not impact other development teams until tests pass. The featuregate is currently in the DevPreviewNoUpgrade featureset. We need to graduate it to the TechPreviewNoUpgrade featureset to give us more CI signal and testing. Ultimately the featuregate needs to graduate to GA (default on) once tests pass so that the feature can GA. See also: - - -"
    },
    {
        "summary": "NetEdge - Maintainability and Debugability & Tech Backlog",
        "description": "tldr: three basic claims, the rest is explanation and one example We cannot improve long term maintainability solely by fixing bugs. Teams should be asked to produce designs for improving maintainability/debugability. Specific maintenance items (or investigation of maintenance items), should be placed into planning as peer to PM requests and explicitly prioritized against them. While bugs are an important metric, fixing bugs is different than investing in maintainability and debugability. Investing in fixing bugs will help alleviate immediate problems, but doesn't improve the ability to address future problems. You (may) get a code base with fewer bugs, but when you add a new feature, it will still be hard to debug problems and interactions. This pushes a code base towards stagnation where it gets harder and harder to add features. One alternative is to ask teams to produce ideas for how they would improve future maintainability and debugability instead of focusing on immediate bugs. This would produce designs that make problem determination, bug resolution, and future feature additions faster over time. I have a concrete example of one such outcome of focusing on bugs vs quality. We have resolved many bugs about communication failures with ingress by finding problems with point-to-point network communication. We have fixed the individual bugs, but have not improved the code for future debugging. In so doing, we chase many hard to diagnose problem across the stack. The alternative is to create a point-to-point network connectivity capability. this would immediately improve bug resolution and stability (detection) for kuryr, ovs, legacy sdn, network-edge, kube-apiserver, openshift-apiserver, authentication, and console. Bug fixing does not produce the same impact. We need more investment in our future selves. Saying, \"teams should reserve this\" doesn't seem to be universally effective. Perhaps an approach that directly asks for designs and impacts and then follows up by placing the items directly in planning and prioritizing against PM feature requests would give teams the confidence to invest in these areas and give broad exposure to systemic problems. ---- Relevant links: Documentation: Edge Diagnostics Scratchpad the SDN team's diagnostic guide. Linux Performance OpenShift Router Reload Technical Overview on Access. How to collect worker metrics to troubleshoot CPU load, memory pressure and interrupt issues and networking on worker nodes in OCP 4 on Mojo, results from OpenShift scalability testing. Scalability and performance OCP 3.11 documentation about the manual performance configuration that was possible in OCP 3. Timing web requests with cURL and Chrome some useful tcpdump commands. OpenShift SDN - Networking design document for improved status condition reporting. Observability tips for HAProxy analysis using tshark. The PCP Book: A Complete Documentation of Performance Co-Pilot brief guide to using SystemTap on RHCOS. Troubleshooting throughput issues Red Hat Enterprise Linux Network Performance Tuning Guide (PDF) a diagnostic built into the kube-apiserver operator. Diagnostic tools: dropwatch to check NIC configuration. iovisor/bcc: BCC - Tools for BPF-based Linux IO analysis, networking, monitoring, and more to gather timing information about HTTP/HTTPS connections. route-monitor a programmable packet generator. OpenTracing node-problem-detector by Brendan Gregg. DTrace SystemTap cheatsheet (PDF) kubectl plugin for tcpdump & Wireshark. ironcladlou/ditm network diagnostic and visualization tool. ali a general stress-loading tool (CPU, filesystem, network, ...). mb is an example of diagnosing DNS latency/timeouts. BZ1829779 Investigation is an example of diagnosing misconfigured DNS for an external LB. Debugging network stalls on Kubernetes| from the GitHub Blog, about diagnosing Kubernetes performance issues related to ksoftirqd."
    },
    {
        "summary": "Prometheus 3 integration",
        "description": "Ref:"
    },
    {
        "summary": "Metrics Server Post GA 2",
        "description": "This epic is to track stories that are not completed in MON-3865"
    },
    {
        "summary": "GA Machine Config Node",
        "description": "This epic describes the work required to GA a minimal viable version of the Machine Config Node feature to enable the subsequent GAing of the Pinned Image Sets feature. The GAing of status reporting as well as any further enhancements for the Machine Config Node feature will be tracked in MCO-1506. Related Items: Original MCN/State Reporting Enhancement Pinned Image Sets Enhancement 4.19 backport SBAR| Doc work tracked in OSDOCS-14404 Done when: MCN API is GAed MCN functionality is consistent across all MCPs (default & custom) and both clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing The team is confident that the state of MCN is robust enough to support the GAing of Pinned Image Sets"
    },
    {
        "summary": "Actionable Error Messaging",
        "description": "The error propagation is generally speaking not 1-to-1. The operator status will generally capture the pool status, but the full error from Controller/Daemon does not fully bubble up to pool/operator, and the journal logs with error generally don\u2019t get bubbled up at all. This is very confusing for customers/admins working with the MCO without full understanding of the MCO\u2019s internal mechanics: The real error is hard to find The error message is often generic and ambiguous The solution/workaround is not clear at all Using \"unexpected on-disk state\" as an example, this can be caused by any amount of the following: An incomplete update happened, and something rebooted the node The node upgrade was successful until rpm-ostree, which failed and atomically rolled back The user modified something manually Another operator modified something manually Some other service/network manager overwrote something MCO writes Etc. etc. Since error use cases are wide and varied, there are many improvements we can perform for each individual error state. This epic aims to propose targeted improvements to error messaging and propagation specifically. The goals being: De-ambigufying different error cases with the same message Adding more error catching, including journal logs and rpm-ostree errors Propagating full error messages further up the stack, up to the operator status in a clear manner Adding actionable fix/information messages alongside the error message With a side objective of observability, including reporting all the way to the operator status items such as: Reporting the status of all pools Pointing out current status of update/upgrade per pool What the update/upgrade is blocking on How to unblock the upgrade Approaches can include: Better error messaging starting with common error cases De-ambigufying config mismatch Capturing rpm-ostree logs from previous boot, in case of osimageurl mismatch errors Capturing full daemon error message back to pool/operator status Adding a new field to the MCO operator spec, that attempts to suggest fixes or where to look next, when an error occurs Adding better alerting messages for MCO errors Options"
    },
    {
        "summary": "Opt-out updated bootimage for GCP and AWS",
        "description": "This epic will encompass work required to switch boot image updates on GCP to be opt-out."
    },
    {
        "summary": "Bump ignition to spec 3.5",
        "description": "Once ignition spec 3.5 stablizes, we should switch to using spec 3.5 as the default in the MCO to enable additional features in RHCOS. (example: needs 3.5)"
    },
    {
        "summary": "GA Pin and pre-load images",
        "description": "This epic describes the work required to GA the Pinned Image Sets feature. Related Documentation: Pinned Image Sets Enhancement Design Review Document Comment with helpful info on how to use PIS| Done when: Pinned Image Set API is GAed (here is the API in tech preview: ) Pinned Image Set functionality is consistent for clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing e2e testing: create PIS for custom pool and run garbage collection and check if PIS remain on node add PIS to custom pool and check if they have been successfully added add PIS to standard pool and check if they have been successfully added add invalid PIS and check if MCN has degraded in standard pool add invalid PIS and check if MCN has degraded in custom pool"
    },
    {
        "summary": "On Cluster Layering Disconnected Support",
        "description": "Post GA On Cluster Build Enhancement work"
    },
    {
        "summary": "Manage the MCS ignition-ca cert",
        "description": "Spun out of This aims to capture the work required to rotate the MCS-ignition CA + cert. Original description copied from MCO-668: Today in OCP there is a TLS certificate generated by the installer | which is called \"root-ca\" but is really \"the MCS CA\". A key derived from this is injected into the pointer Ignition configuration under the \"security.tls.certificateAuthorities\" section, and this is how the client verifies it's talking to the expected server. If this key expires (and by default the CA has a 10 year lifetime), newly scaled up nodes will fail in Ignition (and fail to join the cluster). The MCO should take over management of this cert, and the corresponding user-data secret field, to implement rotation. Reading: - There is a section in the customer facing documentation that touches on this: - There's a section in the customer facing documentation for this: that needs updating for clarification. - There's a pending PR to openshift/api: - Also see old (related) bug: - This is also separate to which describes the management of kubelet certs"
    },
    {
        "summary": "Tech debt 4.18",
        "description": "These are items that the team has prioritized to address in 4.18."
    },
    {
        "summary": "On-Cluster Layering - upgrades and integrations",
        "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
    },
    {
        "summary": "On-Cluster Layering GA",
        "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
    },
    {
        "summary": "On Cluster Layering: Address Image Pruning",
        "description": "Done When: We implement a solution that mitigates the node disruptions when applying Machine Configs. Notes: MVP: when a MachineOSBuild object is deleted, the corresponding image in the registry should also be deleted, whether internal or external Mid term: have good documentation around user-driven prunes, and what can/cannot be pruned Mid term: have an opt-in/opt-out mechanism for auto pruning, where builds not in use and x versions old are deleted automatically Nice to have: tag latest (and maybe latest-1) builds for pools, so the user can easily refer to the latest build and know what not to prune Nice to have: have user-pruned images automatically reflect back on the MachineOSBuild Long term: user defined prune logic"
    },
    {
        "summary": "Update MCO dependencies to Kubernetes 1.32",
        "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that MCO uses to v1.29 which will keep it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? Uncover any possible issues with the openshift/kubernetes rebase before it merges. MCO continues using the latest kubernetes/OpenShift libraries and the kubelet, kube-proxy components. MCO e2e CI jobs pass on each of the supported platform with the updated components. Acceptance Criteria All stories in this epic must be completed. Go version is upgraded for MCO components. CI is running successfully with the upgraded components against the 4.18/master branch. Dependencies (internal and external) ART team creating the go 1.31 image for upgrade to go 1.31. OpenShift/kubernetes repository downstream rebase PR merge. Open questions: Do we need a checklist for future upgrades as an outcome of this epic?- yes, updated below. Done Checklist Step 1 - Upgrade go version to match rest of the OpenShift and Kubernetes upgraded components. Step 2 - Upgrade Kubernetes client and controller-runtime dependencies (can be done in parallel with step 3) Step 3 - Upgrade OpenShift client and API dependencies Step 4 - Update kubelet and kube-proxy submodules in MCO repository Step 5 - CI is running successfully with the upgraded components and libraries against the master branch."
    },
    {
        "summary": "General \"tech debt\" items that don't have a home yet",
        "description": "Background This is intended to be a place to capture general \"tech debt\" items so they don't get lost. I very much doubt that this will ever get completed as a feature, but that's okay, the desire is more that stores get pulled out of here and put with feature work \"opportunistically\" when it makes sense. Goal If you find a \"tech debt\" item, and it doesn't have an obvious home with something else (e.g. with MCO-1 if it's metrics and alerting) then put it here, and we can start splitting these out/marrying them up with other epics when it makes sense."
    },
    {
        "summary": "Support for new AWS regions without code changes",
        "description": "Epic Goal Streamline onboarding of new AWS regions in OCP managed services like ROSA or OSD by removing the need for code changes in the cluster image registry operator Why is this important? Every time AWS is launching a new region the deployment of OpenShift will fail with the Cluster Image Registry Operator failing to reconcile until the newly added region is hard coded into the operator's controller This blocks installer and QE teams from development, testing and rollout of new region support for OCP managed services on AWS Disabling the integrated registry is currently not supported by many of our managed services Acceptance Criteria A new AWS region should not require any code changes to CIRO for the operator to reconcile the request for required cloud infrastructure to run the image registry Backports to all currently supported OCP versions Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Implement automated machine approval for karpenter instances",
        "description": "Goal Instances created by karpenter can automatically become Nodes Why is this important? Reduce operational burden. Scenarios For CAPI/MAPI driven machine management the cluster-machine-approver uses the machine.status.ips to match the CSRs. In karpenter there's no Machine resources We'll need to implement something similar. Some ideas: - Explore using the nodeClaim resource info like status.providerID to match the CSRs - Store the requesting IP when the ec2 instances query ignition and follow similar comparison criteria than machine approver to match CSRs - Query AWS to get info and compare info to match CSRs ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Add a Mechanism to Label all Pods in the Control Plane Namespace",
        "description": "Goal Hypershift has a mechanism for Labeling Control Plane Pods Cluster service should be able to set the label for a given hosted cluster Why is this important? As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. See Documentation: Scenarios Given a subscriptionID of \"1d3378d3-5a3f-4712-85a1-2485495dfc4b\", there needs to be the following label on all pods hosted on behalf of the customer: {code:yaml} kubernetes.azure.com/managedby: sub_1d3378d3-5a3f-4712-85a1-2485495dfc4b{code} Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Rebase openshift/etcd to 3.5.19",
        "description": "rebase etcd to 3.5.19"
    },
    {
        "summary": "Rebase openshift/etcd to 3.5.18",
        "description": "rebase etcd to 3.5.18"
    },
    {
        "summary": "Rebase openshift/etcd to 3.5.21",
        "description": "rebase etcd to 3.5.21"
    },
    {
        "summary": "Static validations",
        "description": "system-assigned identity cannot be set on compute nodes user-assigned identity on compute nodes must == 1 assigned identity must match type assigned identity name is a valid guid",
        "epic_key": "CORS-3883"
    },
    {
        "summary": "Place ingress LBs on specific subnets",
        "description": "User Story: The installer will apply the specified IngressControllerLB subnets to the default IngressController's spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.classicLoadBalancer.subnets or ...networkLoadBalancer.subnets field (based on platform.aws.lbType) in the manifest generated by the generateDefaultIngressController function. Acceptance Criteria: Description of criteria: Ingress LB is placed on specific subnet(s) as specified by user (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: For sanity checking, you can run openshift-install create manifests and check the cluster-ingress-.yaml manifests If no roles are specified in the installconfig, no subnets are supplied in the manifest (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
    },
    {
        "summary": "Place machines only in clusternode subnets (and bootstrap node in bootstrap subnet)",
        "description": "User Story: Machines should only be placed in subnets specified as clusternode. Bootstrap node should be on subnet with bootstrap node role. Acceptance Criteria: Description of criteria: See above (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: This needs to be specified in both the CAPI-created machines Better link for MAPI machines: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
    },
    {
        "summary": "Deprecate installconfig.platform.aws.subnets",
        "description": "User Story: As an openshift-install user I want to be able to continue to use aws.subnets during deprecation (a warning will show) As an openshift developer, I only want a single code path for subnets (via upconversion) Acceptance Criteria: Description of criteria: Validation that both fields are not simultaneously specified When aws.subnets is specified, it's upconverted into aws.vpc.subnets Existing pkg/types/aws/Subnets type is renamed to DeprecatedSubnets Remove all (or as many possible) usages of DeprecatedSubnets, replaced with the new vpc.Subnets field We may need to keep usage of DeprecatedSubnets for certain validations Warning when using deprecated field (optional) Out of Scope: . Engineering Details: Conversion package: Review how subnets are used in and whether any changes/refactoring is needed",
        "epic_key": "CORS-3440"
    },
    {
        "summary": "Integrate CAPZ changes",
        "description": "Once CAPZ changes are integrated into upstream or our fork, we need to vendor those to the installer.",
        "epic_key": "CORS-3272"
    },
    {
        "summary": "Refactor resource group creation/reconciliation to be handled by CAPZ",
        "description": "Currently RG creation is handled by the installer SDK. It can and should be handled by CAPZ so that we have less code to maintain and do not need to handle separate configurations for Azure & Azure Stack",
        "epic_key": "CORS-3272"
    },
    {
        "summary": "Add azure disk nvme controller support",
        "description": "User Story: As a (user persona), I want to be able to: Capability 1 Capability 2 Capability 3 so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3771"
    },
    {
        "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "OpenShift Installer to support Private Google Access to GCP endpoints",
        "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
    },
    {
        "summary": "Add ability to choose ingress controller subnets at installation",
        "description": "OCP/Telco Definition of Done Epic Goal Add the ability to choose subnets for IngressControllers with LoadBalancer-type Services for AWS in the Installer. This install config should be applies to the default IngressController and all future IngressControllers (the design is similar to installconfig.platform.aws.lbtype Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Provision Azure Stack Infra with CAPI",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Hybrid SRE: Remove ARO build-flag in openshift-installer",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Hybrid SRE: Add support to enable boot diagnostics option at installation time in Azure",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Technical debt for 4.19",
        "description": "Epic Goal This epic includes tasks the team would like to tackle to improve our process, QOL, CI. It may include tasks like updating the RHEL base image and vendored assisted-service. Why is this important? We need a place to add tasks that are not feature oriented. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Azure - Add support for Dxv6 machine series",
        "description": "Epic Goal Dlsv6 Dsv6 Why is this important? ARO will need to support Dxv6 instance types supported. These are currently in preview Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "origin: use l2bridge binding for virt suite",
        "description": "We're still using the passt binding, which prevents us from checking the TCP connection persistence. We should start using the correct binding - l2bridge - which will allow us to QE the real binding our customers are using, and will also allow us to improve our test coverage, since we would be able to align w/ our upstream tests.",
        "epic_key": "CORENET-5649"
    },
    {
        "summary": "API Port CUDN e2e tests to openshift/origin",
        "description": "Port e2e tests of CUDN introduced on U/S by",
        "epic_key": "CORENET-4931"
    },
    {
        "summary": "Add placeholder GA tests for persistent IPs feature on openshift conformance tests"
    },
    {
        "summary": "Fix security issues with CNO IPSec certificate signing",
        "description": "In CNO we have an approver that signs certs automatically, without checking any identity information. We should modify this to require that the certificate request contains the kubelet certificate (issued separately) to ensure the identity of the client is an openshift node. We should not just hand out certificates to anyone who asks for them.",
        "epic_key": "CORENET-5361"
    },
    {
        "summary": "UDN API Make IPAM options for explicit in the API; cater to common use cases",
        "description": "See the UDN Sync Meeting notes: In our current UDN API, subnets field is mandatory always for primary role and optional for secondary role. This is because users are allowed to have a pure L2 without subnets for secondary networks. However, in the future if we want to add egress support on secondary networks, we might need subnets... CNV has many different use cases: For UDPNs, we always need subnets for L2 and L3 why not make them optional and let users get default values? - drawback is loosing visibility and this podsubnet now conflicting with other internal subnets and customer in their ignorange have the oopsy stage, we have seen this in plenty with joinsubnets already For UDSNs, we may or maynot have the need for IPAM, today this subnets field is optional, but then when we do need subnets we cannot set default values here so its icky. This card tracks the design changes to the API and the code changes needed to implement this. See for details.",
        "epic_key": "CORENET-4931"
    },
    {
        "summary": "openshift/origin: only provision workloads when network creation has started"
    },
    {
        "summary": "Enable support to enable OVN-Kubernetes BGP in CNO",
        "description": "CNO should deploy the new RouteAdvertisements OVN-K CRD. When the OCP API flag to enable BGP support in the cluster is set, CNO should enable support on OVN-K through a CLI arg.",
        "epic_key": "CORENET-4947"
    },
    {
        "summary": "Make CNO to react for Machine Config Pool status",
        "description": "The CNO rolls out ipsec mc plugin for rolling out IPsec for the cluster, but it doesn't really check master and work role machine config pools status to confirm if that's successfully installed in the cluster nodes. Hence CNO should be made to listen for MachineConfigPool status object updates and set network operator condition accordingly based on ipsec mc plugin rollout status.",
        "epic_key": "CORENET-5361"
    },
    {
        "summary": "L2 NetworkPolicy Support NetworkPolicies on Primary UDNs",
        "description": "We want to do Network Policies not MultiNetwork POlicies",
        "epic_key": "CORENET-4931"
    },
    {
        "summary": "Improve ipsec tests",
        "epic_key": "CORENET-5361"
    },
    {
        "summary": "Whereabouts Downstream Merge",
        "description": "DS merge for fast range fixes and dep bumps"
    },
    {
        "summary": "Whereabouts fast ranges: Get fast ranges working in the CNO",
        "description": "The CNO should deploy the fast ranges CRDs We need to add this CRD And it'll be deployed with the CNO in here: Goal: Enable the perf/scale team to use Whereabouts fast ranges (as well as other interested parties, such as telco)",
        "epic_key": "CORENET-664"
    },
    {
        "summary": "CNCC 1.32 Kube Rebase",
        "epic_key": "CORENET-5635"
    },
    {
        "summary": "SGW Add support for Layer-2 UDNs",
        "description": "The main difficulty of supporting L2 UDN is not having a node-specific pod network subnet to advertise with that node as next hop: L2 UDNs subnet is cluster wide. One of the ideas was to advertise /32 pod specific routes but there are concerns on the scalability of that. The other idea is to advertise the whole L2 UDN subnet with the selected nodes as next hop and let multi-path take care of the rest. With this alternative there is acceptance that this might not always route the traffic on the most optimum path. This effort entails adding support for it in cluster manager route advertisement controller, and mimic the existing support of L3 zone and node network controllers for L2 as well. This includes upstream testcases that should basically mimic L3 existing test cases which add some level of dependency with SDN-5712.",
        "epic_key": "CORENET-5350"
    },
    {
        "summary": "CNO 1.32 Kube rebase",
        "epic_key": "CORENET-5635"
    },
    {
        "summary": "CNO: update MNP CRD",
        "epic_key": "CORENET-5645"
    },
    {
        "summary": "Add ipsec upgrade ci job as mandatory lane",
        "description": "The e2e-aws-ovn-ipsec-upgrade job is currently an optional job and always_run: false because the job not reliable and success rate is so low. This must be made as mandatory CI lane after fixing its relevant issues.",
        "epic_key": "CORENET-5361"
    },
    {
        "summary": "Set the NetworkSegmentation FG on persistent IPs conformance tests using primary UDNs"
    },
    {
        "summary": "Downstream e2e CI tests for PodNetwork Advertisement",
        "epic_key": "CORENET-5350"
    },
    {
        "summary": "monitoringL2/L3 Open default network ports on UDN pods via users's request through pod annotations",
        "epic_key": "CORENET-4931"
    },
    {
        "summary": "Whereabouts perf scale: Re-enable opt-in for the node slice controller.",
        "description": "We made a compromise to automatically enable this for now in order to move forward with perf/scale testing as a short term item. While this shouldn't have a large impact on cluster performance in general (even when installed without use), it should still be optional to use. See also:",
        "epic_key": "CORENET-664"
    },
    {
        "summary": "temp logging change for debugging release",
        "description": "temp for bot"
    },
    {
        "summary": "Whereabouts Downstream Merge"
    },
    {
        "summary": "Universal connectivity: Localnet 4.19",
        "description": "Template: Networking Definition of Planned Epic Goal Provide quality user experience for customers connecting their Pods and VMs to the underlying physical network through OVN Kubernetes localnet. Why is this important? This is a continuation to It covers the UDN API for localnet and other improvements Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (/) Priority+ is set by engineering - (/) +Epic must be Linked to a +Parent Feature+ - (/) Target version+ must be set - (/) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated This must be done downstream too Release Technical Enablement - Provide necessary release enablement details and documents. OVN Kubernetes secondary networks with the localnet topology can be created through ClusterUserDefinedNetworks When possible, user input is validated and any configuration issue is shown on the UDN. Alternatively some issues can be shown on CNI ADD events on Pod -Definition of these networks can be changed even if there are Pods connected to them. When that happens, the UDN is marked as degraded until all the \"old\" pods are gone. The mutable fields should be: MTU, VLAN, physnet name- For cases where a user incorrectly set their MTU, VLAN, or physnet name, there is a clear and foolproof flow describing how to correct this mistake. A single \"bridge-mappings\" \"localnet\" can be referenced from multiple different UDNs The default MTU set for localnet is 1500 Pod requesting UDN without a VLAN is able to connect to services running on the host's network ({-}stretch) The \"physnet\" mapping is a \"supported API\" and available to users - so they can connect to the machine network without a need to configure a custom bridge-mapping{-} we should just always request user to configure the mapping themselves, until we understand all the implications of non-NORMAL mode on br-ex and how it works with local access / bondings / ... (stretch) Scheduling is managed by the platform - if a UDN requests a localnet (as in bridge-mappins.localnet), the Pod requesting this UDN will be only scheduled on a node with this resource available. This can use the same mechanism as the SR-IOV operator - combination of device plugins and \"k8s.v1.cni.cncf.io/resourceName\" annotation ... IPAM is not in the scope of this epic. See RFE-6947. Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Universal connectivity: Localnet 4.20",
        "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "4.18 OVN Kubernetes support for BGP as a routing protocol",
        "description": "Epic Goal OVN Kubernetes support for BGP as a routing protocol. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Tech Preview Whereabouts: Performance and scale considerations",
        "description": "Epic Goal Address performance and scale issues in Whereabouts IPAM CNI Why is this important? Whereabouts is becoming increasingly more popular for use on workloads that operate at scale. Whereabouts was originally built as a convenience function for a handful of IPs, however, more and more customers want to use whereabouts in scale sitatuions. Notably, for telco and ai/ml scenarios. Some ai/ml scenarios launch a large number of pods that need to use secondary networks for related traffic. Supporting Documents Upstream collaboration outline| Acceptance Criteria Both original allocation mode, and fast_ranges mode work without user intervention (e.g. backwards compatible) Reconciler still works with fast_ranges fast_ranges: 50% reduction in allocation time over original allocation method"
    },
    {
        "summary": "4.20 GA OVN Kubernetes support for BGP as a routing protocol: On-Prem",
        "description": "Epic Goal Left over from 4.18 (potentially BGP+UDN, egress IP) perf/scale UX fixes (ovnk specific API) Enabling subset of nodes selected for BGP advertisement with pod network (requirement from customers) Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Rebase Kube version to 1.32 in repos maintained by the SDN team",
        "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Support EndPort in MultiNetworkPolicy",
        "description": "Template: Networking Definition of Planned Epic Goal Add support for endPort field in multinetworkpolicy. The API change is merged upstream time to make d/s update. The support tracked in this epic is only for ovn-kubernetes, multi-netpol implementation will catch up later. It is known that MNP lacks some validations that networkpolicy has due to being a core API. It needs additional discussion (and potentially breaking changes) to decide whether MNP should also introduce similar validation. Opened a bug Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "PatternFly Modal deprecated in PF6",
        "description": "PatternFly Modal was deprecated in PF6, we should replace this component with a new Modal. This issue is a followup to updating to PF6:"
    },
    {
        "summary": "HCP KubeVirt VM Enhanced Topology Spread",
        "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints."
    },
    {
        "summary": "Aggregate the alerts in the Alerts page by alert name and severity",
        "description": "Goal Description As an Admin I want to have an easy view of the alerts that are firing in my cluster. If I have the same alert that fires many time it is very hard to identify the issues. We can simplify the existing Alerts page to make it much clearer by the following quick fix: 1. Aggregate the alerts by alert name and severity 2. For each aggregated line add the \"Total alerts number\" 3. When pressing on the aggregated line it can be expanded with the list of alerts 4.Optional - Add the namespace label to the expanded list of alerts, where each alert can have a different namespace. Note: Not all alerts have this label. Initial mockup by ~fkargbo: Acceptance Criteria Given we have several alerts of the same name and severity When we go to the Alerts page and view the alerts Then We would see a single line for each \"alert name\" and \"severity\", the number of times its shown and I can click on the line to expend the line and get the full list of alerts of that name and severity. User Stories High-Level goal-based user story, with context. \"As a VM owner/cluster administrator, I want to Achieve Some Goal, so that Some Reason/Context.\" another user story Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes Any additional details or decisions made/needed"
    },
    {
        "summary": "Remove Console classes (prefixed with co-, ocs-, odc-) in plugins",
        "description": "Remove Console classes in Kubevirt, Network and NMState UI plugins. - remove those, which have / will have their styling removed Details also on Slack:"
    },
    {
        "summary": "PatternFly6 Upgrade NETWORK UI",
        "description": "Goal Upgrade NETWORK UI make use of PatternFly6 User Stories Notes Any additional details or decisions made/needed"
    },
    {
        "summary": "As an oc-mirror user, I want to be able to skip signature mirroring",
        "description": "For users that might not have their policy.json and/or registries.d correctly configured, one might want to skip signature verification and mirroring completely. This story doesn' t provide a granular way (per image) way to skip signature mirroring. This story only provides a way to enable/disable signature mirroring as a whole. We need to also verify the behavior behind the existing command line arg secure-policy We need to at least ask PM if other parameters related to signature configuration found in skopeo/podman should also be available in oc-mirror. Ex: {code:java} // This is what skopeo uses to not verify signatures --insecure-policy run the tool without any policy check // This is what skopeo uses to set different locations for policy.json and registries.d --policy string Path to a trust policy file --registries.d DIR use registry configuration files in DIR (e.g. for container signature storage) // This is what skopeo uses to stop copying signatures --remove-signatures Do not copy signatures from SOURCE-IMAGE // these shouldn't be needed. --sign-by FINGERPRINT Sign the image using a GPG key with the specified FINGERPRINT --sign-by-sigstore PATH Sign the image using a sigstore parameter file at PATH --sign-by-sigstore-private-key PATH Sign the image using a sigstore private key at PATH --sign-identity string Identity of signed image, must be a fully specified docker reference. Defaults to the target docker reference. --sign-passphrase-file PATH Read a passphrase for signing an image from PATH {code}",
        "epic_key": "CLID-289"
    },
    {
        "summary": "As an oc-mirror user, I want cosign signature tags to be mirrored alongside images during disk to mirror workflow",
        "description": "Acceptance criteria When mirroring from disk to mirror, with an imageSetConfig containing an additional signed image, from an archive that was previously verified to contain the signatures corresponding to that image, the mirror registry should contain all signature tags corresponding to the mirored image",
        "epic_key": "CLID-289"
    },
    {
        "summary": "As an oc-mirror user, I want cosign signature tags to be incrementally saved to archives during mirror to disk workflow",
        "description": "Acceptance criteria when performing a mirror to disk with an empty working-dir, and an imagesetconfig containing a signed additional image, the archive generated contains the signature manifest AND the blobs corresponding to that manifest when performing a mirror to disk with an existing working-dir, and the same imagesetconfig as a previous run from a previous day, the archive doesn't contain signatures that were included in a previous archive",
        "epic_key": "CLID-289"
    },
    {
        "summary": "Helm chart signature support",
        "description": "Currently helm chart support in v2 does not mirror and verify signatures, this user story is to implement the mirroring of the signatures and the verification of them.",
        "epic_key": "CLID-289"
    },
    {
        "summary": "Create defaults configs for signature mirroring/verification",
        "description": "In operating systems (OS) where the registries.d and policy.json does not include our internal registries and the field use-sigstore-attachment: true, it is necessary to have a default embedded in oc-mirror. For oc-mirror cache: {code:java} docker: localhost:55000: use-sigstore-attachments: true{code} For customer regitry (only an example of a registry running on localhost:6000 below) {code:java} docker: localhost:6000: use-sigstore-attachments: true{code} For the release images: {code:java} docker: quay.io: use-sigstore-attachments: true{code} For operator catalog and bundles: {code:java} docker: registry.access.redhat.com: use-sigstore-attachments: true lookaside: {code:java} docker: registry.redhat.io: use-sigstore-attachments: true lookaside: Reference about containers/image policy.json/registries.d:",
        "epic_key": "CLID-289"
    },
    {
        "summary": "As a user I would like to mirror the signatures of the container images",
        "description": "-Open Questions:- -Verifying Third-Party Image Signatures: Support verifying the authenticity and integrity of the non-Red Hat (third-party) image signatures using the public keys.- -Question 1: How complex would it be to allow users to specify the location of their public keys in the configuration file or pass them as arguments?- -Question 2: Is it oc-mirror going to copy the certificate/public key as a resource to the cluster resources folder and ask the customer to apply them?- -Question 3: How about certificates?- Catalog images signatures: scenario when we rebuild the catalog Question 1: The signature of the catalog rebuilt is not like the original one since we changed the image completely, how is it going to work? Is the cluster going to fail because the signature is not the one expected? Support the future OCI 1.1 referrer-based approach: Question 1: Is the container image prioritizing this implementation on their side? Do we already have the Jira issue about this implementation?"
    },
    {
        "summary": "Bundles feature removal",
        "description": "There was a selected bundle feature on v2 that needs to be removed in 4.18 because of the its risk. An alternative solution is required to unblock one of our customers."
    },
    {
        "summary": "readOnlyRootFilesystem should be explicitly to true and if required to false for security reason",
        "description": "_1. Proposed title of this feature request_ openshift-cloud-credential-operator - readOnlyRootFilesystem should be explicitly to true and if required to false for security reason _2. What is the nature and description of the request?_ According to security best practice, it's recommended to set readOnlyRootFilesystem: true for all containers running on kubernetes. Given that openshift-cloud-credential-operator does not set that explicitly, it's requested that this is being evaluated and if possible set to readOnlyRootFilesystem: true or otherwise to readOnlyRootFilesystem: false with a potential explanation why the file-system needs to be write-able. _3. Why does the customer need this? (List the business requirements here)_ Extensive security audits are run on OpenShift Container Platform 4 and are highlighting that many vendor specific container is missing to set readOnlyRootFilesystem: true or else justify why readOnlyRootFilesystem: false is set. _4. List any affected packages or components._ openshift-cloud-credential-operator"
    },
    {
        "summary": "Upgrade to Kubernetes 1.32",
        "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that CCO uses to v1.32 which keeps it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? To make sure that Hive imports of other OpenShift components do not break when those rebase To avoid breaking other OpenShift components importing from CCO. To pick up upstream improvements Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) Kubernetes 1.32 is released Previous Work (Optional): Similar previous epic CCO-595 Done Checklist CI - CI is running, tests are automated and merged."
    },
    {
        "summary": "4.19 Regular Maintenance",
        "description": "Epic Goal Update all golang dependencies Ensure periodic CI jobs for new version Sunset periodic CI jobs for version(s) no longer supported Why is this important? To ensure we are using latest vendor code To reduce security vulnerabilities in vendor code To ensure we are regularly testing the latest version To reduce costs from testing old, unsupported versions. Scenarios ... Acceptance Criteria ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "The most basic e2e to gate upcoming work",
        "description": "Set up a mininum presubmit gate",
        "epic_key": "AUTOSCALE-28"
    },
    {
        "summary": "Implement HCP karpenter deletion logic",
        "description": "As an OpenShift HCP management cluster admin, when I delete a HCP cluster with autonode on, I would also want to make sure any provisioned nodes are removed from the infrastructure. We need to make sure Karpenter related objects are not blocking the deletion or would result in resource leakage. This story should capture the flow described in and implements cascading deletion login in the HCP Karpenter operator. This should also include e2e test(s) and relevant unit tests which make sure a HCP cluster with autonode on gets successfully deleted.",
        "epic_key": "AUTOSCALE-31"
    },
    {
        "summary": "add test for basic scale out",
        "description": "if possible we might be able to reuse upstream tests, based on decisions from PODAUTO-323.",
        "epic_key": "AUTOSCALE-28"
    },
    {
        "summary": "Mirror karpenter-aws repo and include it in OCP Payload",
        "description": "Goal Mirror the karpenter aws source code within openshift GH org Agree and automate a rebase cadence Include the image build within the OCP payload. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Setup e2e testing for techpreview API and add e2e for autoNode via karpenter",
        "description": "Goal Have a CI pipeline that runs HO with --tech-preview Add e2e test for autoNode via karpenter that validates: all manifest are created as expected management and guest side. Karpenter is able to autoprovision and remove compute. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Implement HCP karpenter deletion",
        "description": "Goal The goal of this epic is to support deletion of Karpenter provisioned nodes and their corresponding instances, when a HyperShift HostedCluster is torn down. The end goal is that all infrastructure backed instances are automatically fully removed from their infrastructure when deletion of the HostedCluster is finished. A subgoal includes allowing metrics, alerts, and events to be emitted during teardown. This epic is a part of the strategic feature work for OpenShift AutoNode: Why is this important? This is important because a user will expect all related resources corresponding to a HostedCluster is deleted when it is torn down. We need to specially care for Karpenter instances since they are being provisioned outside of the cluster's environment and being registered with the cluster afterwards. That means we will need to delete them from the infrastructure during teardown, without potentially leaking resources. It is also important that deletion deadlocks are minmized so that users are not stuck during deletion for an excessive amount of time. Additionally, metrics, events, and alerts will allow cluster-admins to diagnose any potential problems related to Karpenter/AutoNode during the tear down phase, and allow them to safely deprovision the cluster. Scenarios A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster. A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster, but the deletion is timed out due to some issue in the deletion process. Acceptance Criteria Dev - Deletion implementation has been merged, and metrics, alerts, events, etc. have been added. Dev - Upstream docs are merged that include document the deletion process, and steps to debug a stuck/failed deletion CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented (created hypershift-hosted cluster with AutoNode on, create some workloads, delete hosted cluster, make sure karpenter provisioned instances are deleted from infrastructure) Release Technical Enablement - Must have TE slides Dependencies (internal and external) None Previous Work (Optional): None Open questions: None for now. Some questions were covered by this spike: Done Checklist CI - CI is running, tests are automated and merged. link to tests in openshift/release Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: N/A"
    },
    {
        "summary": "Implement AutoNode upgrades via karpenter drift/consolidation",
        "description": "Goal Define upgrade criteria for Karpenter Nodes (E.g. follow the control plane, this can be configurable at the HC level so the services can make their choice) Implement it relying on native Drift and Consolidation. Why is this important? Reduce operational burden Scenarios With AutoNode via karpenter the Service is authoritative to manage upgrades of karpenter Nodes. We need to agree on 1..N criteria/strategies. Possibly expose them in the HC API and let them be driven via Drift/Consolidation. Known caveats: in the current prototype everytime the ignition token is rotated would cause drift as a side effect. We'll need to either make it configurable or somehow transparent for drift ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "GUI backend services",
        "description": "Epic Goal Have a friendly graphical user to perform interactive installation that runs on node0 Why is this important? Allows the WebUI to run in _Agent based installation_ where we can only count on node0 to run it Provides a familiar (close to SaaS) interface to walk through the first cluster installation Interactive installation takes us closer to having generated images that serve multiple first cluster installations Scenarios As an admin, I want to generate an ISO that I can send to the field to perform a friendly, interactive installation Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) Assisted-Service WebUI needs an _Agent based installation_ wizard Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "OVE release image generation",
        "description": "Epic Goal Setup a workflow to generate an ISO that will contain all the relevant pieces to install an OVE cluster Why is this important? As per OCPSTRAT-1874, the user must be able to install into a disconnected environment an OVE cluster, with the help of a UI, and without requiring explicitly to setup an external registry Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous work: Dependencies (internal and external) ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Interactively configure the rendezvous address",
        "description": "Epic Goal Allow the user to select a host to be Node 0 interactively after the booting the ISO. On each host the user would be presented with a choice between two options: Select this host as the rendezvous host (it will become part of the control plane) The IP address of the rendezvous host is: Enter IP (If the former option is selected, the IP address should be displayed so that it can be entered in the other hosts.) Why is this important? Currently, when using DHCP the user must determine which IP address is assigned to at least one of the hosts prior to generating the ISO. (OpenShift requires infinite DHCP leases anyway, so no extra configuration is required but it does mean trying to manually match data with an external system.) AGENT-385 would extend a similar problem to static IPs that the user is planning to configure interactively, since in that case we won't have the network config to infer them from. We should permit the user to delay collecting this information until after the hosts are booted and we can discover it for them. Scenarios In a DHCP network, the user creates the agent ISO without knowing which IP addresses are assigned to the hosts, then selects one to act as the rendezvous host after booting. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) AGENT-7 Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    },
    {
        "summary": "Unplanned work for 4.18",
        "description": "Template: Networking Definition of Planned Epic Goal Track work that needs to happen in 4.18 but was not part of the original planning. Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
    }
]