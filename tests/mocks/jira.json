{
  "OpenShift Workloads": {
    "epics": {
      "WRKLDS-1449": {
        "summary": "Upgrade to Kubernetes 1.31",
        "description": "Epic Goal Drive the technical part of the Kubernetes 1.31 upgrade, including rebasing openshift/kubernetes repositiry and coordination across OpenShift organization to get e2e tests green for the OCP release. Why is this important? (mandatory) OpenShift 4.18 cannot be released without Kubernetes 1.31 Scenarios (mandatory) Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\" PRs: Retro: Kube 1.31 Rebase Retrospective Timeline (OCP 4.18)| Retro recording:"
      }
    }
  },
  "Subscription Watch": {
    "stories": {
      "SWATCH-3413": {
        "summary": "Support ACM recording rule in SWATCH",
        "description": "Note once ACM validates recording rule we can utilize it in SWATCH like below. Me and ~lburnett0 are still debating whether to set this query in swatch-metrics application.yaml in rosa, default or acm. For org 5691294 We have provided few options to ACM team on changing the recording rule. Since SWATCH isn't the appropriate place to maintain each product's custom business logic to compute the values of usage to be metered. We may not need to do the following but, we are still ironing out with ACM: {code:java} topk(1, max(acm_capacity_effective_cpu_cores) by (_id) on(_id) group_right min_over_time(ocm_subscription{product=~\"moa-hostedcontrolplane\", external_organization=\"5691294\", support=~\"Premium)) {code} Result {code:java} {_id=\"17277f6d-f760-4949-af1a-1d93983639d5\", account=\"1xwWrms9ndnGhveUGg5iitacj3D\", billing_marketplace=\"aws\", billing_marketplace_account=\"515966534054\", billing_model=\"marketplace\", class=\"Unknown\", cloud_account_id=\"515966534054\", disconnected=\"false\", display_name=\"hcp-poc\", ebs_account=\"1082951\", email_domain=\"rbs.co.uk\", external_organization=\"5691294\", has_csm=\"false\", has_tam=\"true\", instance=\"uhc-acct-mngr-green-metrics.uhc-production.svc:8080\", job=\"uhc-acct-mngr-green-metrics\", managed=\"true\", metered_by_rh=\"true\", namespace=\"uhc-production\", organization=\"1Np3zXRXH2QrF3jaIJ7dIzc8khP\", product=\"moa-hostedcontrolplane\", prometheus=\"openshift-customer-monitoring/ams\", receive=\"true\", rhit_web_user_id=\"51367488\", risk=\"5\", service=\"uhc-acct-mngr-green-metrics\", support=\"Premium\", tenant_id=\"FB870BF3-9F3A-44FF-9BF7-D7A047A52F43\"} Value: 56 {code}"
      }
    }
  },
  "OpenShift Storage": {
    "stories": {
      "STOR-2319": {
        "summary": "Remove cns-migration CLI tool",
        "description": "We have decided to remove cns-migration CLI tool for now.",
        "epic_key": "STOR-2301"
      },
      "STOR-2285": {
        "summary": "Add e2e for running volume group snapshot tests",
        "description": "We need to make sure that we have e2e tests in Openshift that exercise this feature.",
        "epic_key": "STOR-2265"
      },
      "STOR-2263": {
        "summary": "Chore: update csi-driver-smb to the latest release",
        "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
        "epic_key": "STOR-2241"
      },
      "STOR-2257": {
        "summary": "Chore: Update gcp-pd-csi-driver to the latest release",
        "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
        "epic_key": "STOR-2241"
      },
      "STOR-2253": {
        "summary": "Chore: Update aws-ebs-csi-driver to the latest release",
        "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated) 4.19 special: check make sure it's enabled + tested in OCP",
        "epic_key": "STOR-2241"
      },
      "STOR-2251": {
        "summary": "Chore: update CSI sidecars",
        "description": "Update all CSI sidecars to the latest upstream release from external-attacher external-provisioner external-resizer external-snapshotter node-driver-registrar livenessprobe Corresponding downstream repos have `csi-` prefix, e.g. github.com/openshift/csi-external-attacher operator assets I.e. copy all snapshot CRDs from upstream| to the operator assets + go get -u github.com/kubernetes-csi/external-snapshotter/client/v6 in the operator repo.",
        "epic_key": "STOR-2241"
      },
      "STOR-2249": {
        "summary": "Chore: Update ibm-vpc-block-csi-driver to the latest release",
        "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
        "epic_key": "STOR-2241"
      },
      "STOR-2245": {
        "summary": "CI implementation: OCP 4.19 release chores",
        "epic_key": "STOR-2241"
      },
      "STOR-2260": {
        "summary": "Early chore: update OCP version in OLM metadata",
        "description": "Update OCP release number in OLM metadata manifests of: local-storage-operator aws-efs-csi-driver-operator gcp-filestore-csi-driver-operator secrets-store-csi-driver-operator smb-csi-driver-operator OLM metadata of the operators are typically in /config/manifest directory of each operator. Example of such a bump: We should do it early in the release, so QE can identify new operator builds easily and they are not mixed with the old release.",
        "epic_key": "STOR-2241"
      },
      "STOR-2256": {
        "summary": "Chore: Update azure-disk-csi-driver to the latest release",
        "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
        "epic_key": "STOR-2241"
      },
      "STOR-2252": {
        "summary": "Chore: update libraries in all operators",
        "description": "Update all OCP and kubernetes libraries in storage operators to the appropriate version for OCP release. Please wait for openshift/api, openshift/library-go, and openshift/client-go are updated to the newest Kubernetes release! There may be non-trivial changes in these libraries. This includes (but is not limited to): Kubernetes: client-go controller-runtime OCP: library-go openshift/api openshift/client-go operator-sdk Operators: csi-operator gcp-filestore-csi-driver-operator ibm-vpc-block-csi-driver-operator cluster-storage-operator local-storage-operator (please cross-check with -operator + vsphere-problem-detector in our tracking sheet and tools/bump-all| may be useful. For 4.16, this was enough: {code:java} mkdir 4.16-bump cd 4.16-bump ../library-bump.py --debug --web file with repo list STOR-1574 --run \"$PWD/../bump-all github.com/google/cel-go@v0.17.7\" --commit-message \"Bump all deps for 4.16\" {code} 4.17 perhaps needs an older prometheus: {code:java} ../library-bump.py --debug --web file with repo list STOR-XXX --run \"$PWD/../bump-all github.com/google/cel-go@v0.17.8 github.com/prometheus/common@v0.44.0 github.com/prometheus/client_golang@v1.16.0 github.com/prometheus/client_model@v0.4.0 github.com/prometheus/procfs@v0.10.1\" --commit-message \"Bump all deps for 4.17\" {code} 4.18 special: Add \"spec.unhealthyEvictionPolicy: AlwaysAllow\" to all PodDisruptionBudget objects of all our operators + operands. See WRKLDS-1490 for details There has been change in library-go function called `WithReplicasHook`. See",
        "epic_key": "STOR-2241"
      },
      "STOR-2136": {
        "summary": "Move snapshot featuregate to GA",
        "epic_key": "STOR-2120"
      }
    },
    "epics": {
      "STOR-2301": {
        "summary": "Document and support CNS volume migration using vCenter UI",
        "description": "Epic Goal We need to document and support CNS volume migration using native vCenter UI, so as customers can migrate volumes between datastores. Why is this important? (mandatory) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. Scenarios (mandatory) As an vCenter/Openshift admin, I want to migrate CNS volumes between datastores for existing vSphere CSI persistent volumes (PVs). This should cover attached and detached volumes. Special cases such as RWX, zonal or encrypted should also be tested to confirm is there is any limitation we should document. Dependencies (internal and external) (mandatory) This feature depends on VMware vCenter Server 7.0 Update 3o or vCenter Server 8.0 Update 2. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - STOR QE - STOR PX - Others - Acceptance Criteria (optional) This is mostly a testing / documentation epic, which will change current wording about unsupported CNS volume migration using vCenter UI. As part of this epic, we also want to remove the CLI tool we developed for from the payload. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2265": {
        "summary": "Upstream Beta Tracking: VolumeGroupSnapshot (TP)",
        "description": "Epic Goal Support upstream feature \"VolumeGroupSnapshot\"\" in OCP as -Beta- -GA- Beta, i.e. test it and have docs for it. Why is this important? We get this upstream feature through Kubernetes rebase. We should ensure it works well in OCP and we have docs for it. Upstream links Enhancement issue: KEP: Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) External: the feature is currently scheduled for GA in Kubernetes 1.32, i.e. OCP 4.19. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "STOR-2281": {
        "summary": "Remove volume snapshot webhook",
        "description": "Epic Goal Remove the csi-snapshot-validation-webhook image from OCP and all references to it. Why is this important? (mandatory) Upstream has removed the webhook. We have removed its Deployment from hypershift and standalone OCP| but our CI still builds an image with that name with a fake content. We should remove the image build + remove all references to it. Scenarios (mandatory) As OCP engineering team, we don't build & ship & reference useless images. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) `oc adm release info` does not show csi-snapshot-validation-webhook image + it has been removed from CI builds. Drawbacks or Risk (optional) Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2267": {
        "summary": "Upstream Cycle BETA - SELinux context mounts for RWO/RWX PVs",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Tracking upstream beta promotion of the SELinux context mounts for RWX/RWO PVs Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2241": {
        "summary": "OCP 4.19 release chores",
        "description": "Epic Goal Update all images that we ship with OpenShift to the latest upstream releases and libraries. Exact content of what needs to be updated will be determined as new images are released upstream, which is not known at the beginning of OCP development work. We don't know what new features will be included and should be tested and documented. Especially new CSI drivers releases may bring new, currently unknown features. We expect that the amount of work will be roughly the same as in the previous releases. Of course, QE or docs can reject an update if it's too close to deadline and/or looks too big. Traditionally we did these updates as bugfixes, because we did them after the feature freeze (FF). Why is this important? We want to ship the latest software that contains new features and bugfixes. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents."
      },
      "STOR-2126": {
        "summary": "StoragereadOnlyRootFilesystem should be explicitly to true and if required to false for security reason",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? According to security best practice, it's recommended to set readOnlyRootFilesystem: true for all containers running on kubernetes. Given that openshift-cluster-storage does not set that explicitly, it's requested that this is being evaluated and if possible set to readOnlyRootFilesystem: true or otherwise to readOnlyRootFilesystem: false with a potential explanation why the file-system needs to be write-able. Applies to openshift-cluster-storage, we should also check other storage operator for the same Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Extensive security audits are run on OpenShift Container Platform 4 and are highlighting that many vendor specific container is missing to set readOnlyRootFilesystem: true or else justify why readOnlyRootFilesystem: false is set. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As an OCP admin I want to ensure that best practice are applied unless there is a valid reason not to do so Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Need to be careful readOnlyRootFilesystem: true doesn't break anything Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2078": {
        "summary": "Upstream Beta Tracking: VolumeAttributesClass (TP)",
        "description": "Epic Goal Support upstream feature \"VolumeAttributesClass\" in OCP as Beta, i.e. test it and have docs for it. Why is this important? We get this upstream feature through Kubernetes rebase. We should ensure it works well in OCP and we have docs for it. Upstream links Enhancement issue: KEP: Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "STOR-1823": {
        "summary": "Remove Shared Resource CSI Driver Feature",
        "description": "Epic Goal Remove the Shared Resource CSI Driver as a tech preview feature. Why is this important? (mandatory) Shared Resources was originally introduced as a tech preview feature in OpenShift Container Platform. After extensive review, we have decided to GA this component through the Builds for OpenShift layered product. Expected GA will be alongside OpenShift 4.16. Therefore it is safe to remove in OpenShift 4.17 Scenarios (mandatory) Accessing RHEL content in builds/workloads Sharing other information across namespaces in the cluster (ex: OpenShift pull secret) Dependencies (internal and external) (mandatory) BUILD-793 Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - OpenShift Storage, OpenShift Builds (forum-openshift-builds) Documentation - QE - PX - Others - Acceptance Criteria (optional) Shared Resource CSI driver cannot be installed using OCP feature gates/tech preview feature set. Drawbacks or Risk (optional) Using Shared Resources requires installation of a layered product, not part of OCP core. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2141": {
        "summary": "Make maxAllowedBlockVolumesPerNode configurable (TP)",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? vSphere 8 now allows 255 volumes per VM (i.e OCP workers) and we use the default value of 59 which is safe for vsphere 7 but vsphere 8 customers want to be able to leverage the new improved limit. In order to limit potentials failures, reduce the complexity of that epic and deliver it in time, we are going to limit this feature to homogeneous vsphere 8 environments which only contains ESXi 8 hypervisors. Heterogeneous environment which contain a mix of ESXi 7 & 8 will not be allowed to use this feature. This will be explicitly documented. Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Customers who are using vsphere 8 are stuck with the default limit that applies to vsphere 7. They want to increase the value to benefit from vsphere 8 improvements. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As an OCP admin on top of vsphere 8 i want to increase the maximum number of volumes that can be attached to OCP workers. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. We need a cluster that has pvscsiCtrlr256DiskSupportEnabled set to true Confirm with VMware has support for it and link to official doc Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - STOR QE - STOR PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. maxAllowedBlockVolumesPerNode config is changed accordingly Should we reject values greater than 255? Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      },
      "STOR-2120": {
        "summary": "Support for VolumeGroup Snapshots (TP)",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Add Volume Group Snapshots as Tech Preview. This is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. We will rely on the newly beta promoted feature. This feature is driver dependent. This will need a new external-snapshotter rebase + removal of the feature gate check in csi-snapshot-controller-operator. Freshly installed or upgraded from older release, will have group snapshot v1beta1 API enabled + enabled support for it in the snapshot-controller (+ ship corresponding external-snapshotter sidecar). No opt-in, no opt-out. OCP itself will not ship any CSI driver that supports it. Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? This is also a key requirement for backup and DR solutions specially for OCP virt. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. External snapshotter rebase to the upstream version that include the beta API. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR / ODF Documentation - STOR QE - STOR / ODF PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Since we don't ship any driver with OCP that support the feature we need to have testing with ODF Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. No risk, behind feature gate Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
      }
    }
  },
  "OpenShift Container Platform (OCP) Strategy": {
    "description": "For the strategic OpenShift (OCP) work across the product (Outcomes, Features, Initiatives)",
    "features": {
      "OCPSTRAT-1680": {
        "summary": "Migrating CNS volumes between datastores via vSphere UI (GA)",
        "description": "Feature Overview (aka. Goal Summary) Allow customers to migrate CNS volumes (i.e vsphere CSI volumes) from one datastore to another. This operator relies on a new VMware CNS API and requires 8.0.2 or 7.0 Update 3o minimum versions In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. Goals (aka. expected user outcomes) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. As a first version, we developed a CLI tool that is shipped as part of the vsphere CSI operator. We keep this tooling internal for now, support can guide customers on a per request basis. This is to manage current urgent customer's requests, a CLI tool is easier and faster to develop it can also easily be used in previous OCP releases. After multiple discussion with VMware we now have confidence that we can rely on their built-in vSphere UI tool to migrate CNS volume from one datastore to another. This includes attached and detached volumes. Vmware confirmed they have confidence in this scenario and they fully support this operation for attached volumes. Requirements (aka. Acceptance Criteria): SInce the feature is external to OCP, it is mostly a matter of testing it works as expected with OCP but customers will be redirected to Vmware documentation as all the steps are done through the vSphere UI. Perform testing for attached and detached volumes + special cases such as RWX, zonal, encrypted. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YesHosted control planes YesConnected / Restricted Network x86Operator compatibility noUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCP on vsphere only| Use Cases (Optional): As a admin - want to migrate all my PVs or optional PVCs belonging to certain namespace to a different datastore within cluster without potentially requiring extended downtime. I want to move volumes to another datastore that has better performances I want to move volumes to another datastore current the current one is getting full I want to move all volumes to another datastore because the current one is being decommissioned. Questions to Answer (Optional): Get full support confirmation from vmware that their CNS volume migration feature Can be supported for OCP - YES is supported with attached volumes - YES Should detect if a volume is not migreable - YES Out of Scope Limited to what VMware supports. At the moment only one volume can be migrated at a time. Background We had a lot of requests to migrate volumes between datastore for multiple reason. Up until now it was not natively supported by VMware. In 8.0.2 they added a CNS API and a vsphere UI feature to perform volume migration. In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. This feature also includes the work needed to remove the CLI tool Customer Considerations Need to be explicit on requirements and limitations. Documentation Considerations Documented as part of the vsphere CSI OCP documentation. Specify min vsphere version. Document any limitation found during testing Redirect to vmware documentation. Announce removal of the CLI tool + update KB. Interoperability Considerations OCP on vSphere only"
      },
      "OCPSTRAT-1921": {
        "summary": "Support for VolumeGroup Snapshots (GA)",
        "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as GA, have docs updated, testing as well as removing feature gate to enable it by default. Requirements (aka. Acceptance Criteria): Tests and CI must pass. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want my customers to benefit from the VolumeGroupSnapshot feature included in my CSI driver. As a backup/DR software vendor I want to use the VolumeGroupSnapshot feature. As a customer I want access to use VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs or use a backup/DR solution that leverages VolumeGroupSnapshot Out of Scope CSI drivers development/support for this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
      },
      "OCPSTRAT-1783": {
        "summary": "vSphere - MachineSet - Support of more than one disk",
        "description": "Goal Support for more than one disk in machineset API for vSphere provider Feature description Customers using vSphere should be able to create machines with more than one disk. This is already available for other cloud and on-prem providers. Why do customers need this? To have Proper disk layout that better address their needs. Some examples are using the local storage operator or ODF. Affected packages or components RHCOS, Machine API, Cluster Infrastructure, CAPV."
      },
      "OCPSTRAT-1577": {
        "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
        "description": "Feature Overview Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
      },
      "OCPSTRAT-1823": {
        "summary": "GA 'oc adm upgrade status' command (and optionally status API)",
        "description": "As a customer of self managed OpenShift or an SRE managing a fleet of OpenShift clusters I should be able to determine the progress and state of an OCP upgrade and only be alerted if the cluster is unable to progress. Support a cli-status command and status-API which can be used by cluster-admin to monitor the progress. status command/API should also contain data to alert users about potential issues which can make the updates problematic. Feature Overview (aka. Goal Summary) {color:676767}_Show nodes where pod draining is taking more time._ _Customers have to dig deeper often to find the nodes for further debugging._ _The ask has been to bubble up this on the update progress window._{color} {color:676767}_oc update status ?_ _From the UI we can see the progress of the update. From oc cli we can see this from \"oc get cvo\"_ _But the ask is to show more details in a human-readable format._{color} _Know where the update has stopped. Consider adding at what run level it has stopped._ {code:java} oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.12.0 True True 16s Working towards 4.12.4: 9 of 829 done (1% complete) {code} Documentation Considerations {color:676767}_Update docs for UX and CLI changes_ _Reference :"
      },
      "OCPSTRAT-1711": {
        "summary": "Tech Preview OLM v1: Manage operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes",
        "description": "Feature Overview (aka. Goal Summary) OLM v1 effectively manages operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Goals (aka. expected user outcomes) Users can rely on OLM v1 to manage operators packaged in registry+v1 bundle format, including those with OwnNamespace and SingleNamespace installModes. Operator authors can rely on OLM v1 to propagate the specified targetNamespaces to the operator deployment manifest during installation as the WATCH_NAMESPACE env vars, ensuring that the operator is scoped to the correct namespace without modifications Background Our Telco customers and ISV partners are eager to leverage OLM v1's ability to declare specific Operator versions for managed clusters using GitOps/ZTP workflows. By defining Operator versions directly in Git repositories, customers can ensure that only compatible versions are deployed to specific configurations. This GitOps process streamlines initial deployment and subsequent updates, ensuring alignment between Operator versions and managed cluster configurations. While the initial GA release of OLM v1 may have limitations, our goal is to support a broader range of operators, including those packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. This will enable us to meet the evolving needs of our Telco customers and protect our existing investments. By preserving compatibility with the current operator landscape, we can facilitate a smoother transition to the OLM v1. This not only secures existing workloads but also opens up new opportunities for growth within the OpenShift business. Requirements (aka. Acceptance Criteria) TargetNamespace propagation: OLM v1 can handle situations where the targetNamespace differs from or equals to the installNamespace when installing a registry+v1 bundle. OLM v1 can propagate the specified targetNamespace to the operator deployment manifest, ensuring correct namespace scoping for registry+v1 bundles. RBAC enforcement: OLM v1 enforces RBAC permissions based on the targetNamespace to prevent unauthorized access to cluster-wide resources Error handling and troubleshooting: OLM v1 provides warning and error logs to help users troubleshoot potential issues related to installing registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Customer Considerations Telco customers. Documentation Considerations A step-by-step guide on configuring and managing registry+v1 bundles with OwnNamespace and SingleNamespace install modes in OLM v1. Interoperability Considerations Existing Red Hat and certified operators packaged in registry+v1 bundles support OwnNamespace and SingleNamespace installModes."
      },
      "OCPSTRAT-1583": {
        "summary": "Tech Preview OLM v1: Create a ServiceAccount with necessary permissions for managing cluster content lifecycle",
        "description": "Feature Overview (aka. Goal Summary) OLM v1 assists users in creating required ServiceAccounts with necessary permissions for managing cluster content lifecycle. Goals (aka. expected user outcomes) - Users can easily preview the required permissions before installing or upgrading an extension/operator. - Users can create a ServiceAccount with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Background By default, OLM v1 requires users to provide a service account for installing, upgrading, and deleting cluster content. This aligns with the least privilege principle, as OLM v1's default service account is limited to granted permissions and cannot easily perform actions on behalf of users with lower privileges. However, this requires cluster administrators or users with sufficient permissions to create a service account capable of creating, modifying, and deleting Kubernetes resources like Deployments, Services, and ConfigMaps, as needed by the extension/operator packages. To simplify this process, OLM v1 aims to assist users in determining and creating service accounts with appropriate permissions to manage cluster content. Requirements (aka. Acceptance Criteria) - Required permissions analysis: OLM v1 analyzes an extension/operator bundle to determine the required permissions for its lifecycle management. -- The required permissions include CRUD (Create, Read, Update, Delete) operations on all Kubernetes objects within the extension/operator bundle, as well as any permissions granted by included RBAC resources (if any) and resource dependencies. - Required permissions preview: OLM v1 provides a preview of required permissions for installing or upgrading operators/extensions to users. - Roles and RoleBindings creation: OLM v1 assists users in generating Role and RoleBinding objects based on the determined permissions, adhering to security best practices and least privilege principles. - ServiceAccount creation: OLM v1 assists users in creating a ServiceAccount and associate it with the generated RoleBindings with appropriate permissions for installing or upgrading extensions/operators. -- Provides an option for customizing the ServiceAccount name. - User Interaction: OLM v1 offers guidance and options for users to review and modify the generated Role/RoleBinding/ServiceAccount before creation. -- Provides an option for specifying custom permissions if needed. -- Handles errors during Role/RoleBinding/ServiceAccount creation with retry. -- Provides error messages for troubleshooting. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Open Questions: - How does this work for the contents packaged in Helm charts? -- Helm does not strictly require users to provide a ServiceAccount to create Kubernetes objects but can rely on the context in the Kubeconfig or the service account token mounted in the pod. Should we follow that pattern as one of the options to streamline the UX? Out of Scope __ your text here Documentation Considerations __ - The steps for previewing the required permissions before installing or upgrading an extension/operator. - The steps for creating a ServiceAccount and those associated Roles/Rolebindings with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1327": {
        "summary": "Tech Preview (Phase 1) Next-gen OLM UX: Unifying workload management in the console",
        "description": "Feature Overview (aka. Goal Summary) : This ticket introduces the initial Tech Preview release of the next-generation OLM (OLM v1) user experience in the console. : This ticket focuses on enabling a unified catalog UX in the console. This will allow customers to manage layered capabilities delivered through operators and partners' workloads, including OpenShift certified Helm charts, using the next-generation OLM (OLM v1) in the OpenShift console. : This is enabled through the novel in-cluster efficient catalog content service designed in OCPSTRAT-1655 and delivered in the 4.18 timeframe. Goals (aka. expected user outcomes) In essence, customers can: discover collections of k8s extension/operator contents released in the FBC format with richer visibility into their release channels, versions, update graphs, and the deprecation information (if any) to make informed decisions about installation and/or update them. install a k8s extension/operator declaratively and potentially automate with GitOps to ensure predictable and reliable deployments. update a k8s extension/operator to a desired target version or keep it updated within a specific version range for security fixes without breaking changes. remove a k8s extension/operator declaratively and entirely including cleaning up its CRDs and other relevant on-cluster resources (with a way to opt out of this coming up in a later release). Requirements (aka. Acceptance Criteria): 1) Pre-installation: Both cluster-admins or non-privileged end-users can explore and discover the layered capabilities or workloads delivered by k8s extensions/operators or plain helm charts from a unified ecosystem catalog UI in the \u2018Administrator Perspective\u2019 in the console. Users can filter the available offerings based on the delivery mechanism/source type (i.e., operator-backed or plain helm charts), providers (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Users can discover all versions in all channels that an offering/package defines in a catalog, select a version from a channel, and see its detailed description, provided APIs, and other metadata before the installation. 2) Installation: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) using a ServiceAccount with sufficient permissions can install a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users can see the recommended installation namespace if provided by the package authors for installation. Users get notified through error messages from the OLM API whenever two conflicting k8s extensions/operators (will be) owning the same API objects, i.e., no conflicting ownership, after triggering the installation. During the installation, users can see the installation progress reported from the \u2018ClusterExtension\u2019 API object. After installed, users (who have access to OLM v1\u2019s user-facing \u2018ClusterExtension\u2019 API) can see can access the metadata of the installed k8s extension/operator to see essential information such as its provided APIs, example YAMLs of its provided APIs, descriptions, infrastructure features, valid subscriptions, etc. 3) Update: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) can see what updates are available for their k8s extension/operators in the form of immediate target versions and the associated update channels. Users can trigger the update of a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users get notified through error messages whenever a k8s extension/operator is prevented from updating to a newer version that has a backward incompatible CustomResourceDefinition (CRD) that will cause workload or k8s extension/operator breakage. During OpenShift cluster update, users get Informed when installed k8s extensions/operators do not support the next OpenShift version (when annotated by the package author/provider). Customers must update those k8s extensions/operators to a newer/compatible version before OLM unblocks the OpenShift cluster update. During the update, users can see the progress reported from the \u2018ClusterExtension\u2019 API object. 4) Uninstallation/Deletion: Users are made aware of OLM v1 by default cleanly remove an installed k8s extension/operator including deleting CustomResourceDefinitions (CRDs), custom resource objects (CRs) of the CRDs, and other relevant resources to revert the cluster to its original state before the installation. Users can see a list of resources that are relevant to the installed k8s extension/operator they are about to remove and then explicitly confirm the deletion. Questions to Answer (Optional): What impact will the console's \"perspective consolidation\" initiative have on this? Out of Scope __ your text here Background Our customers will experience a streamlined approach to managing layered capabilities and workloads delivered through operators, operators packaged in Helm charts, or even plain Helm charts. The next generation OLM will power this central distribution mechanism within the OpenShift in the future. Customers will be able to explore and discover the layered capabilities or workloads, and then install those offerings and make them available on their OpenShift clusters. Similar to the experience with the current OperatorHub, customers will be able to sort and filter the available offerings based on the delivery mechanism (i.e., operator-backed or plain helm charts), source type (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Once click on a specific offering, they see the details which include the description, usage, and requirements of the offering, the provided services in APIs, and the rest of the relevant metadata for making the decisions. The next-gen OLM aims to unify workload management. This includes operators packaged for current OLM, operators packaged in Helm charts, and even plain Helm charts for workloads. We want to leverage the current support for managing plain Helm charts within OpenShift and the console for leveraging our investment over the years. Documentation Considerations Refer to the \"Documentation Considerations\" section of the OLM v1 GA feature. Relevant documents Next-gen OLM UX: Unifying workload management Operator Framework F2F - PM Session OLM F2F Discussion - Roadmap"
      },
      "OCPSTRAT-1684": {
        "summary": "Add all Dev only UI pages to the Admin Perspective",
        "description": "Feature Overview (aka. Goal Summary) __ your text here Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1973": {
        "summary": "BYOPKI for image verification in OCP - TP in 4.20",
        "description": "Feature Overview (aka. Goal Summary) Tech P : OCP 4.20 BYOPKI for image verification in OCP"
      },
      "OCPSTRAT-683": {
        "summary": "Migrate MAPI to Cluster API for AWS (TP) - Phase 1",
        "description": "Feature Overview (aka. Goal Summary) Implement Migration core for MAPI to CAPI for AWS This feature covers the design and implementation of converting from using the Machine API (MAPI) to Cluster API (CAPI) for AWS This Design investigates possible solutions for AWS Once AWS shim/sync layer is implemented use the architecture for other clouds in phase-2 & phase 3 Acceptance Criteria When customers use CAPI, There must be no negative effect to switching over to using CAPI . Seamless migration of Machine resources. the fields in MAPI/CAPI should reconcile from both CRDs."
      },
      "OCPSTRAT-330": {
        "summary": "Upstream OpenShift AutoScaler TechDebt (Phase 3)",
        "description": "Feature Overview This is a TechDebt and doesn't impact OpenShift Users. As the autoscaler has become a key feature of OpenShift, there is the requirement to continue to expand it's use bringing all the features to all the cloud platforms and contributing to the community upstream. This feature is to track the initiatives associated with the Autoscaler in OpenShift. Goals Scale from zero available on all cloud providers (where available) Required upstream work Work needed as a result of rebase to new kubernetes version Requirements RequirementNotesisMvp? Out of Scope n/a Background, and strategic fit Autoscaling is a key benefit of the Machine API and should be made available on all providers Assumptions Customer Considerations Documentation Considerations Target audience: cluster admins Updated content: update docs to mention any change to where the features are available."
      },
      "OCPSTRAT-680": {
        "summary": "Integrate Cluster API in standalone OCP-Phase 2",
        "description": "Feature Overview (aka. Goal Summary) Phase 2 Goal: Complete the design of the Cluster API (CAPI) architecture and build the core operator logic attach and detach of load balancers for internal and external load balancers for control plane machines on AWS, Azure, GCP and other relevant platforms manage the lifecycle of Cluster API components within OpenShift standalone clusters E2E tests for Phase-1, incorporating the assets from different repositories to simplify asset management. Background, and strategic fit Overarching Goal Move to using the upstream Cluster API (CAPI) in place of the current implementation of the Machine API for standalone Openshift. Phase 1 & 2 covers implementing base functionality for CAPI. Phase 2 also covers migrating MAPI resources to CAPI. Initially CAPI did not meet the requirements for cluster/machine management that OCP had the project has moved on, and CAPI is a better fit now and also has better community involvement. CAPI has much better community interaction than MAPI. Other projects are considering using CAPI and it would be cleaner to have one solution Long term it will allow us to add new features more easily in one place vs. doing this in multiple places. Acceptance Criteria There must be no negative effect to customers/users of the MAPI, this API must continue to be accessible to them though how it is implemented \"under the covers\" and if that implementation leverages CAPI is open"
      },
      "OCPSTRAT-2071": {
        "summary": "GA Allow Custom machine names when using the CPMS feature",
        "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
      },
      "OCPSTRAT-172": {
        "summary": "GA Cert-manager support router to load secrets",
        "description": "Epic Goal Review design and development PRs that require feedback from NE team. Why is this important? Customer requires certificates to be managed by cert-manager on configured/newly added routes. Acceptance Criteria All PRs are reviewed and merged. Dependencies (internal and external) CFE team dependency for addressing review suggestions. Done Checklist DEV - All related PRs are merged."
      },
      "OCPSTRAT-1418": {
        "summary": "Tech Preview Allow Custom machine names when using the CPMS feature",
        "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
      },
      "OCPSTRAT-134": {
        "summary": "Gateway API using Istio for Cluster Ingress - GA",
        "description": "Goal: Graduate to GA (full support) Gateway API with Istio to unify the management of cluster ingress with a common, open, expressive, and extensible API. Description: Gateway API is the evolution of upstream Kubernetes Ingress APIs. The upstream project is part of Kubernetes, working under SIG-NETWORK. OpenShift is contributing to the development, building a leadership position, and preparing OpenShift to support Gateway API, with Istio as our supported implementation. The plug-able nature of the implementation of Gateway API enables support for additional and optional 3rd-party Ingress technologies."
      },
      "OCPSTRAT-525": {
        "summary": "Enable HAProxy Dynamic Configuration Manager for OpenShift - Tech Preview",
        "description": "We need to do a lot of R&D and fix some known issues (e.g., see linked BZs). R&D targetted at 4.16 and productisation of this feature in 4.17"
      },
      "OCPSTRAT-1845": {
        "summary": "GADisconnected Cluster Update and Boot without local image registry - phase 2",
        "description": "Feature Overview Note: This feature will be a TechPreview in 4.16 since the newly introduced API must graduate to v1. Overarching Goal Customers should be able to update and boot a cluster without a container registry in disconnected environments. This feature is for Baremetal disconnected cluster. Background For a single node cluster effectively cut off from all other networking, update the cluster despite the lack of access to image registries, local or remote. For multi-node clusters that could have a complete power outage, recover smoothly from that kind of disruption, despite the lack of access to image registries, local or remote. Allow cluster node(s) to boot without any access to a registry in case all the required images are pinned"
      },
      "OCPSTRAT-1945": {
        "summary": "Updated boot images: Phase 4 (GCP, AWS to opt-out)",
        "description": "Feature Overview OCP 4 clusters still maintain pinned boot images. We have numerous clusters installed that have boot media pinned to first boot images as early as 4.1. In the future these boot images may not be certified by the OEM and may fail to boot on updated datacenter or cloud hardware platforms. These \"pinned\" boot images should be updateable so that customers can avoid this problem and better still scale out nodes with boot media that matches the running cluster version. In phase 1 provided tech preview for GCP. In phase 2, GCP support goes to GA and AWS goes to TP. In phase 3, AWS support goes to GA . In phase 4, AWS and GCP goes to opt-out. Requirements"
      },
      "OCPSTRAT-1474": {
        "summary": "Exploitation of hardware based root volume LUKS encryption (IBM Z)",
        "description": "Feature Overview (aka. Goal Summary) As LUKS encryption is required for certain customer environments e.g. being PCI compliant and the current implementation with Network Based LUKS encryption are a) complex and b) not reliable and secure we need to support our Customers with an way to have the Root Device encrypted on a secure way with IBM HW based HSM to secure the LUKS Key. This is a kind of TPM approach to store the luks key but fence it from the user. Hardware based LUKS encryption requires injection of the read of secure keys in clevis during boot time. Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YHosted control planes YConnected / Restricted Network IBM ZOperator compatibility n/aUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1938": {
        "summary": "On Cluster Layering: Parity",
        "description": "Feature Overview (aka. Goal Summary) The original release of on cluster layering (OCL informally) came with known limitations and parity gaps. We need to close those gaps so everyone can have \"Image mode on OpenShift\". Goals (aka. expected user outcomes) OCL should work for as many OCP deployment patterns and footprints as possible. Ideally everywhere you can run OpenShift. Requirements (aka. Acceptance Criteria): Priority platform issues: Disconnected Single Node Two Node Hosted Control Planes Multi-arch (homogeneous arch cluster) Multi-arch (heterogeneous arch cluster) Priority feature issues: Node disruption policy Extensions"
      },
      "OCPSTRAT-1389": {
        "summary": "On Cluster Layering: Phase 3 (GA)",
        "description": "Feature Overview This is Image mode on OpenShift. It uses the rpm-ostree native containers interface and not bootc but that is an implementation detail. In the initial delivery of CoreOS Layering, it is required that administrators provide their own build environment to customize RHCOS images. That could be a traditional RHEL environment or potentially an enterprising administrator with some knowledge of OCP Builds could set theirs up on-cluster. The primary virtue of an on-cluster build path is to continue using the cluster to manage the cluster. No external dependency, batteries-included. On-cluster, automated RHCOS Layering builds are important for multiple reasons: One-click/one-command upgrades of OCP are very popular. Many customers may want to make one or just a few customizations but also want to keep that simplified upgrade experience. Customers who only need to customize RHCOS temporarily (hotfix, driver test package, etc) will find off-cluster builds to be too much friction for one driver. One of OCP's virtues is that the platform and OS are developed, tested, and versioned together. Off-cluster building breaks that connection and leaves it up to the user to keep the OS up-to-date with the platform containers. We must make it easy for customers to add what they need and keep the OS image matched to the platform containers. Goals & Requirements The goal of this feature is primarily to bring the 4.14 progress (OCPSTRAT-35) to a Tech Preview or GA level of support. Customers should be able to specify a Containerfile with their customizations and \"forget it\" as long as the automated builds succeed. If they fail, the admin should be alerted and pointed to the logs from the failed build. The admin should then be able to correct the build and resume the upgrade. Intersect with the Custom Boot Images such that a required custom software component can be present on every boot of every node throughout the installation process including the bootstrap node sequence (example: out-of-box storage driver needed for root disk). Users can return a pool to an unmodified image easily. RHEL entitlements should be wired in or at least simple to set up (once). Parity with current features - including the current drain/reboot suppression list, CoreOS Extensions, and config drift monitoring."
      },
      "OCPSTRAT-943": {
        "summary": "Dev Preview AutoNode (Native Karpenter) with HCP",
        "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP -OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift) as APIs for Managed Services (ROSA) to consume. It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 (out of scope for Dev Preview) _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade (TBD for Dev Preview but potentially out of scope for Dev Preview, i.e. may slip to Tech Preview) Expose Karpenter metrics to Cluster Admin (out of scope for Dev Preview, Targeting Tech Preview) Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1657": {
        "summary": "Add a Mechanism to Label all Pods for a Hosted Cluster in the Control Plane Namespace",
        "description": "Background As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. Goal Implement Mechanism for Labeling Hosted Cluster Control Plane Pods Use-cases - Adherance to Microsoft 1p Resource Provider Requirements Components - Any pods that hypershift deploys or run in the hosted cluster namespace."
      },
      "OCPSTRAT-1916": {
        "summary": "Azure - Remove not required permissions from the Nodes",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria) _The Installer only creates the minimum permissions required to deploy OpenShift on Azure_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background Customer Considerations _A KCS will be created for customers running previous OpenShift releases who want to remove this resource_ Documentation Considerations"
      },
      "OCPSTRAT-561": {
        "summary": "Support Private Google Access to GCP endpoints",
        "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
      },
      "OCPSTRAT-569": {
        "summary": "AWS - Allocate Load Balancers (API & Ingress) to Specific Subnets",
        "description": "Add ability to choose subnet while creating ingresscontroller of type LoadBalancerService. Checking ingresscontroller CRD could see that there is no such way to set subnet of load balancer. Why is this important? Currently, when deploying an IngressController instance , all the FrontendIPs will be in the same subnet. However, the LoadBalancer Service implementation allows specifying the target subnet through service annotation. Therefore the need to introduce an additional field to the ingresscontroller CRD, that allows to specify the target subnet. The value of this field is then used to annotate the created LoadBalancer Service from the beginning on, so the ingress controller immediately gets its FrontendIP into the right subnet. Scenarios If the cluster is spread across multiple subnets then its good to have a way to set subnet while creating ingresscontroller of type LoadBalancerService."
      },
      "OCPSTRAT-1005": {
        "summary": "Remove Terraform from the Azure Stack Hub IPI installer",
        "description": "Feature Overview (aka. Goal Summary) As a result of Hashicorp's license change to BSL, Red Hat OpenShift needs to remove the use of Hashicorp's Terraform from the installer - specifically for IPI deployments which currently use Terraform for setting up the infrastructure. To avoid an increased support overhead once the license changes at the end of the year, we want to provision Azure Stack Hub infrastructure without the use of Terraform. Requirements (aka. Acceptance Criteria): _The Azure Stack Hub IPI Installer no longer contains or uses Terraform._ _The new provider should aim to provide the same results and have parity with the existing Azure Stack Hub Terraform provider. Specifically,_ we should aim for feature parity against the install config and the cluster it creates to minimize impact on existing customers' UX. Use Cases (Optional): __ Questions to Answer (Optional): __ Out of Scope __ Background __ Customer Considerations __ Documentation Considerations __ Interoperability Considerations __"
      },
      "OCPSTRAT-1360": {
        "summary": "Remove ARO build-flag in openshift-installer (ARO fork removal - Phase II - Part 1)",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1097": {
        "summary": "Add support to enable boot diagnostics option at installation time in Azure (ARO fork removal - Phase II - Part 2)",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ Documentation Considerations Interoperability Considerations"
      },
      "OCPSTRAT-1665": {
        "summary": "CAPI-based Installer technical debt",
        "description": "Feature Overview (aka. Goal Summary) _Review, refine and harden the CAPI-based Installer implementation introduced in 4.16_ Goals (aka. expected user outcomes) _From the implementation of the CAPI-based Installer started with OpenShift 4.16 there is some technical debt that needs to be reviewed and addressed to refine and harden this new installation architecture._ Requirements (aka. Acceptance Criteria): _Review existing implementation, refine as required and harden as possible to remove all the existing technical debt_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Documentation Considerations _There should not be any user-facing documentation required for this work_"
      },
      "OCPSTRAT-1361": {
        "summary": "BGP for UDN GA On-prem",
        "description": "Feature Overview (aka. Goal Summary) OVN Kubernetes BGP support as a routing protocol for User Defined Network (Segmentation) pod and VM addressability. Goals (aka. expected user outcomes) OVN-Kubernetes BGP support enables the capability of dynamically exposing cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. OVN-Kubernetes currently has no native routing protocol integration, and relies on a Geneve overlay for east/west traffic, as well as third party operators to handle external network integration into the cluster. This enhancement adds support for BGP as a supported routing protocol with OVN-Kubernetes. The extent of this support will allow OVN-Kubernetes to integrate into different BGP user environments, enabling it to dynamically expose cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. In a follow-on release, this enhancement will provide support for EVPN, which is a common data center networking fabric that relies on BGP via OCPSTRAT-1744 Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Use Cases (Optional): Integration with 3rdparty load balancers that send packets directly to OpenShift nodes with the destination IP address of a targeted pod, without needing custom operators to detect which node a pod is scheduled to and then add routes into the load balancer to send the packet to the right node. Questions to Answer (Optional): Out of Scope Support of any other routing protocol Running separate BGP instances per VRF network Support for any other type of L3VPN with BGP, including MPLS Providing any type of API or operator to automatically connect two Kubernetes clusters via L3VPN Replacing the support that MetalLB provides today for advertising service IPs Asymmetric Integrated Routing and Bridging (IRB) with EVPN Background BGP Importing Routes from the Provider Network Today in OpenShift there is no API for a user to be able to configure routes into OVN. In order for a user to change how cluster traffic is routed egress into the cluster, the user leverages local gateway mode, which forces egress traffic to hop through the Linux host's networking stack, where a user can configure routes inside of the host via NM State. This manual configuration would need to be performed and maintained across nodes and VRFs within each node. Additionally, if a user chooses to not manage routes within the host and use local gateway mode, then by default traffic is always sent to the default gateway. The only other way to affect egress routing is by using the Multiple External Gateways (MEG) feature. With this feature the user may choose to have multiple different egress gateways per namespace to send traffic to. As an alternative, configuring BGP peers and which route-targets to import would eliminate the need to manually configure routes in the host, and would allow dynamic routing updates based on changes in the provider\u2019s network. Exporting Routes into the Provider Network There exists a need for provider networks to learn routes directly to services and pods today in Kubernetes. Metal LB is already one solution whereby load balancer IPs are advertised by BGP to provider networks, and this feature development does not intend to duplicate or replace the function of Metal LB. Metal LB should be able to interoperate with OVN-Kubernetes, and be responsible for advertising services to a provider\u2019s network. However, there is an alternative need to advertise pod IPs on the provider network. One use case is integration with 3rd party load balancers, where they terminate a load balancer and then send packets directly to OCP nodes with the destination IP address being the pod IP itself. Today these load balancers rely on custom operators to detect which node a pod is scheduled to and then add routes into its load balancer to send the packet to the right node. By integrating BGP and advertising the pod subnets/addresses directly on the provider network, load balancers and other entities on the network would be able to reach the pod IPs directly. EVPN Extending OVN-Kubernetes VRFs into the Provider Network This is the most powerful motivation for bringing support of EVPN into OVN-Kubernetes. A previous development effort enabled the ability to create a network per namespace (VRF) in OVN-Kubernetes, allowing users to create multiple isolated networks for namespaces of pods. However, the VRFs terminate at node egress, and routes are leaked from the default VRF so that traffic is able to route out of the OCP node. With EVPN, we can now extend the VRFs into the provider network using a VPN. This unlocks the ability to have L3VPNs that extend across the provider networks. Utilizing the EVPN Fabric as the Overlay for OVN-Kubernetes In addition to extending VRFs to the outside world for ingress and egress, we can also leverage EVPN to handle extending VRFs into the fabric for east/west traffic. This is useful in EVPN DC deployments where EVPN is already being used in the TOR network, and there is no need to use a Geneve overlay. In this use case, both layer 2 (MAC-VRFs) and layer 3 (IP-VRFs) can be advertised directly to the EVPN fabric. One advantage of doing this is that with Layer 2 networks, broadcast, unknown-unicast and multicast (BUM) traffic is suppressed across the EVPN fabric. Therefore the flooding domain in L2 networks for this type of traffic is limited to the node. Multi-homing, Link Redundancy, Fast Convergence Extending the EVPN fabric to OCP nodes brings other added benefits that are not present in OCP natively today. In this design there are at least 2 physical NICs and links leaving the OCP node to the EVPN leaves. This provides link redundancy, and when coupled with BFD and mass withdrawal, it can also provide fast failover. Additionally, the links can be used by the EVPN fabric to utilize ECMP routing. Customer Considerations For customers using MetalLB, it will continue to function correctly regardless of this development. Documentation Considerations Interoperability Considerations Multiple External Gateways (MEG) Egress IP Services Egress Service Egress Firewall Egress QoS"
      },
      "OCPSTRAT-1682": {
        "summary": "OCP Console - Upgrade to PatternFly 6 (PF6)",
        "description": "Feature Overview (aka. Goal Summary) __ Upgrade the OCP console to Pattern Fly 6. Goals (aka. expected user outcomes) __ The core OCP Console should be upgraded to PF 6 and the Dynamic Plugin Framework should add support for PF6 and deprecate PF4. Requirements (aka. Acceptance Criteria): __ Console, Dynamic Plugin Framework, Dynamic Plugin Template, and Examples all should be upgraded to PF6 and all PF4 code should be removed. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background __ As a company we have all agreed to getting our products to look and feel the same. The current level is PF6. Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1844": {
        "summary": "TechDebt - OCP Console - Dependency Cleanup",
        "description": "Feature Overview (aka. Goal Summary) We need to maintain our dependencies across all the libraries we use in order to stay in compliance. Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1390": {
        "summary": "HCP KubeVirt VM Enhanced Topology Spread",
        "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1869": {
        "summary": "Phase 1: Cosign tag-based discovery oc-mirror v2: Discover and mirror SigStore-style attachments",
        "description": "Feature Overview (aka. Goal Summary) oc-mirror v2 can mirror images and their associated signatures and public keys, providing flexibility for both current \"SigStore/Cosign tag-based\" and the future \"OCI 1.1 referrer-based\" discovery modes. Goals (aka. expected user outcomes) oc-mirror v2 can discover and mirror image signatures alongside the images it mirrors, adhering to the Cosign tag convention. {color:00875a}(Out of scope for Phase 1, see reason in the comment Public Key Mirroring (or made available offline): Mirror public keys required for signature verification, including: Red Hat's public key for verifying Red Hat content. Rekor's public key for verifying signature transaction records for Red Hat content. To support enclave use cases, mirror public keys and store them locally to enable offline verification. This may require special permissions to access the /etc directory for storage. Out of Scope in Phase 1 __ OCI 1.1 referrer-based Discovery: The tool should be able to discover signatures referenced in the OCI 1.1 image manifest. SigStore-style Attachments Mirroring: SigStore-style attachments, such as SBOMs in formats like text/spdx or application/vnd.cyclonedx, should be optionally discoverable and mirrorable. Users can choose to enable this feature and specify the desired OCI media types. Flexible Signature Discovery - OCI 1.1 First, Cosign Second: oc-mirror v2 will prioritize discovering image signatures using the OCI 1.1 referrer-based approach, falling back to the SigStore/Cosign tag-based approach if necessary. Documentation Considerations __ your text here"
      },
      "OCPSTRAT-1426": {
        "summary": "GA OC mirror v2",
        "description": "Feature description Oc-mirror v2 is focuses on major enhancements that include making oc-mirror faster and more robust and introduces caching as well as address more complex air-gapped scenarios. OC mirror v2 is a rewritten version with three goals: Manage complex air-gapped scenarios, providing support for the enclaves feature Faster and more robust: introduces caching, it doesn\u2019t rebuild catalogs from scratch Improves code maintainability, making it more reliable and easier to add features, and fixes, and including a feature plugin interface"
      },
      "OCPSTRAT-1699": {
        "summary": "Configure containers to set readOnlyRootFilesystem to true starting in OCP 4.19",
        "description": "Red Hat Product Security recommends that pods be deployed with readOnlyRootFilesystem set to true in the SecurityContext, but does not require it because a successful attack can only be carried out with a combination of weaknesses and OpenShift runs with a variety of mitigating controls. However, customers are increasingly asking questions about why pods from Red Hat, and deployed as part of OpenShift, do not follow common hardening recommendations. Note that setting readOnlyRootFilesystem to true ensures that the container's root filesystem is mounted as read-only. This setting has nothing to do with host access. For more information, see Setting the readOnlyRootFilesystem flag to true reduces the attack surface of your containers, preventing an attacker from manipulating the contents of your container and its root file system. If your container needs to write temporary files, you can specify the ability to mount an emptyDir in the Security Context for your pod as described here. The following containers have been identified by customer scans as needing remediation. If your pod will not function with readOnlyRootFilesystem set to true, please document why so that we can document the reason for the exception. Service Mesh operator with sidecar-injector (this needs some additional investigation as we no longer ship the sidecar-injector with Service Mesh) S2I and Build operators: webhook tekton-pipelines-controller tekton-chains-controller openshift-pipelines-operator-cluster-operations tekton-operator-webhook openshift-pipelines-operator-lifecycle-event-listener Pac-webhook (part of Pipelines) Cluster ingress operator: serve-healthcheck-canary Node tuning operator: Tuned Machine Config Operator: Machine-config-daemon ACM Operator: Klusterlet-manifestwork-agent. This was fixed in ACM 2.10."
      },
      "OCPSTRAT-1946": {
        "summary": "Tech Preview AutoNode (Native Karpenter) with ROSA-HCP",
        "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP - OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift). It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade Expose Karpenter metrics to Cluster Admin Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1874": {
        "summary": "Dev Preview Agent-Installer Installation UI for OpenShift Virtualization",
        "description": "Summary The installation process for the OpenShift Virtualization Engine (OVE) has been identified as a critical area for improvement to address customer concerns regarding its complexity compared to competitors like VMware, Nutanix, and Proxmox. Customers often struggle with disconnected environments, operator configuration, and managing external dependencies, making the initial deployment challenging and time-consuming. To resolve these issues, the goal is to deliver a streamlined, opinionated installation workflow that leverages existing tools like the Agent-Based Installer, the Assisted Installer, and the OpenShift Appliance (all sharing the same underlying technology) while pre-configuring essential operators and minimizing dependencies, especially the need for an image registry before installation. By focusing on enterprise customers, particularly VMware administrators working in isolated networks, this effort aims to provide a user-friendly, UI-based installation experience that simplifies cluster setup and ensures quick time-to-value. Objectives and Goals Primary Objectives Simplify the OpenShift Virtualization installation process to reduce complexity for enterprise customers coming from VMware vSphere. Enable installation in disconnected environments with minimal prerequisites. Eliminate the dependency on a pre-existing image registry in disconnected installations. Provide a user-friendly, UI-driven installation experience for users used to VMware vSphere. Goals Deliver an installation experience leveraging existing tools like the Agent-Based Installer, Assisted Installer, and OpenShift Appliance, i.e. the Assisted Service. Pre-configure essential operators for OVE and minimize external day 1 dependencies (see OCPSTRAT-1811 \"Agent Installer interface to install Operators\") Ensure successful installation in disconnected environments with standalone OpenShift, with minimal requirements and no pre-existing registry Personas Primary Audience VMware administrators transitioning to OpenShift Virtualization in isolated/disconnected environments. Pain Points Lack of UI-driven workflows; writing YAML files is a barrier for the target user (virtualization platforms admins) Complex setup requirements (e.g., image registries in disconnected environments). Difficulty in configuring network settings interactively. Lack of understanding when to use a specific installation method Hard time finding the relevant installation method (docs or at console.redhat.com) Technical Requirements Image Registry Simplification Eliminate the dependency on an existing external image registry for disconnected environments. Support a workflow similar to the OpenShift Appliance model, where users can deploy a cluster without external dependencies. Agent-Based Installer Enhancements Extend the existing UI to capture all essential data points (e.g., cluster details, network settings, storage configuration) without requiring YAML files. Install without a pre-existing registry in disconnected environment Install required operators for virtualization OpenShift Virtualization Reference Implementation Guide v1.0.2) doing a POC which was promising: User Interface (no configuration files) The type of users coming from VMware vSphere expect a UI. They aren't used to writing YAML files and this has been identified as a blocker for some of them. We must provide a simple UI to stand up a cluster. Proposed Workflow PRD and notes from regular meetings|"
      },
      "OCPSTRAT-1093": {
        "summary": "Support for VolumeGroup Snapshots (TP)",
        "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. This feature tracks the Tech Preview implementation behind feature gate. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as tech preview have docs, testing as well as a feature gate to enable it in order for customers and partners to test it in advance. Requirements (aka. Acceptance Criteria): The feature should be graduated beta upstream to become TP in OCP. Tests and CI must pass and a feature gate should allow customers and partners to easily enable it. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs. Out of Scope CSI drivers development/support of this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
      },
      "OCPSTRAT-1785": {
        "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
        "description": "Feature Overview Requirements Users can specify multiple NICs for the OpenShift VMs that will be created for the OpenShift cluster nodes with different subnets."
      },
      "OCPSTRAT-1892": {
        "summary": "vSphere - Delete PV and PVCs when destroying a cluster",
        "description": "Goal Remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? When an OpenShift cluster on vSphere with CSI volumes is destroyed the volumes are not deleted, leaving behind multiple objects within vSphere. This leads to storage usage by orphan volumes that must be manually deleted. Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
      },
      "OCPSTRAT-1834": {
        "summary": "Tech Preview OCP Update Precheck command to improve update experience",
        "description": "Feature Overview (aka. Goal Summary) As a cluster-admin I can use a single command to see all upgrade checklist before I trigger an update. Create a Update precheck command that is part of core openshift that helps customers identify potential issues before triggering an OpenShift cluster upgrade, without blocking the upgrade process. This tool aims to reduce upgrade failures and support tickets by surfacing common issues beforehand. Goals (aka. expected user outcomes) Enable users (especially those with limited OpenShift expertise) to identify potential upgrade issues before starting the upgrade Reduce the number of failed upgrades and support tickets Provide clear, actionable information about cluster state relevant to upgrades Help customers make informed decisions about when to initiate upgrades Requirements (aka. Acceptance Criteria): Check Pod Disruption Budgets (PDBs): Identify existing PDBs that might impact the upgrade Display information about PDBs in a way that's understandable to users with limited Kubernetes experience workaround - Check DVO PDB checks Image Registry Access Verification: Validate access to required image repositories Pre-check ability to pull images needed for the upgrade Verify connectivity to public registries or repository of choice workaround : Image pinning GA Node Health Verification: Check for unavailable nodes Identify nodes in maintenance mode Detect unscheduled nodes Verify overall node health status Core Platform Component Health: Verify health of control plane workloads Check core platform operators' health Alert Analysis: List any active critical alerts Display relevant warning alerts Focus on alerts that could impact upgrade success Version-Specific Checks: Include checks specific to the target upgrade version Verify requirements for new features or changes between versions Check networking-related requirements (e.g., SDN to OVN migrations) Output Requirements: Provide clear, understandable output for users without deep OpenShift knowledge Don't block upgrades even if issues are found Present information in an easily digestible format ============== {}New additions in 2025{} MCP status Check the maxUnavailable Compare maxUnavailable to the request level or current load level (if above request level) and determine if this is the correct setting Check to see if the MCPs are paused Make a note if etcd is backed up Other operators Note which operators are set to manual vs automatic update Check to determine the next update of all OLM based operators __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both standaloneHosted control planes AllConnected / Restricted Network AllOperator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope Blocking upgrade execution Checking entire cluster state Verifying non-platform workloads Automated issue resolution Comprehensive cluster health checking Extensive operator compatibility verification beyond core platform ACM integration Although Operations will use ACM for day 2 operations. Customer Engineering will use cli for patching, updating, precheck etc. Background __ your text here Customer Considerations Target users may have limited Kubernetes/OpenShift expertise Many users coming from VMware background Customers often don't have TAM or premium support Users may not be familiar with platform-specific concepts Need to accommodate users who prefer not to read extensive documentation Documentation Considerations Interoperability Considerations __ your text here",
        "epic_key": "OTA-1432"
      },
      "OCPSTRAT-1585": {
        "summary": "Cluster-version operator version-pod failure accessability",
        "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Simplify debugging when a cluster fails to update to a new target release image, when that release image is unsigned or otherwise fails to pull. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here - Kubelet/CRIO to verify RH images & release payload sigstore signatures - ART will add sigstore signatures to core OCP images __ These acceptance criteria are for all deployment flavors of OpenShift. List applicable specific needs (N/A = not applicable)Self-managed, managed, or both yesHosted control planes Connected / Restricted Network Operator compatibility none, Other (please specify) Documentation Considerations Add documentation for sigstore verification and gpg verification Interoperability Considerations For folks mirroring release images (e.g. disconnected/restricted-network): oc-mirror need to support sigstore mirroring (OCPSTRAT-1417). Customers using BYO image registries need to support hosting sigstore signatures."
      },
      "OCPSTRAT-2240": {
        "summary": "Adding topology-awareness to Cinder CSI Driver",
        "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs| Since OpenStack does not provide a mechanism to map compute nodes to block storage AZs, the Cinder CSI driver treats the compute AZ as a block storage AZ, assuming that the operator has used the same naming convention across their deployment (that is, if there are three compute AZs, {{{}az-0{}}}, {{{}az-1{}}}, and {{{}az-2{}}}, then there will always be at least three block storage AZs with the same name and same semantic meaning (e.g. azN implies a particular rack, room, or data center for both the compute and block storage services). This is a reasonable position and is one the Nova project endorses, however, it isn't always true. Where a deployment is not doing and has divergent compute and block storage AZs, the Cinder CSI driver can end up requesting volumes with block storage AZs that don't exist. The way we have worked around this to date is to selectively enable or disable the topology feature flag provided to the external provisioner side car container, as deployed and managed by the Cinder CSI Driver Operator. This feature flag is being removed in a future release (when?), which means we can't rely on this long-term. We should therefore port the logic for determining whether or not to enable the topology feature from the Cinder CSI Driver Operator to the Cinder CSI Driver itself. Once this is done, we should remove the logic from the Operator since it should no longer be needed and will eventually not be supported. This epic tracks the above work. Why is this important? If we don't do this, we would lose the ability to disable the topology feature in environment where this is not supported (due to mismatched compute and block storage AZ sets). This will affect a number of customers."
      },
      "OCPSTRAT-1873": {
        "summary": "OLMv1: Downstream Feature Gate Promotion Mechanics",
        "description": "Feature Overview (aka. Goal Summary) __ Support iterative development by enabling OLMv1 to work both with a _TechPreviewNoUpgrade_ feature set and a _GeneralAvailability_ feature set. Goals (aka. expected user outcomes) __ Users will be able to opt into testing out new features via _TechPreviewNoUpgrade_ Users will be able to use OLMv1 GA features without the risk of _TechPreview_ features Requirements (aka. Acceptance Criteria): __ All necessary infrastructure in place to enable the use of feature gates CI jobs to ensure that nothing from the _TechPreviewNoUpdate_ feature set breaks OLMv1 __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Out of Scope __ OLMv1 will soon need a mechanism for supporting alpha/beta fields in CRDs, but it has been decided to explore this as a separate effort. Background __ Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP which might span multiple releases to reach completeness and stability (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will need to work both with TechPreviewNoUpgrade and without it but will have a different set of features. Customer Considerations __ your text here Documentation Considerations __ Will need documentation on how to enable the _TechPreviewNoUpgrade_ feature set Interoperability Considerations __ your text here"
      },
      "OCPSTRAT-1341": {
        "summary": "Remove Cgroup v1 from OCP in 4.19",
        "description": "Feature Overview (aka. Goal Summary) Cgroup V1 was deprecated in OCP 4.16 . RHEL will be removing support for cgroup v1 in RHEL 10 so we will remove it in OCP 4.19 Goal Upgrade Scenario For clusters running cgroup v1 on OpenShift 4.18 or earlier, upgrading to OpenShift 4.19 will be blocked. To proceed with the upgrade, clusters on OpenShift 4.18 must first switch from cgroup v1 to cgroup v2. Once this transition is complete, the cluster upgrade to OpenShift 4.19 can be performed."
      },
      "OCPSTRAT-1654": {
        "summary": "GA User Name Space in OpenShift 4.20",
        "description": "Feature Overview (aka. Goal Summary) GA User Name Space in OpenShift 4.20 continue work from"
      },
      "OCPSTRAT-2073": {
        "summary": "GA for sigstore API(clusterimagepolicy, imagepolicy)",
        "description": "GA for sigstore API(clusterimagepolicy, imagepolicy)"
      },
      "OCPSTRAT-1359": {
        "summary": "BYOPKI for image verification in OCP - Dev P in 4.19",
        "description": "Feature Overview (aka. Goal Summary) BYOPKI for image verification in OCP"
      },
      "OCPSTRAT-1805": {
        "summary": "Azure - Add support for Dxv6 machine series",
        "description": "Feature Overview (aka. Goal Summary) Dlsv6 Dsv6 Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Dldsv6 Ddsv6| Documentation Considerations Interoperability Considerations"
      },
      "OCPSTRAT-1854": {
        "summary": "Support EndPort in MultiNetworkPolicy",
        "description": "Feature Overview (aka. Goal Summary) This Feature adds support for EndPort in MultiNetworkPolicy for customers migrating VM instances to OpenShift Virtualization, and with a requirement to specify a port-range without having to individually specify each port separately. Without this Feature, customers will have issues migrating specific VMs to OpenShift Virtualization. It is currently supported with NetworkPolicy, but not yet with MultiNetworkPolicy. Goals (aka. expected user outcomes) A port range can be specified in MultiNetworkPolicy, instead of having to specify each port individually. Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) NetworkPolicy product docs| Questions to Answer (Optional): Out of Scope Background Customer Considerations Documentation Considerations Interoperability Considerations"
      }
    }
  },
  "OpenShift Specialist Platform Team": {
    "stories": {
      "SPLAT-2079": {
        "summary": "vSphere CPMS GA+1 Cleanup",
        "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere CPMS support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for vSphere CPMS. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the vSphere CPMS feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD"
      },
      "SPLAT-2078": {
        "summary": "vSphere Static IP GA+1 Cleanup",
        "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere Static IP support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for Static IP. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the static ip feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD"
      },
      "SPLAT-2072": {
        "summary": "vSphere Multi vCenter GA+1 Cleanup",
        "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere muliti vCenter support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for multi vCenter. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the multi vCenter feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD"
      },
      "SPLAT-1809": {
        "summary": "Enhance Installer to support vSphere multi disk",
        "description": "User Story: As an OpenShift Engineer I need to enhance the OpenShift installer to support creating a cluster with additional disks added to control plane and compute nodes so that I can use the new data disks for various OS needs. Description: This task is to enhance the installer to allow configuring data disks in the install-config.yaml. This will also require setting the necessary fields in machineset and machine definitions. The important one being for CAPV to do the initial creation of disks for the configured masters. Acceptance Criteria: - install-config.yaml supports configuring data disks in all machinepools. - CAPV has been updated with new multi disk support. - CAPV machines are created that result in control plane nodes with data disks. - MachineSet definitions for compute nodes are created correctly with data disk values from compute pool. - CPMS definition for masters has the data disks configured correctly. Notes: We need to be sure that after installing a cluster, the cluster remains stable and has all correct configurations.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-1801": {
        "summary": "vsphere problem detector needs additional checks for group zonal",
        "description": "As an openshift engineer make changes to vpd for host vm zonal so that a support engineer or customer is notified of potential missing groups or rules. Acceptance Criteria - Since there is a new requirement for: host groups, vm groups and host vm group rules make sure these exist properly",
        "epic_key": "SPLAT-1728"
      },
      "SPLAT-1800": {
        "summary": "Enable host vm group zonal for mao",
        "description": "As an openshift engineer enable host vm group zonal in mao so that compute nodes properly are deployed Acceptance Criteria: - Modify workspace to include vmgroup - properly configure vsphere cluster to add vm into vmgroup",
        "epic_key": "SPLAT-1728"
      },
      "SPLAT-1799": {
        "summary": "Enable host vm group zonal for cpms",
        "description": "As an openshift engineer enable host vm group zonal in CPMS so that control plane nodes properly are redeployed Acceptance Criteria: - Control plane nodes properly roll out when requried - Control plane nodes do not roll out when not needed",
        "epic_key": "SPLAT-1728"
      },
      "SPLAT-1742": {
        "summary": "Modify the installer to support host and vm group based zonal",
        "description": "USER STORY: As someone that installs openshift on vsphere, I want to install zonal via host and vm groups so that I can use a stretched physical cluster or use a cluster as a region and hosts as zones . DESCRIPTION: Required: Nice to have: ACCEPTANCE CRITERIA: - start validating tag naming - validate tags exist - validate host group exists - update platform spec - unit tests - create capv deployment and failure domain manifests - per failure domain, create vm group and vm host rule ENGINEERING DETAILS: Configuration steps: - Create tag and tag categories - Attach zonal tags to ESXi hosts - capv will complain extensively if this is not done - Host groups MUST be created and populated prior to installation (maybe, could we get the hosts that are attached or vice versa hosts in host group are attached?) one per zone - vm groups wil be created by the installer one per zone - vm / host rule will be created by the installer one per zone",
        "epic_key": "SPLAT-1728"
      },
      "SPLAT-1722": {
        "summary": "remove alibaba from cluster-ingress-operator",
        "description": "Acceptance Criteria - Since api and library-go are the last projects for removal, remove only alibaba specific code and vendoring",
        "epic_key": "SPLAT-1454"
      },
      "SPLAT-2060": {
        "summary": "Create e2e tests for vSphere Data Disks",
        "description": "USER STORY: As a developer, I need to create e2e tests for the new vSphere Data Disk feature so that we have proper code coverage and meet the required metrics to allow the feature to become GA some point in the future Required: Need to create e2e tests that meet the metrics defined in the openshift/api project Nice to have: Move these e2e tests into the projects that should own them using the new external tests feature being added to origin. ACCEPTANCE CRITERIA: At least 5 tests 95% of past tests have passed ENGINEERING DETAILS: Lets use this opportunity to start migrating e2e tests to the project where the new features are being added which will remove the dependency of having to stage merging code in one repo and then updating e2e in origin.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-2051": {
        "summary": "create origin e2e tests",
        "description": "{}USER STORY:{} As an OpenShift engineer, I want to create 5 e2e tests to validate multi-nic capabilities so that regressions are uncovered more quickly. {}DESCRIPTION:{} Create tests which specifically address 5 test cases/configurations to validate. {}Required:{} {}Nice to have:{} ... {}ACCEPTANCE CRITERIA:{} 5 test cases related to the multi-network feature gate Tests are passing {}ENGINEERING DETAILS:{} !-- Any additional information that might be useful for engineers: related repositories or pull requests, related email threads, GitHub issues or other online discussions, how to set up any required accounts and/or environments if applicable, and so on. --",
        "epic_key": "SPLAT-1944"
      },
      "SPLAT-2000": {
        "summary": "Add thinProvisioned to the new multi disk api",
        "description": "USER STORY: As an OpenShift administrator, I want to be able to configure thin provisioned for my new data disks so that adjust the behavior that may be different than my default storage policy. DESCRIPTION: Currently, we have the machine api changes forcing the thin provisioned flag to true. We need to add a flag to allow admin to configure this. The default behavior will be to not set the flag and use default storage policy. ACCEPTANCE CRITERIA: API has new flag Machine API has been modified to to use the new flag if set, else do not set the thinProvisioned attribute during clone.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-1995": {
        "summary": "Destroy and confirm there are no lingering CNS volumes",
        "description": "As a openshift engineer I want the installer to make sure there are no CNS volumes so we are not leaking volumes that could be taking disk space or alerting in vCenter. acceptance criteria - check if there are cns volumes, check if there are still pv from previous delete. If there are still CNS volumes and no PV(s) try to delete, if unsuccessful just return list - are you sure warning",
        "epic_key": "SPLAT-1993"
      },
      "SPLAT-1817": {
        "summary": "Update CPMS operator to support vSphere multi disk",
        "description": "User Story: As an OpenShift Engineer I need to ensure the the CPMS Operator now works with detecting any changes needed when data disks are added to the CPMS definition. Description: This task is to verify if any changes are needed in the CPMS Operator to handle the change data disk definitions in the CPMS. Acceptance Criteria: - CPMS does not roll out changes when initial install is performed. - Adding a disk to CPMS results in control plane roll out. - Removing a disk from CPMS results in control plane roll out. - No errors logged as a result of data disks being present in the CPMS definition. Notes: Ideally we just need to make sure the operator is updated to pull in the new CRD object definitions that contain the new data disk field.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-1811": {
        "summary": "Add vSphere multi disk support to machine api operator",
        "description": "User Story: As an OpenShift Engineer I need to ensure the the MAPI Operator. Description: This task is to verify if any changes are needed in the MAPI Operator to handle the change data disk definitions in the CPMS. Acceptance Criteria: - Adding a disk to MachineSet does not result in new machines being rolled out. - Removing a disk from MachineSet does not result in new machines being rolled out. - After making changes to a MachineSet related to data disks, when MachineSet is scaled down and then up, new machines contain the new data disk configurations. - All attempts to modify existing data disk definitions in an existing Machine definition are blocked by the webhook. Notes: The behaviors for the data disk field should be the same as all other provider spec level fields. We want to make sure that the new fields are no different than the others. This field is not hot swap capable for running machines. A new VM must be created for this feature to work.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-1808": {
        "summary": "Create feature gate and initial CRD changes for multi disk",
        "description": "User Story: As an OpenShift Engineer I need to create a new feature gate and CRD changes for vSphere multi disk so that we can gate the new function until all bugs are ironed out. Description: This task is to create the new feature gate to be used by all logical changes around multi disk support for vSphere. We also need to update the types for vsphere machine spec to include new array field that contains data disk definitions. Acceptance Criteria: - New feature gate exists for components to use. - Initial changes to the CRD for data disks are present for components to start using.",
        "epic_key": "SPLAT-1880"
      },
      "SPLAT-1743": {
        "summary": "Change API to support host vm group based zonal",
        "description": "As an openshift engineer update the infrastructure and the machine api objects so it can support host vm group zonal Acceptance criteria - Add the appropriate fields for host vm group zonal - Add the appropriate documentation for the above fields - Add a new feature gate for host vm zonal",
        "epic_key": "SPLAT-1728"
      }
    },
    "epics": {
      "SPLAT-1880": {
        "summary": "vsphere Multi Disk Support",
        "description": "User Story: As an OpenShift administrator, I need to be able to configure my OpenShift cluster to have additional disks on each vSphere VM so that I can use the new data disks for various OS needs. Description: This goal of this epic is to be able to allow the cluster administrator to install and configure after install new machines with additional disks attached to each virtual machine for various OS needs. Required: Installer allows configuring additional disks for control plane and compute virtual machines Control Plane Machine Sets (CPMS) allows configuring control plane virtual machines with additional disks Machine API (MAPI) allows for configuring Machines and MachineSets with additional disks Cluster API (CAPI) allows for configuring Machines and MachineSets with additional disks Nice to Have: Acceptance Criteria: Notes:"
      },
      "SPLAT-1728": {
        "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
        "description": "Epic Goal Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements{} Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
      },
      "SPLAT-1454": {
        "summary": "Remove IPI/UPI support of Alibaba cloud from OpenShift",
        "description": "OCP/Telco Definition of Done Epic Goal We want to remove official support for UPI and IPI support for Alibaba Cloud provider. Going forward, we are recommending installations on Alibaba Cloud with either external platform installation method. Why is this important? __ Scenarios Impacted areas based on CI: alibaba-cloud-csi-driver/openshift-alibaba-cloud-csi-driver-release-4.16.yaml alibaba-disk-csi-driver-operator/openshift-alibaba-disk-csi-driver-operator-release-4.16.yaml cloud-provider-alibaba-cloud/openshift-cloud-provider-alibaba-cloud-release-4.16.yaml cluster-api-provider-alibaba/openshift-cluster-api-provider-alibaba-release-4.16.yaml cluster-cloud-controller-manager-operator/openshift-cluster-cloud-controller-manager-operator-release-4.16.yaml machine-config-operator/openshift-machine-config-operator-release-4.16.yaml Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI jobs are removed Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "SPLAT-1944": {
        "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
        "description": "OCP/Telco Definition of Done Epic Goal In 4.18, support for multiple NICs was released as tech preview. The goal of this epic is to promote the feature to GA. Primarily, this involves proving the stability of the feature through supporting CI jobs and Sippy. Once proven, the feature gate and associated logic across impacted components are removed. Why is this important? Provides production/typical support for this feature. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "SPLAT-1993": {
        "summary": "vSphere - Delete PV and PVCs on installer destroy",
        "description": "Epic Goal The goal of this epic is upon destroy to remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
      }
    }
  },
  "Observability UI": {
    "stories": {
      "OU-659": {
        "summary": "Migrate the monitoring plugin to use patternfly 6",
        "description": "Background OCP 4.19 introduces PF6, this changes dramatically the UI. As the monitoring plugin is included by default with OCP we need to upgrade it so is consistent with the console UI Outcomes The plugin works without errors in the 4.19 console running Patternfly v6 Custom classes are removed when possible to make future upgrades easier",
        "epic_key": "OU-639"
      }
    },
    "epics": {
      "OU-639": {
        "summary": "Migrate UIPlugins to PF6",
        "description": "Description \"In order to keep plugins working with future versions of the OpenShift web console and match the new PF6 design, we as the Observability UI Team need to upgrade them to PF6.\" Goals & Outcomes +Engineering/Data Analytics Requirements:+ All UI plugins use PF6 dependency Documentation Patternfly support for plugins in the console:"
      }
    }
  },
  "OpenShift Over the Air": {
    "stories": {
      "OTA-1427": {
        "summary": "USC: Maintain status insights for Nodes",
        "description": "Implement a new Informer controller in the Update Status Controller to watch Node resources in the cluster and maintain an update status insight for each. The informer will need to interact with additional resources such as MachineConfigPools and MachineConfigs, e.g. to discover the OCP version tied to config that is being reconciled on the Node, but should not attempt to maintain the MachineConfigPool status insights. Generally the node status insight should carry enough data for any client to be able to render a line that the oc adm upgrade status currently shows: {code} NAME ASSESSMENT PHASE VERSION EST MESSAGE build0-gstfj-ci-prowjobs-worker-b-9lztv Degraded Draining 4.16.0-ec.2 ? failed to drain node: node after 1 hour. Please see machine-config-controller logs for more informatio build0-gstfj-ci-prowjobs-worker-d-ddnxd Unavailable Pending ? ? Machine Config Daemon is processing the node build0-gstfj-ci-tests-worker-b-d9vz2 Unavailable Pending ? ? Not ready build0-gstfj-ci-tests-worker-c-jq5rk Unavailable Updated 4.16.0-ec.3 - Node is marked unschedulable {code} The basic expectations for Node status insights are described in the design docs| but the current source of truth for the data structure is the NodeStatusInsight structure from . Definition of Done - During the upgrade, the status api contains a Node status insight for each Node in the cluster - Do not bother with the status insight lifecycle (when a Node is removed from the cluster, the status insight should technically disappear, but do not address that in this card, suitable lifecycle mechanism for this does not exist yet and OTA-1418 will address it) - Overall the functionality should match what oc adm upgrade status client-based checks - The NodeStatusInsight should have correctly populated: name, resource, poolResource, scopeType, version, estToComplete and message fields, following the existing logic from oc adm upgrade status - Health insights are out of scope - Status insights for MCPs are out of scope - The Updating condition has a similar meaning and interpretation like in the other insights. -- When its status is False, it will contain a reason which needs to be interpreted. Three known reasons are Pending, Updated and Paused: --- Pending: Node will eventually be updated but has not started yet --- Updated: Node already underwent the update. --- Paused: Node is running an outdated version but something is pausing the process (like parent MCP .spec.paused field) -- When Updating=True, there are also three known reasons: Draining, Updating and Rebooting. --- Draining: MCO drains the node so it can be updated and rebooted --- Updating: MCO applies the new config and prepares the node to be rebooted into the new OS version --- Rebooting: MCO is rebooting the node, after which it (hopefully) becomes ready again - The Degraded and Unavailable condition logic should match the existing assessment logic from oc adm upgrade status",
        "epic_key": "OTA-1260"
      },
      "OTA-1418": {
        "summary": "USC: Implement proper lifecycle for health insights",
        "description": "Health insights have a lifecycle that is not suitable for the async producer/consumer architecture USC has right now, where update informers send individual insights to the controller that maintains the API instance. Health insights are expected to disappear and appear following the trigger condition, and this needs to be respected through controller restart, API unavailability etc. Basically this means that the informer should ideally work as a standard, full-reconciliation controller over a set of its insights. We should also have an easy method to test health insight lifecycle: easy on/off switch for an artificial health insight to be reported or not, to avoid relying on true problematic conditions for testing the controller operation. Something like an existence of a label or an annotation on a resource to trigger a health insight directly. Definition of Done - When a resource watched by USC (so ClusterVersion ATM) has a certain annotation, USC should produce an artificial health insight - The properties of the health insight do not matter but must clearly indicate the health insight is artificial and intended for testing - All these scenarios must work: -- When insight is not in API, USC is running and the annotation is added to CV - insight is added to API -- When insight is not in API, USC is running, annotation is not on CV, insight is manually added to API - insight is removed from API -- When insight is in the API, USC is running, annotation is removed from CV - insight is removed from API -- When insight is in the API, USC is running, insight is manually removed from API - insight is added from API -- When insight is not in API, USC is stopped, annotation is added to CV, USC is started - insight is added to API -- When insight is in the API, USC is stopped, annotation is removed from CV, USC is started - insight is removed from API - In all cases where there is an existing insight in the API and the annotation was never observed removed from the CV, any \"refresh\" by USC must respect the original properties of the insight (start time, uid, etc) - Identical sync mechanism should be used for Status Insights",
        "epic_key": "OTA-1260"
      },
      "OTA-1405": {
        "summary": "Introduce the configuration API into HyperShift",
        "description": "The CVO is configurable using a manifest file. We now can proceed to HyperShift. Introduce the HyperShift API changes described in the enhancement. Definition of Done: API changes are merged in HyperShift.",
        "epic_key": "OTA-923"
      },
      "OTA-1393": {
        "summary": "status: recognize the process of migration to multi-arch",
        "description": "After OTA-960 is fixed, ClusterVersion/version and oc adm upgrade can be used to monitor the process of migrating a cluster to multi-arch. {noformat} $ oc adm upgrade info: An upgrade is in progress. Working towards 4.18.0-ec.3: 761 of 890 done (85% complete), waiting on machine-config Upgradeable=False Reason: PoolUpdating Message: Cluster operator machine-config should not be upgraded between minor versions: One or more machine config pools are updating, please see `oc get mcp` for further details Upstream: Channel: candidate-4.18 (available channels: candidate-4.18) No updates available. You may still upgrade to a specific release image with --to-image or wait for new updates to be available. {noformat} But oc adm upgrade status reports COMPLETION 100% while the migration/upgrade is still ongoing. {noformat} $ OC_ENABLE_CMD_UPGRADE_STATUS=true oc adm upgrade status Unable to fetch alerts, ignoring alerts in 'Update Health': failed to get alerts from Thanos: no token is currently in use for this session = Control Plane = Assessment: Completed Target Version: 4.18.0-ec.3 (from 4.18.0-ec.3) Completion: 100% (33 operators updated, 0 updating, 0 waiting) Duration: 15m Operator Status: 33 Healthy Control Plane Nodes NAME ASSESSMENT PHASE VERSION EST MESSAGE ip-10-0-95-224.us-east-2.compute.internal Unavailable Updated 4.18.0-ec.3 - Node is unavailable ip-10-0-33-81.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - ip-10-0-45-170.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - = Worker Upgrade = WORKER POOL ASSESSMENT COMPLETION STATUS worker Completed 100% 3 Total, 2 Available, 0 Progressing, 0 Outdated, 0 Draining, 0 Excluded, 0 Degraded Worker Pool Nodes: worker NAME ASSESSMENT PHASE VERSION EST MESSAGE ip-10-0-72-40.us-east-2.compute.internal Unavailable Updated 4.18.0-ec.3 - Node is unavailable ip-10-0-17-117.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - ip-10-0-22-179.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - = Update Health = SINCE LEVEL IMPACT MESSAGE - Warning Update Speed Node ip-10-0-95-224.us-east-2.compute.internal is unavailable - Warning Update Speed Node ip-10-0-72-40.us-east-2.compute.internal is unavailable Run with --details=health for additional description and links to related online documentation $ oc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.18.0-ec.3 True True 14m Working towards 4.18.0-ec.3: 761 of 890 done (85% complete), waiting on machine-config $ oc get co machine-config NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE MESSAGE machine-config 4.18.0-ec.3 True True False 63m Working towards 4.18.0-ec.3 {noformat} The reason is that PROGRESSING=True is not detected for co/machine-config as the status command checks only and it needs to check ClusterOperator.Status.Versionsname==\"operator-image\" as well. For grooming: It will be challenging for the status command to check the operator image's pull spec because it does not know the expected value. CVO knows it because CVO holds the manifests (containing the expected value) from the multi-arch payload. One \"hacky\" workaround is that the status command gets the pull spec from the MCO deployment: {noformat} oc get deployment -n openshift-machine-config-operator machine-config-operator -o json .image' quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:787a505ca594b0a727549353c503dec9233a9d3c2dcd6b64e3de5f998892a1d5 {noformat} Note this co/machine-config - deployment/machine-config-operator trick may not be feasible if we want to extend it to all cluster operators. But it should work as a hacky workaround to check only MCO. We may claim that the status command is not designed for monitoring the multi-arch migration and suggest to use oc adm upgrade instead. In that case, we can close this card as Obsolete/Won'tDo. ^manifests.ziphas the mockData/manifests for the status cmd that are taken during the migration. oc1920| started the work for the status command to recognize the migration and we need to extend the work to cover (the comments from Petr's review): - \"Target Version: 4.18.0-ec.3 (from 4.18.0-ec.3)\": confusing. We should tell \"multi-arch\" migration somehow. Or even better: from the current arch to multi-arch, for example \"Target Version: 4.18.0-ec.3 multi (from x86_64)\" if we could get the origin arch from CV or somewhere else. -- We have spec.desiredUpdate.architecture since forever, and can use that being Multi as a partial hint. MULTIARCH-4559 is adding tech-preview status properties around architecture in 4.18, but tech-preview, so may not be worth bothering with in oc code. Two history entries with the same version string but different digests is probably a reliable-enough heuristic, coupled with the spec-side hint. - \"Duration: 6m55s (Est. Time Remaining: 1h4m)\": We will see if we could find a simple way to hand this special case. I do not understand \"the 97% completion will be reached so fast.\" as I am not familiar with the algorithm. But it seems acceptable to Petr that we show N/A for the migration. -- I think I get \"the 97% completion will be reached so fast.\" now as only MCO has the operator-image pull spec. Other COs claim the completeness immaturely. With that said, \"N/A\" sounds like the most possible way for now. - Node status like \"All control plane nodes successfully updated to 4.18.0-ec.3\" for control planes and \"ip-10-0-17-117.us-east-2.compute.internal Completed\". It is technically hard to detect the transaction during migration as MCO annotates only the version. This may become a separate card if it is too big to finish with the current one. - \"targetImagePullSpec := getMCOImagePullSpec(mcoDeployment)\" should be computed just once. Now it is in the each iteration of the for loop. We should also comment about why we do it with this hacky way.",
        "epic_key": "OTA-1260"
      },
      "OTA-1269": {
        "summary": "Scaffold the status API controller",
        "description": "On the call to discuss oc adm upgrade status roadmap to server side-implementation (notes) we agreed on basic architectural direction and we can starting moving in that direction: - status API will be backed by a new controller - new controller will be a separate binary but delivered in the CVO image (=release payload) to avoid needing new ClusterOperator - new operator will maintain a singleton resource of a new UpgradeStatus CRD - this is the interface to the consumers Let's start building this controller; we can implement the controller perform the functionality currently present in the client, and just expose it through an API. I am not sure how to deal with the fact that we won't have the API merged until it merges into o/api, which is not soon. Maybe we can implement the controller over a temporary fork of o/api and rely on manually inserting the CRD into the cluster when we test the functionality? Not sure. We need to avoid committing to implementation details and investing effort into things that may change though. Definition of Done - (/) CVO repository has a new controller (a new cluster-version-operator cobra subcommand sounds like a good option; an alternative would a completely new binary included in CVO image) - (/) The payload contains manifests (SA, RBAC, Deployment) to deploy the new controller when DevPreviewNoUpgrade feature set is enabled (but not TechPreview) - (/) The controller uses properly scoped minimal necessary RBAC through a dedicated SA - (/) The controller will react on ClusterVersion changes in the cluster through an informer - (/) The controller will maintain a single ClusterVersion status insight as specified by the Update Health API Draft together with the necessary generated code (like deepcopy). These local types will need to be replaced with the types eventually merged into o/api and vendored to o/cluster-version-operator Testing notes This card only brings a _skeleton_ of the desired functionality to the DevPreviewNoUpgrade feature set. Its purpose is mainly to enable further development by putting the necessary bits in place so that we can start developing more functionality. There's not much point in automating testing of any of the functionality in this card, but it should be useful to start getting familiar with how the new controller is deployed and what are its concepts. For seeing the new controller in action: 1. Launch a cluster that includes both the code and manifests. As of Nov 11, 1107 is not yet merged so you need to use launch 4.18,openshift/cluster-version-operator1107 aws,no-spot 2. Enable the DevPreviewNoUpgrade feature set. CVO will restart and will deploy all functionality gated by this feature set, including the USC. It can take a bit of time, ~10-15m should be enough though. 3. Eventually, you should be able to see the new openshift-update-status-controller Namespace created in the cluster 4. You should be able to see a update-status-controller Deployment in that namespace 5. That Deployment should have one replica running and being ready. It should not crashloop or anything like that. You can inspect its logs for obvious failures and such. At this point, its log should, near its end, say something like \"the ConfigMap does not exist so doing nothing\" 6. Create the ConfigMap that mimics the future API (make sure to create it in the openshift-update-status-controller namespace): oc create configmap -n openshift-update-status-controller status-api-cm-prototype 7. The controller should immediately-ish insert a usc-cv-version key into the ConfigMap. Its content is a YAML-serialized ClusterVersion status insight (see design doc| As of OTA-1269 the content is not _that_ important, but the (1) reference to the CV (2) versions field should be correct. 8. The status insight should have a condition of Updating type. It should be False at this time (the cluster is not updating). 9. Start upgrading the cluster (it's a cluster bot cluster with ephemeral 4.18 version so you'll need to use --to-image=pullspec and probably force it 10. While updating, you should be able to observe the controller activity in the log (it logs some diffs), but also the content of the status insight in the ConfigMap changing. The versions field should change appropriately (and startedAt too), and the Updating condition should become True. 11. Eventually the update should finish and the Updating condition should flip to False again. Some of these will turn into automated testcases, but it does not make sense to implement that automation while we're using the ConfigMap instead of the API.",
        "epic_key": "OTA-1260"
      },
      "OTA-1029": {
        "summary": "Enhancement proposal to allow changing the log level of CVO using API configuration",
        "description": "We need to send an enhancement proposal that would contain the design changes we suggest in openshift/api/config/v1/types_cluster_version.go Merged in the API repository.",
        "epic_key": "OTA-923"
      },
      "OTA-1488": {
        "summary": "Remove USC manifests from DevPreview payload content and related tests",
        "description": "For the effort to deliver the UpdateStatus API in 4.19 we made DevPreview OCP deploy the Update Status Controller skeleton via payload manifests: new namespace, service account, rbac and deployment. We are not going to ship any meaningful UpdateStatus API implementation in 4.19, so we should remove all these manifests so that no useless ballast is deployed on 4.19 clusters after GA, even in DevPreview. We also have CI jobs / tests that exercise USC deployment in DevPreview; these need to be removed / disabled as well.",
        "epic_key": "OTA-1260"
      },
      "OTA-1426": {
        "summary": "Extend tech-preview 'oc adm upgrade recommend' to render relevant alerts",
        "description": "oc adm upgrade recommend should retrieve alerts from the cluster (similar to how oc adm upgrade status already does), and inject them as conditional update risks (similar to how OTA-902 / cvo1907 injected Upgradeable issues). The set of alerts to include is: All critical alerts, because that's explicitly selected in OCPSTRAT-1834. This includes {{{}ClusterOperatorDown{}}}, which overlaps with the existing ClusterVersion Failing condition which is part of PDB coverage. PDB coverage: warning PodDisruptionBudgetAtLimit Nodes: warning KubeNodeNotReady and KubeNodeUnreachable| Definition of done / test-plan: Find a cluster with both update recommendations and some of the mentioned alerts firing. Run {{{}OC_ENABLE_CMD_UPGRADE_RECOMMEND=true OC_ENABLE_CMD_UPGRADE_RECOMMEND_PRECHECK=true oc adm upgrade recommend{}}}. Confirm that the command calls out the expected alerts for all update targets.",
        "epic_key": "OTA-1422"
      },
      "OTA-1411": {
        "summary": "USC: Maintain status insights for ClusterOperator resources",
        "description": "Extend the Control Plane Informer in the Update Status Controller so it watches ClusterOperator resources in the cluster and maintains an update status insight for each. The actual API structure for an update status insights needs to be taken from whatever state is at the moment. The story does not include the actual API form nor how it is exposed by the cluster (depending on the state of API approval, the USC may still expose the API as a ConfigMap or an actual custom resource), it includes just the logic to watch ClusterOperator resources and producing a matching set of cluster operator status insights. The basic expectations for cluster operator status insights are described in the design docs like we have in the prototype) ClusterOperator in the cluster - Outside of control plane update, the cluster operator status insights are not present - Updating condition: -- If a CO is updating right now (Progressing=True && does not have a target operator version), then Updating=True and a suitable reason -- Otherwise, if it has target version, Updating=False and Reason=Updated -- Otherwise, if it does not have target version, Updating=False and Reason=Pending - Healthy condition -- Corresponds to the existing checks in the client prototype, taking into account the thresholds (Healthy=True if the \"bad\" condition is not holding long enough yet, but we may invent a special Reason for this case) - This card does not involve creating Health Insights when there are unhealthy operators - This card does not involve updating a ClusterVersion status insight from the CO-related data (such as completeness percentage or duration estimate)",
        "epic_key": "OTA-1260"
      },
      "OTA-1403": {
        "summary": "Create e2e test/s to ensure that the CVO is correctly reconciling the new configuration API in standalone",
        "description": "Feature gates must demonstrate completeness and reliability. As per ??Tests must contain either OCPFeatureGate:FeatureGateName or the standard upstream FeatureGate:FeatureGateName.?? ??There must be at least five tests for each FeatureGate.?? ??Every test must be run on every TechPreview platform we have jobs for. (Ask for an exception if your feature doesn't support a variant.)?? ??Every test must run at least 14 times on every platform/variant.?? ??Every test must pass at least 95% of the time on every platform/variant.?? ??If your FeatureGate lacks automated testing, there is an exception process that allows QE to sign off on the promotion by commenting on the PR.?? The introduced functionality is not that complex. The only newly introduced ability is to modify the CVO log level using the API. However, we should still introduce an e2e test or tests to demonstrate that the CVO correctly reconciles the new configuration API. The tests may be: Check whether the CVO notices a new configuration in a reasonable time. Check whether the CVO increments the observedGeneration correctly. Check whether the CVO changes its log level correctly. TODO: Think of more cases. Definition of Done: e2e test/s exists to ensure that the CVO is correctly reconciling the new configuration API",
        "epic_key": "OTA-923"
      },
      "OTA-1307": {
        "summary": "ClusterVersion status should include version-Pod error details",
        "description": "Currently the CVO launches a Job and waits for it to complete| to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so we probably want to drop it and create and monitor the Pod ourselves. Definition of done: failure modes like unretrievable image digests (e.g. quay.io/openshift-release-dev/ocp-release@sha256:0000000000000000000000000000000000000000000000000000000000000000) or images with missing or unacceptable Sigstore signatures with OTA-1304's ClusterImagePolicy) have failure-mode details in ClusterVersion's RetrievePayload message, instead of the current Job was active longer than specified deadline. Not clear to me what we want to do with reason, which is currently DeadlineExceeded. Keep that? Split out some subsets like SignatureValidationFailed and whatever we get for image-pull-failures? Other?",
        "epic_key": "OTA-1321"
      },
      "OTA-861": {
        "summary": "CVO allows new unforced updates even when it is currently midway through a partial update. It should require a force to retarget mid-update",
        "description": "Moving the bug to this Jira card Currently we had a customer that triggered the upgrade from 4.1.27 to 4.3, having intermediate versions on 4.2 in partial state. We have asked for details of the CVO from the customer to understand better the procedure taken, but we might need to implement a way to either stop the upgrade in case customer makes a mistake or block the upgrade if the customer changes the channel on the console to a version which the upgrade does not support, like in this case. As per OTA - Inhibit minor version upgrades when an upgrade is in progress We should inhibit minor version upgrades via Upgradeable=False whenever an existing upgrade is in progress. This prevents retargetting of upgrades before we've reached a safe point. Imagine: Be running 4.6.z. Request an update to 4.7.z'. CVO begins updating to 4.7.z'. CVO requests recommended updates from 4.7.z', and hears about 4.8.z\". User accepts recommended update to 4.8.z\" before the 4.7.z' OLM operator had come out to check its children's max versions against 4.8 and set Upgradeable=False. Cluster core hits 4.8.z\" and some OLM operators fail on compat violations. This should not inhibit further z-stream upgrades, but we should be sure that we catch the case of 4.6.z to 4.7.z to 4.7.z+n to 4.8.z whenever 4.7.z was not marked as Complete. Update: Eventually, the output of this card: y-then-y: blocked (originally required in this card description). z-then-y: blocked (not originally required in this card description but during the implementation we think it is good to have). y-then-z or z-then-z: accepted because we need to always allow z-upgrade to include fixes."
      },
      "OTA-209": {
        "summary": "Enable the CVO in standalone OpenShift to change its log level based on the new API",
        "description": "The ClusterVersionOperator API has been introduced in the DevPreviewNoUpgrade feature set. Enable the CVO in standalone OpenShift to change its log level based on the new API. Definition of Done: New DevPreviewNoUpgrade manifests are introduced in the OCP payload The CVO is correctly reconciling the new CR and all of its new fields The CVO is changing its log level based on the log level in the new API",
        "epic_key": "OTA-923"
      }
    },
    "epics": {
      "OTA-1260": {
        "summary": "Status API for oc adm upgrade status command",
        "description": "Epic Goal Add a new command `oc adm upgrade status` command which is backed by an API. Please find the mock output of the command output attached in this card. Why is this important? (mandatory) From the UI we can see the progress of the update. Using OC CLI we can see some of the information using \"oc get clusterversion\" but the output is not readable and it is a lot of extra information to process. Customer as asking us to show more details in a human-readable format as well provide an API which they can use for automation. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
      },
      "OTA-923": {
        "summary": "Reduce cluster version operator logging",
        "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
      },
      "OTA-1422": {
        "summary": "Extend tech-preview 'oc adm upgrade recommend' to render relevant alerts",
        "description": "Epic Goal OCPSTRAT-1834 is requesting an oc precheck command that helps customers identify potential issues before triggering an OpenShift cluster upgrade. For 4.18, we'd built a tech-preview oc adm upgrade recommend (OTA-1270, product docs?\" space, and this Epic is about extending that subcommand with alerts to deliver the coverage requested by OCPSTRAT-1834. Why is this important? We currently document some manual checks for customer admins to run before launching an update. For example, RFE-5104 is up asking to automate whatever we're hoping customer are supposed to look for vs. critical alerts. But updating the production OpenShift Update Service is complicated, and it's easier to play around in a tech-preview oc subcommand, while we get a better idea of what information is helpful, and which presentation approaches are most accessible. 4.18's OTA-902 / cvo1907 and this Epic proposes to continue in that direction by retrieving update-relevant alerts and folding those in as additional client-side Conditional Update risks. Scenarios As a cluster administrator interested in launching an OCP update, I want to run an oc command that talks to me about my next-hop options, including any information related to known regressions with those target releases, and also including any information about things I should consider addressing in my current cluster state. Dependencies The initial implementation can be delivered unilaterally by the OTA updates team. The implementation may surface ambiguous or hard-to-actuate alert messages, and those messages will need to be improved by the component team responsible for maintaining that alert. Contributing Teams (and contacts) Development - OTA Documentation - no docs required QE - OTA PX - OTA Others - Acceptance Criteria OCPSTRAT-1834 customer is happy :) Drawbacks or Risk Client-side Conditional Update risks are helpful for cluster administrators who use that particular client. But admins who use older oc or who are using the in-cluster web-console and similar will not see risks known only to newer oc. If we can clearly tie a particular cluster state to update risk, declaring that risk via the OpenShift Update Service would put the information in front of all cluster administrators, regardless of which update interface they use. However, trialing update risks client-side in tech-preview oc and then possibly promoting them to risks served by the OpenShift Update Service in the future might help us identify cluster state that's only weakly coupled to update success but still interesting enough to display. Or help us find more accessible ways of displaying that context before putting the message in front of large chunks of the fleet. Done - Checklist CI Testing - Tests are merged and completing successfully Documentation - No docs. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
      },
      "OTA-1432": {
        "summary": "oc precheck command for oc cli",
        "description": "Epic Goal oc precheck is a command that the customer can run pre-update in 4.16 and above clusters to check for a limited number of conditions that might cause issues in upgrading the cluster. this command will create a report that the customer can read before they start the upgrade. the command output will NOT make any decisions regarding to proceed with the upgrade or not. That decision should be taken by the customer after reading the report. Why is this important? (mandatory) The customer requires a command that they can run on the cluster that prints a report of cluster conditions that might affect the upgrade process. Scenarios (mandatory) customer uses oc precheck command to check for cluster conditions that can hinder the upgrade process like strict pdb, etc Dependencies (internal and external) (mandatory) this is entirely an OTA team undertaking. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - OTA Documentation - OTA docs team QE - OTA PX - Others - Acceptance Criteria (optional) OTA team ships a `oc precheck` command to check for cluster conditions. Drawbacks or Risk (optional) We can check for frequent conditions that cause issues with upgrades, but we cannot guarantee that the cluster upgrade will go smoothly even after the customer reads the report of this command as every cluster is different and there are lot of unknowns. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
      },
      "OTA-1321": {
        "summary": "ClusterVersion status should include version-Pod error details",
        "description": "Epic Goal Currently the CVO launches a Job and waits for it to complete to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so the goal of this Epic is to drop it and create and monitor the Pod ourselves, so we can deliver better reporting of version-Pod state. Why is this important? When the version Pod fails to run, the cluster admin will likely need to take some action (clearing the update request, fixing a mirror registry, etc.). The more clearly we share the issues that the Pod is having with the cluster admin, the easier it will be for them to figure out their next steps. Scenarios oc adm upgrade and other ClusterVersion status UIs will be able to display Init:SignatureValidationFailed and other version-Pod failure modes directly. We don't expect to be able to give ClusterVersion consumers more detailed next-step advice, but hopefully the easier access to failure-mode context makes it easier for them to figure out next-steps on their own. Dependencies This change is purely and updates-team/OTA CVO pull request. No other dependencies. Contributing Teams Development - OTA Documentation - OTA QE - OTA Acceptance Criteria Definition of done: failure modes like unretrievable image digests (e.g. quay.io/openshift-release-dev/ocp-release@sha256:0000000000000000000000000000000000000000000000000000000000000000) or images with missing or unacceptable Sigstore signatures with OTA-1304's ClusterImagePolicy) have failure-mode details in ClusterVersion's RetrievePayload message, instead of the current Job was active longer than specified deadline. Drawbacks or Risk Limited audience, and failures like Init:SignatureValidationFailed are generic, while CVO version-Pod handling is pretty narrow. This may be redundant work if we end up getting nice generic init-Pod-issue handling like RFE-5627. But even if the work ends up being redundant, thinning the CVO stack by removing the Job controller is kind of nice. Done - Checklist The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
      }
    }
  },
  "OpenStack as Infra": {
    "stories": {
      "OSASINFRA-3731": {
        "summary": "Consume CA bundle from CCO-provisioned credential secret in csi-operator, cluster-storage-operator",
        "description": "Since 4.18, the csi-operator contains the operators and manifests for both the Cinder and the Manila CSI Drivers. We should modify this component to consume the CA cert from the credentials secrets created by cloud-credential-operator (CCO) in response to CredentialsRequest CRs. We may wish to leave the legacy syncing code that copies the CA bundle from the openshift-cloud-controller-manager / cloud-conf config map until 4.20, to allow time for CCO to do its job and sync everything across."
      },
      "OSASINFRA-3730": {
        "summary": "Add CA bundle to root credential secret created by installer",
        "description": "The Installer creates the initial version of the root credential secret at kube-system / openstack-credentials, which cloud-credential-operator (CCO) will consume. Once we have support in CCO for consuming a CA cert from this root credential, we should modify the Installer to start populating the CA cert field. We should also stop adding the CA cert to the openshift-cloud-controller-manager / cloud-conf config map since the Cloud Config Operator (and CSI Drivers) will be able to start consuming the CA cert from the secret instead. This may need to be done separately depending on the order that patches land in.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3755": {
        "summary": "Consume CA bundle from CCO-provisioned credential secret in cloud-network-config-controller",
        "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify cloud-network-config-controller to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3747": {
        "summary": "Consume CA bundle from CCO-provisioned credential secret in cluster-image-registry-operator",
        "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify cluster-image-registry-operator to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map when Swift configuration is enabled.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3746": {
        "summary": "Consume CA bundle from CCO-provisioned credential secret in machine-api-provider-openstack",
        "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify MAPO to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map. This has the added bonus of aligning MAPO with CAPO's behavior.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3732": {
        "summary": "Publish CA cert to well-known location in hypershift",
        "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify Hypershift to use the same secret key, cacert, rather than the one it currently uses, cabundle.pem.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3657": {
        "summary": "Sync CA bundle from root credential to generated credentials",
        "description": "The cloud-credential-operator (CCO) consumes a root credential secret from kube-system / openstack-credentials and rolls it out to secrets in different namespaces, in response to CredentialsRequests CRs. We should modify CCO to start looking for a cacert field in this root credential secret and copying it to the generated secrets. This key name is chosen since it aligns with the expected secret key name used by components like CAPO and ORC.",
        "epic_key": "OSASINFRA-3729"
      },
      "OSASINFRA-3652": {
        "summary": "Remove configuration of '--feature-gates=Topology' opt",
        "description": "We currently set this for Cinder in the csi-operator. We should remove it once we have a new mechanism implemented for enabling/disable the topology feature.",
        "epic_key": "OSASINFRA-3650"
      }
    },
    "epics": {
      "OSASINFRA-3729": {
        "summary": "Sync CA bundle to credentials",
        "description": "Goal Add support for syncing CA bundle to the credentials generated by Cloud Credential Operator. Why is this important? It it generally necessary to provide a CA file to OpenStack clients in order to communicate with a cloud that uses self-signed certificates. The cloud-credential-operator syncs clouds.yaml files to various namespaces so that services running in those namespaces are able to communicate with the cloud, but it does not sync the CA file. Instead, this must be managed using another mechanism. This has led to some odd situations, such as the Cinder CSI driver operator inspecting cloud-provider configuration to pull out this file. We should start syncing not only the clouds.yaml file but also the CA file to anyone that requests it via a CredentialsRequest. Once we've done this, we can modify other components such as the Installer, CSI Driver Operator, Hypershift, and CCM Operator to pull the CA file from the same secrets that they pull the clouds.yaml from, rather than the litany of places they currently use. Scenarios As a deployer, I should be able to update all cloud credential-related information - including certificates - in one central place and see these rolled out to all components that require them. Acceptance Criteria The cloud-credential-operator is capable of consuming a CA cert from kube-system / openstack-credentials and rolling this out to the secrets in other namespaces The installer includes the CA cert in the root kube-system / openstack-credentials secret The UPI playbooks are modified to includes the CA cert in the root kube-system / openstack-credentials secret No regressions. Since we use self-signed certificates in many of our CI systems, we should see regressions early. Release notes and credential rotation documentation is updated to document this change Dependencies (internal and external) None. Previous Work (Optional): None. Open questions: None."
      },
      "OSASINFRA-3650": {
        "summary": "Add topology-awareness to Cinder CSI Driver",
        "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs to cover much of this and prevent regressions. We will need to manually test the negative case, where there is a mismatch between the set of Cinder AZs and set of Nova AZs, but this should be trivial to do. Open questions: None."
      }
    }
  },
  "Operator Runtime": {
    "stories": {
      "OPRUN-3821": {
        "summary": "Modify cluster-olm-operator to watch for the new Own/SingleNamespace InstallMode feature gate",
        "description": "Modify cluster-olm-operator to watch for the new Own/SingleNamespace InstallMode feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
        "epic_key": "OPRUN-3748"
      },
      "OPRUN-3783": {
        "summary": "Create origin tests to test preflight permission Techpreview",
        "description": "Add origin tests in the openshift/| repo using the featureGate to ensure tests run/don't run when the FeatureGate(s) are enabled/disabled. See dev guide for more info:",
        "epic_key": "OPRUN-3613"
      },
      "OPRUN-3780": {
        "summary": "Add TechPreviewNoUpgrade feature gate for permissions preflight to openshift/api",
        "description": "Add a new TechPreviewNoUpgrade feature gate for the permissions preflight feature as a PR to openshift/api (propose NewOLMPermissionsPreflight). See dev guide for more information:",
        "epic_key": "OPRUN-3613"
      },
      "OPRUN-3692": {
        "summary": "Add openshift/origin tests for catalogd web api feature gate",
        "description": "Add origin tests in the openshift/| repo using the featureGate to ensure tests run/don't run when the FeatureGates are enabled/disabled on. See dev guide for more info: AC: - openshift/origin tests that exercise the feature gate",
        "epic_key": "OPRUN-3597"
      },
      "OPRUN-3818": {
        "summary": "marketplace-operator: Update controller-runtime and consequently k8s api to latest (v0.20.3 and v0.32.3)",
        "description": "We should keep the marketplace operator updated."
      },
      "OPRUN-3782": {
        "summary": "Modify cluster-olm-operator to watch for new permissions preflight feature gate",
        "description": "Modify cluster-olm-operator to watch for the new permissions preflight feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
        "epic_key": "OPRUN-3613"
      },
      "OPRUN-3766": {
        "summary": "Add TechPreviewNoUpgrade feature gate for Own/SingleNamespace InstallMode to openshift/api",
        "description": "Add a new TechPreviewNoUpgrade feature gate for the Own/SingleNamespace InstallMode feature. See dev guide for more information: AC: - openshift/api PR for adding a new feature gate with name NewOLMOwnSingleNamespace",
        "epic_key": "OPRUN-3748"
      },
      "OPRUN-3722": {
        "summary": "UPSTREAM Consolidate catalogd and operator-controller Kustomize configs 1341",
        "description": "Consolidate the catalogd and operator-controller kustomize configuration dirs. e.g. ``` $ tree -d config config \u251c\u2500\u2500 catalogd \u2514\u2500\u2500 operator-controller ``` Goal: We should be able to render separate overlays for each project. However, de-duplication of kustomize configuration files could be done.",
        "epic_key": "OPRUN-3703"
      },
      "OPRUN-3690": {
        "summary": "Modify cluster-olm-operator to watch for new catalogd web api feature gate",
        "description": "Modify cluster-olm-operator to watch for the new catalogd web api feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
        "epic_key": "OPRUN-3597"
      },
      "OPRUN-3663": {
        "summary": "Modify cluster-olm-operator to watch feature gates",
        "description": "Modify cluster-olm-operator to allow it to respect feature gates AC: cluster-olm-operator watches for OCP's FeatureGate and reconcile OLMv1 components accordingly See RFC| for additional background and details",
        "epic_key": "OPRUN-3646"
      }
    },
    "epics": {
      "OPRUN-3748": {
        "summary": "Own/SingleNamespace InstallMode Support",
        "description": ""
      },
      "OPRUN-3613": {
        "summary": "UPSTREAM Permission validation pre-flight check 988",
        "description": "From the WIP brieferror Permission Verification do the doc| Step 1 permission verification as well as escalate/bind checking Permissions and Validation Checks SelfSubjectRulesReview runner Testing unit test suites for each of the above two (2) new e2e for this work: happy path and common failure path"
      },
      "OPRUN-3597": {
        "summary": "UPSTREAM catalogd web interface performance improvements 451 TP",
        "description": "The RFC written for identified a desire to formalize the catalogd web API, and divided work into a set of v1.0-blocking changes to enable versioned web interfaces (phase 1( and non-blocking changes to express and extend a formalized API specification (phase 2). This epic is to track the design and implementation work associated with phase 2. During phase 1 RFC review we identified that we needed more work to capture the extensibility design but didn't want to slow progress on the v1.0 blocking changes so the first step should be an RFC to capture the design goals for phase 2, and then any implementation trackers we feel are necessary. Work here will be behind a feature gate. catalogd web api performance improvements RFC 1569 serve catalog content based on supplied parameters 1606 Downstreaming this feature We need to follow this guide to downstream this feature:"
      },
      "OPRUN-3703": {
        "summary": "EPIC - (Post-Monorepo Integration) - Optimize and Streamline Controller Operator and Catalogd",
        "description": "Note: This epic only tracks the phase one of the work listed in the RFC Consolidate catalogd and operator-controller Kustomize configs 1341 DOWNSTREAM: Changes in the kustomize configs might need downstream effort"
      },
      "OPRUN-3646": {
        "summary": "OLMv1 Downstream feature gate promotion mechanics",
        "description": "OCP/Telco Definition of Done Epic Goal To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time Why is this important? Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path or breaking customers. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will work both with TechPreviewNoUpgrade and without it but will have a different set of features. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "Origin Community Distribution of Kubernetes": {
    "stories": {
      "OKD-235": {
        "summary": "Move to using pure centos os bootimages in the installer",
        "description": ""
      }
    }
  },
  "OpenShift Dev Console": {
    "stories": {
      "ODC-7778": {
        "summary": "Include quick create in Admin perspective guided tour",
        "description": "Description As a user, I want to be introduced to Quick create in the Guided tour. Acceptance Criteria Should include a step-in guided tour for quick create. Additional Details: Title: Quickly create Descriptions: Create resources in just a few steps via Git, YAML, or container images.",
        "epic_key": "ODC-7716"
      },
      "ODC-7776": {
        "summary": "Add getting started alert",
        "description": "Description As a user, I want to see getting started alert when I log in to the console for the first time. Acceptance Criteria Should add getting started alert on the Software catalog, Helm repositories and Helm releases pages. Additional Details: !image-2025-03-04-16-31-15-184.png!",
        "epic_key": "ODC-7716"
      },
      "ODC-7775": {
        "summary": "Update getting started resources section actions",
        "description": "Description As a user, I want to see actions regarding the merge perspective in getting started resources section Acceptance Criteria Update getting started resources action on the cluster overview page Update getting started resources action on the project overview page in admin perspective content for cluster overview page and project overview page will be same Additional Details: Overview page",
        "epic_key": "ODC-7716"
      },
      "ODC-7773": {
        "summary": "Add e2e tests for Favorites feature",
        "description": "Description As a user, Acceptance Criteria Add e2e tests for Favorites feature Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-7767": {
        "summary": "Expose remaining Topology components and utils to openshift-console/dynamic-plugin-sdk",
        "description": "Description As a user, I want to use Topology components in the dynamic plugin Acceptance Criteria Should expose the below Topology components and utils to dynamic-plugin-sdk getModifyApplicationAction baseDataModelGetter getWorkloadResources contextMenuActions CreateConnector createConnectorCallback (e",
        "epic_key": "ODC-7716"
      },
      "ODC-7766": {
        "summary": "Add Admin navigation menu tests for developer console items",
        "description": "Description As a user, Acceptance Criteria Have some tests testing the developer console items on the admin side Update existing tests to reflect the changes in the UI like changing Developer catalog to Software catalog Additional Details:",
        "epic_key": "ODC-7763"
      },
      "ODC-7727": {
        "summary": "Favoriting page in the Console admin perspective",
        "description": "Description As a user, I want to favorite pages in the Console admin perspective Acceptance Criteria Should favorite the pages in the console Favorite pages can be accessible from the left navigation Additional Details: Design",
        "epic_key": "ODC-7716"
      },
      "ODC-7726": {
        "summary": "Expose Topology components and utils to openshift-console/dynamic-plugin-sdk",
        "description": "Description As a user, I want to use Topology components in the dynamic plugin Acceptance Criteria Should expose the Topology components and utils to dynamic-plugin-sdk Additional Details: Utils and component needs to be exposed",
        "epic_key": "ODC-7716"
      },
      "ODC-7723": {
        "summary": "Add quick start for how to enable developer perspective",
        "description": "Description As a user, I want to know how I can enable the developer perspective in the Web console Acceptance Criteria Add a quick start to let the user know about the steps to enable the Developer perspective in the web console Additional Details: Steps to enable dev perspective through UI search for console (console.operator.openshift.io/cluster| on search page open cluster details page click on action menu and select Customize option. It will open Cluster configuration page under General tab there is Perspectives option to enable and disabled UXD:",
        "epic_key": "ODC-7716"
      },
      "ODC-7720": {
        "summary": "Add dev perspective nav options to admin perspective",
        "description": "Description As a user, I want access to all the pages present in the developer perspective from the admin perspective. Acceptance Criteria Add Topology, Helm, Serverless function, and Developer catalog nav items to Admin perspective as per the UX design. Additional Details: UX design",
        "epic_key": "ODC-7716"
      },
      "ODC-7710": {
        "summary": "Remove RHOAS plugin from the console",
        "description": "Description As a developer, I do not want to maintain the code for a project already dead. Acceptance Criteria Remove RHOAS plugin Remove RHOAS-catalog-source Check if there is dependencies in other package and fix it Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-6775": {
        "summary": "Upgrade jest from 21 to 29",
        "description": "Description of problem: For ODC-6264, and ODC-6265 as well as updating TypeScript from 3 to 4 as part of CONSOLE-2501 we want to update Jest from version 21 to 29.",
        "epic_key": "CONSOLE-4562"
      },
      "ODC-7781": {
        "summary": "Updating serverless ci tests to run in admin view",
        "description": "Description Currently, serverless tests are being run from the developer perspective. As a part of merging perspective, tests should prioritise administration perspective only Acceptance Criteria Serverless workload should be created from an admin perspective All tests should only be running in admin perspective Additional Details:",
        "epic_key": "ODC-7763"
      },
      "ODC-7780": {
        "summary": "Update all quick starts for ODC",
        "description": "Description As a user, I want to use quick starts by following the given steps. Acceptance Criteria Update the steps in the quick starts as the developer perspective is disabled by default. Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-7770": {
        "summary": "Remove perspective switcher if only one perspective is present",
        "description": "Description As a user, Acceptance Criteria criteria Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-7769": {
        "summary": "Add Getting started section on the Project overview page",
        "description": "Description As a user, Acceptance Criteria criteria Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-7725": {
        "summary": "Hide perspective preference option on user preferences page incase on one perspective",
        "description": "Description As a user, I do not want a perspective preferences option if only one perspective is enabled. Acceptance Criteria Should not show perspective preference option if only one perspective is enabled. Additional Details:",
        "epic_key": "ODC-7716"
      },
      "ODC-7724": {
        "summary": "Add guided tour in the Admin perspective",
        "description": "Description As a user, I want to know about the nav option added to the Admin perspective from the Developer perspective. Acceptance Criteria Add guided tour to the Admin perspective to let the user know about the unified perspective Additional Details: updated design",
        "epic_key": "ODC-7716"
      },
      "ODC-7698": {
        "summary": "Replace react-copy-to-clipboard with PatternFly ClipboardCopyButton component",
        "description": "Location: pipelines-plugin/src/components/repository/form-fields/CopyPipelineRunButton.tsx PF component: AC: Replace react-copy-to-clipboard with PatternFly ClipboardCopyButton component Remove react-copy-to-clipboard as a dependency",
        "epic_key": "CONSOLE-4132"
      }
    },
    "epics": {
      "ODC-7716": {
        "summary": "Merge Admin and Dev Perspectives",
        "description": "Epic Goal Base on user analytics many of customers switch back and fourth between perspectives, and average15 times per session. The following steps will be need: Surface all Dev specific Nav items in the Admin Console Disable the Dev perspective by default but allow admins to enable via console setting All quickstarts need to be updated to reflect the removal of the dev perspective Guided tour to show updated nav for merged perpspective Why is this important? We need to alleviate this pain point and improve the overall user experience for our users. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "ODC-7763": {
        "summary": "Automation enhancement for 4.19",
        "description": "Problem: This epic covers the scope of automation-related stories in ODC Goal: Automation enhancements for ODC Why is it important? Use cases: case Acceptance criteria: Automation enhancements as per the perspective merged Tests to be updated according to the default setting of having only Admin perspective Dependencies (External/Internal): Design Artifacts: Exploration: Note:"
      }
    }
  },
  "OpenShift Console": {
    "epics": {
      "CONSOLE-4562": {
        "summary": "OCP 4.20 - Console Dependencies & Tech Debt",
        "description": "Over time, our OpenShift Console has accumulated technical debt in its libraries, frameworks, and underlying infrastructure. This epic is focused on auditing, updating, and standardizing dependencies to the latest supported versions, while ensuring compatibility and minimizing user impact. Addressing this tech debt will: Reduce security vulnerabilities by patching known CVEs Improve performance through optimized libraries Streamline developer onboarding and maintenance Lay the groundwork for future features by aligning on current platform standards Goals & Objectives: Frontend Dependencies Audit third-party UI components (React, PatternFly, etc.) Upgrade to the latest stable major versions Refactor any deprecated APIs Backend Dependencies Update Go modules and middleware libraries Migrate from deprecated frameworks (if applicable) Ensure backward compatibility for REST/gRPC endpoints Infrastructure Harden CI/CD pipelines with up-to-date build agents Refresh container base images (e.g., Red Hat UBI versions) Align Kubernetes manifests with current API versions Acceptance Criteria: All frontend NPM package versions are updated to their latest non-breaking releases, with no failing unit or e2e tests. Backend Go modules have no outdated major versions; existing integration tests pass without regression. CI/CD pipeline definitions use current Docker image tags; all automated builds succeed in consumed staging clusters. No known high- or critical-severity vulnerabilities remain in project dependencies (scan report attached). Documentation updated to reflect new version requirements and rollback procedures."
      },
      "CONSOLE-4325": {
        "summary": "Adopt PatternFly 6 and remove PatternFly 4",
        "description": "Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CONSOLE-4350": {
        "summary": "OCP 4.19 - Console Dependencies & Tech Debt",
        "description": "An epic we can duplicate for each release to ensure we have a place to catch things we ought to be doing regularly but can tend to fall by the wayside."
      },
      "CONSOLE-3945": {
        "summary": "Prepare to update OCP Console React dependency",
        "description": "Epic Goal Update the OCP Console frontend React dependency to a more recent version, as the current version has been end-of-life for over a year. Why is this important? The longer we wait to make this update, the harder it will be. It's important to stay current so that we can be more nimble with tech debt and dependencies. Scenarios As a developer, I am assigned a high-priority feature. I find that React must be updated as part of the feature. I also find that because there are many breaking changes between our version and the latest, an update would be out of scope for this story. The feature must be deferred until we can address the tech debt. A major issue is found in our current version of React, and an update is required. The scope of this update work has ballooned over the time since our last update. We have to drop other important work to prioritize this update and again, other important features or work are deferred. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. OCP Console React dependency and all other tangential dependencies have been updated to an agreed-upon recent version. Other stakeholders, like plugin consumers, should not be affected by this change. Previous Work (Optional): Open questions: Should this be accomplished as a swarm activity where we spend a sprint addressing all blockers? Should we take a cautious approach and resolve blockers over time until we reach a point where an update is feasible? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CONSOLE-4352": {
        "summary": "OCP 4.19 - Address tech debt in frontend/public/components/secrets/create-secret.tsx",
        "description": "Epic Goal Migrate all components to functional components Remove all HOC patterns Break the file down into smaller files Improve type definitions Improve naming for better self-documentation Address any React anti-patterns like nested components, or mirroring props in state. Address issues with handling binary data Add unit tests to these components Acceptance Criteria Refactor secret forms Adding unit tests to these components. Fix Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CONSOLE-4132": {
        "summary": "Replace legacy custom components with PatternFly components",
        "description": "Epic Goal Goal is to locate and replace old custom components with PatternFly components. Why is this important? Custom components require supportive css to mimic the visual theme of PatternFly. Over time these supportive styles have grown and interspersed through the console codebase, which require ongoing efforts to carry along, update and maintain consistency across product areas and packages. Also, custom components can have varying behaviors that diverge from PatternFly components, causing bugs and create discordance across the product. Future PatternFly version upgrades will be more straightforward and require less work. Acceptance Criteria Identify custom components that have a PatternFly equivalent component. Create stories which will address those updates and fixes Update integration tests if necessary. Open questions: ..."
      }
    },
    "stories": {
      "CONSOLE-4542": {
        "summary": "Use PatternFly DescriptionList",
        "description": "AC: custom styles for dl, dt, dd are removed all dl, dt, dd base html elements in the source code are replaced with the PatternFly styled ones",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4540": {
        "summary": "Set as default StorageClass Action",
        "description": "1. Allow setting default StorageClass in UI 2. Currently, the process for changing (from one to another) the default StorageClass involves patching or editing annotations on both SCs manually. 3. The UI shows an indicator of which SC is default, which would imply you could/should be able to set this in UI"
      },
      "CONSOLE-4538": {
        "summary": "Use PatternFly-recommended 404 page in console",
        "description": "see AC: - 404 page uses above component",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4524": {
        "summary": "Allow Deletion of Identity Providers (IDPs) via OpenShift Web Console",
        "description": "As an OpenShift administrator, I want to be able to delete identity providers (IDPs) through the web console UI, So that I can manage authentication configurations without manually editing YAML files. Acceptance Criteria: The OpenShift web console provides a UI option to remove an existing IDP. Users can view a list of configured IDPs and select one for deletion. A confirmation prompt appears before deleting an IDP to prevent accidental removals. Appropriate success or error messages are displayed based on the outcome of the deletion. Add integration test RFE:",
        "epic_key": "CONSOLE-4334"
      },
      "CONSOLE-4523": {
        "summary": "Distribute the OpenShift CLI for both RHEL8 and RHEL9",
        "description": "As a developer I want to download rhel8 and rhel9 _oc_ binaries which are now part of the cli-artifacts image which is meing used as builder image for _downloads_ image used used for downloads _oc_ binaries. AC: Update the console-operator code to provide rhel8 and rhel9 _oc_ binaries as part of the ConsoleCLIDownloads CR Add tests RFE - This story is probably a bug, which we should backport to the version in which the cli-artifacts image started to provide additional RHEL binaries.",
        "epic_key": "CONSOLE-4562"
      },
      "CONSOLE-4521": {
        "summary": "Remove old polyfills",
        "description": "As a user, I do not want to load polyfills for browsers that OCP console no longer supports.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4517": {
        "summary": "update or remove focus-trap-react",
        "description": "warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react@0.14.x ^15.0.0 ^16.0.0\". warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react-dom@0.14.x ^15.0.0 ^16.0.0\".",
        "epic_key": "CONSOLE-3945"
      },
      "CONSOLE-4515": {
        "summary": "update or remove react-modal",
        "description": "warning \" react-modal@3.12.1\" has incorrect peer dependency \"react@^0.14.0 ^15.0.0 ^16 ^17\". warning \" react-modal@3.12.1\" has incorrect peer dependency \"react-dom@^0.14.0 ^15.0.0 ^16 ^17\".",
        "epic_key": "CONSOLE-3945"
      },
      "CONSOLE-4508": {
        "summary": "Enable CSP tests for console-operator",
        "description": "Part of lifting feature gate for the CSP API we need to enable e2e CSP tests for console-operator.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4507": {
        "summary": "Remove and/or replace Bootstrap styles with PatternFly equivalents where possible",
        "description": "There are a number of `co-` styles in the console from the days of CoreOS. We should remove and/or replace these with PatternFly equivalents where possible.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4501": {
        "summary": "Add unit tests for Timestamp component",
        "description": "Add unit tests for the Timestamp component to prevent regressions like AC: A unit test is implemented which tests the functionality of the Timestamp component The unit test exercises both the relative and full date formats.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4498": {
        "summary": "Replace checkboxes with Switch in ResourceLog",
        "description": "The checkboxes at the top of the ResourceLog component should be changed to Switches as that same change is being made for the YAMLEditor| AC: Replace CheckBox component in the ResourceLog component with Switch from PF6",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4492": {
        "summary": "Align spacing of page elements with PatternFly",
        "description": "Long before PatternFly, the page layout was controlled with `co-m-pane` and `co-m-page` classes. These classes have different padding or margin values than PatternFly. We should update these existing classes to align with PatternFly.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4484": {
        "summary": "use PF6 component for tabs",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4481": {
        "summary": "Lift the FeatureGate for the CSP API",
        "description": "The CSP epic was delivered in 4.18 the what steps need to be made in order to lift the FeatureGate. AC: Review the enhancement docs and address the remaining steps in oder to lift the CSP API FeatureGate and create follow up stories.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4448": {
        "summary": "Add the ability to specify a second custom logo for PatternFly 6 api",
        "description": "In PatternFly 6, the colour of the masthead changes depending upon the mode (light vs dark). As a result, a single custom logo may not work for both cases (as is the case with the existing OKD and OpenShift logos as they assume the masthead background is always dark and include white text as a result). We need to add the ability to specify a second logo to account for this. See AC: propose chances writing an enhancement document which should cover the fullstack change - API, console-operator, console update console operator's config API with additional field for defining second custom logo for PF6 add unit tests for the API change",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4430": {
        "summary": "Automated Content Security Policy testing of Console pages",
        "description": "In Console 4.18 we introduced an initial Content Security Policy| (CSP) implementation (CONSOLE-4263). This affects both Console web application as well as any dynamic plugins loaded by Console. In production, CSP violations are sent to telemetry service for analysis (CONSOLE-4272). We need a reliable way to detect new CSP violations as part of our automated CI checks. We can start with testing the main dashboard page of Console and expand to more pages as necessary. Acceptance criteria: Console project provides a script to test for CSP violations. CSP violation test script does not report any errors for Console.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4407": {
        "summary": "Update YAML language server and monaco",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4400": {
        "summary": "Update to TypeScript 5",
        "description": "Currently console is using TypeScript 4, which is preventing us from upgrading to NodeJS-22. Due to that we need to update TypeScript 5 (not necessarily latest version). AC: Update TypeScript to version 5 Update ES build target to ES-2021 Note: In case of higher complexity we should be splitting the story into multiple stories, per console package.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4399": {
        "summary": "Add \"Created Time\" Column to Job Listing in OCP Console",
        "description": "Introduce a \"Created Time\" column to the Job listing in the OpenShift Container Platform (OCP) console to enhance the ability to sort jobs by their creation date. This feature will help users efficiently manage and navigate through numerous jobs, particularly in environments with frequent CronJob executions and a high volume of job runs. Acceptance Criteria: Add a \"Created Time\" column to the Job listing in the OCP console. Display the creation timestamp in a format consistent with the console's date and time standards. Enable sorting of jobs by the \"Created Time\" column.",
        "epic_key": "CONSOLE-4334"
      },
      "CONSOLE-4393": {
        "summary": "Update to NodeJS v22",
        "description": "Current version of NodeJS is in the maintainance mode and we need to update to the next version with long term support which is currently NodeJS v22. Acceptance Criteria: update console builder image update demo-dynamic-plugin base image fix any related build issues",
        "epic_key": "CONSOLE-4562"
      },
      "CONSOLE-4381": {
        "summary": "Adopt PatternFly 6",
        "description": "AC: Vendor PF6 Remove all the unecessary PF5 packages and keep the patternfly/patternfly as a npm dependancy Remove the overpass font from webpack config Follow the PF6 upgrade guide. to automatically identify and fix major issues. Create a followUp stories for addressing the remaining update issues, presumably per console package. Update docs Check",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4380": {
        "summary": "Use alerts rules detail page from monitoring-plugin",
        "epic_key": "OU-224"
      },
      "CONSOLE-4379": {
        "summary": "Remove PatternFly 4",
        "description": "Before we can adopt PatternFly 6, we need to drop PatternFly 4. We should drop 4 first so we can understand what impact if any that will have on plugins. AC: Remove PF4 package. Console should not load any PF4 assets during the runtime. Remove PF4 support for DynamicPlugins - SharedModules + webpack configuration",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4378": {
        "summary": "Remove PopupKebabMenu and related code",
        "description": "PopupKebabMenu is orphaned and contains a reference to `@patternfly/react-core/deprecated`. It and related code should be removed so we can drop PF4 and adopt PF6.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4377": {
        "summary": "Update ActionItemMenu.tsx to use new DropdownItemProps from PatternFly 5",
        "description": "contains a reference to `@patternfly/react-core/deprecated`. In order to drop PF4 and adopt PF6, this reference needs to be removed.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4376": {
        "summary": "Remove orphaned ClusterConfigurationDropdownField.tsx and related code",
        "description": "This component was never finished| and should be removed as it includes a reference to `@patternfly/react-core/deprecated`, which blocks the removal of PF4 and the adoption of PF6.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4081": {
        "summary": "Address tech debt in EditSecret component",
        "description": "The EditSecret component needs to be refactored to address several tech debt issues: Remove Firehose and SecretLoadingWrapper usage Utilize the 'useK8sWatchResource' hook to fetch Integrate SecretLoadingWrapper logic into EditSecret component Improve type definitions",
        "epic_key": "CONSOLE-4352"
      },
      "CONSOLE-4076": {
        "summary": "Address tech debt in SourceSecretForm component",
        "description": "The SourceSecretForm component needs to be refactored to address several tech debt issues: Rename to AuthSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
        "epic_key": "CONSOLE-4352"
      },
      "CONSOLE-3905": {
        "summary": "Update Webpack package to version 5",
        "description": "As a developer I want to make sure we are running the latest version of webpack in order to take advantage of the latest benefits and also keep current so that future updating is a painless as possible. We are currently on v4.47.0. Changelog: By updating to version 5 we will need to update following pkgs as well: html-webpack-plugin webpack-bundle-analyzer copy-webpack-plugin fork-ts-checker-webpack-plugin AC: Update webpack to version 5 and determine what should be the ideal minor version.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-3247": {
        "summary": "Remove react-measure dependency from the console and demo plugin",
        "description": "Since react-measure is not compatible with react 18 we need to remove our react-measure dependency from the console repo. One solution is to follow patternfly and use react-resize-detector: Acceptance criteria: the console repo should no longer depend on react-measure the dynamic demo plugin should no longer depend on react-measure",
        "epic_key": "CONSOLE-3945"
      },
      "CONSOLE-4541": {
        "summary": "Deprecate VirtualizedTable and ListPageFilter and useListPageFilter",
        "description": "We plan to remove VirtualizedTable and ListPageFilter and useListPageFilter as PatternFly's Data View Table| is a suitable replacement for users, but first we must deprecrate these components. We should do this in 4.19 so the removal can happen sooner.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4516": {
        "summary": "update or remove react-tagsinput",
        "description": "warning \" react-tagsinput@3.19.0\" has incorrect peer dependency \"react@^16.0.0 ^15.0.0 ^0.14.0\".",
        "epic_key": "CONSOLE-3945"
      },
      "CONSOLE-4505": {
        "summary": "Update i18next package",
        "description": "As a developer, I want to update the i18next and react-i18next dependencies in OpenShift Console to the latest compatible versions so that we can ensure better performance, security, and compatibility with new features. Acceptance Criteria: Update i18next and react-i18next to the latest stable versions. Verify compatibility with existing translations and ensure no breaking changes. Test the application to confirm that all internationalization features work as expected. Ensure all integration tests pass successfully. Technical Notes: Check the official i18next release notes Run npm outdated or yarn outdated to check the current and latest versions.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4504": {
        "summary": "Align LogViewer theme with console theme",
        "description": "Instances of LogViewer are hard coded to use the dark theme. We should make that responsive to the user's choice the same we we are with the CodeEditor|",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4503": {
        "summary": "Replace custom Banner with PatternFly equivalent",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4496": {
        "summary": "Replace custom Checkbox filter with PatternFly equivalent",
        "description": "The API Explorer Resource details Access review page utilizes a custom Checkbox filter. This custom component is unnecessary as PatternFly offers comparable functionality. We should replace this customer component with a PatternFly one. AC: Replace the CheckBox component for the Switch in the API Explorer",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4464": {
        "summary": "Update PatternFly to official releases",
        "description": "Some of the PatternFly releases in are prereleases. Once final releases are available (v.6.2.0 is scheduled for the end of March), we should update to them. Also update to the same versions.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4462": {
        "summary": "Verify -theme-dark classes are still necessary and remove if not",
        "description": "Most of the -theme-dark classes defined in the console code base were for PF5 and are likely unnecessary in PF6 (although the version number was updated). We should evaluate each class and determine if it is still necessary. If it is not, we should remove it.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4443": {
        "summary": "Upgrade dynamic-demo-plugin to use PatternFly 6",
        "description": "The dynamic-demo-plugin in the openshift/console All affected components and styles are updated to comply with PatternFly 6 standards. Update integration tests, if necessary Plugin functionality is tested to ensure no regressions.",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4437": {
        "summary": "Automated Content Security Policy testing in CI",
        "description": "In Console 4.18 we introduced an initial Content Security Policy| (CSP) implementation (CONSOLE-4263). This affects both Console web application as well as any dynamic plugins loaded by Console. In production, CSP violations are sent to telemetry service for analysis (CONSOLE-4272). We need a reliable way to detect new CSP violations as part of our automated CI checks. We can start with testing the main dashboard page of Console and expand to more pages as necessary. Acceptance criteria: Update the release repo, so the CSP violation test script is executed as part of Console CI checks.",
        "epic_key": "CONSOLE-4350"
      },
      "CONSOLE-4434": {
        "summary": "Update login page",
        "epic_key": "CONSOLE-4325"
      },
      "CONSOLE-4409": {
        "summary": "Refactor CodeEditor to use the PF equivalent",
        "description": "Description As a user already accustomed to PatternFly-based web applications, when I use the CodeEditor within OpenShift Console, I expect the same experience as the PF CodeEditor| Acceptance Criteria The CodeEditor adopts the Patternfly design The YAML language support remains the same Additional Details:",
        "epic_key": "CONSOLE-4132"
      },
      "CONSOLE-4080": {
        "summary": "Address tech debt in KeyValueEntryForm component",
        "description": "The KeyValueEntryForm component needs to be refactored to address several tech debt issues: Rename to OpaqueSecretFormEntry Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
        "epic_key": "CONSOLE-4352"
      },
      "CONSOLE-4079": {
        "summary": "Address tech debt in GenericSecretForm component",
        "description": "The GenericSecretForm component needs to be refactored to address several tech debt issues: Rename to OpaqueSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
        "epic_key": "CONSOLE-4352"
      },
      "CONSOLE-4077": {
        "summary": "Address tech debt in BasicAuthSubform component",
        "description": "The BasicAuthSubform component needs to be refactored to address several tech debt issues: Rename to BasicAuthSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
        "epic_key": "CONSOLE-4352"
      },
      "CONSOLE-3960": {
        "summary": "Migrate react-helmet to react-helmet-async",
        "description": "Remove the react-helmet package since it is not supported in React 18. In React 19, we can remove react-helmet-async entirely and use the built-in functionality: AC: remove all usage of react-helmet from the console code remove the react-helmet dependency from the console remove shared modules for react-helmet",
        "epic_key": "CONSOLE-3945"
      },
      "CONSOLE-3414": {
        "summary": "Inconsistency in the loader/spinner/dots component used throughout the unified console",
        "description": "Inconsistency in the loader/spinner/dots component used throughout the unified console. The dots animation is used widely through the spoke clusters, but it is not a Patternfly component(This component was originally inhereted from CoreOS console). Spoke clusters also uses skeleton states on certain pages, which is a Patternfly component. Hub uses a mix of dots animation first for half a second, and then spinners and skeletons. Currently there is a discussion with PF to update with clearer guidelines. According to the current PF guidelines, we should be using the large spinner if we cannot anticipate the data being loaded, and the skeleton state if we do know. Link to doc|",
        "epic_key": "CONSOLE-4350"
      }
    }
  },
  "OpenShift Node": {
    "stories": {
      "OCPNODE-2340": {
        "summary": "BYOPKI Container runtime config controller implementation",
        "description": "As an openshit developer, I want container runtime config controller to roll out BYOPKI config from ClusterImagePolicy CR to policy.json, so the customer can verifying the image signed with cosign BYOPKI",
        "epic_key": "OCPNODE-3039"
      },
      "OCPNODE-3020": {
        "summary": "MCO Remove all the cgroupv1 references",
        "description": "This is the continuation of Remove the Cgroupsv1 references from the MCO code Bump ocp/api to include the enum removal of cgroup \"v1\" Remove the cgroupv1 references and the related kArgs settings Modify the UTs, bootstrap e2e tests",
        "epic_key": "OCPNODE-2841"
      },
      "OCPNODE-2940": {
        "summary": "update admission pieces of o/k && o/cluster-kube-apiserver PRs for comments",
        "description": "and have some updated comments",
        "epic_key": "OCPNODE-2506"
      },
      "OCPNODE-2877": {
        "summary": "API Enhancement Remove CgroupModeV1 from openshift - Enhancement Proposal, API",
        "description": "Remove the CgroupModeV1 config option from the openshift/api repository Ref: Add a CRD validation check on the CgroupMode field of the nodes.config spec to avoid the update to \"v1\" and only allow the \"v2\" and \"\" as valid values. Latest update: Raise a PR with the updated enhancement proposal to handle the removal of cgroupsv1",
        "epic_key": "OCPNODE-2841"
      },
      "OCPNODE-2842": {
        "summary": "API CONTD. Removal of Cgroupv1 code implementation",
        "description": "This story involves the following code changes Implement code changes at MCO Remove cgroupv1 based code Add an upgradeable condition set to false of the cluster operator if the cluster is using cgroupv1 Add a CEL validation to reject the setting of cgroupv1 (based on the latest enhancement proposal) Add test cases to test the removal of cgroupv1 functionality Implement code changes at openshift/api Implement code changes at openshift/kubernetes (if we use admission handler to reject setting of cgroupv1)",
        "epic_key": "OCPNODE-2841"
      },
      "OCPNODE-2596": {
        "summary": "end to end CI jobs for image signing",
        "description": "This story is to track the CI jobs that need to be added to test `ClusterImagePolicy` and `ImagePolicy` API. coverd configuration: testing the verification coordinates configs from image.config.openshift.io/cluster: support scopes from allowedRegistries ( covered root of trust type: Public key| Rekor) The tests should cover all supported configuration. track future tests OCPNODE-2951",
        "epic_key": "OCPNODE-2619"
      },
      "OCPNODE-2339": {
        "summary": "Add fields to ClusterImagePolicy API for BYOPKI",
        "description": "As an openshift developer, I want to extend the fields of ClusterImagePolicy CRD for BYOPKI verification, so the containerruntimeconfig controller can roll out the configurationto policy.json for verification.",
        "epic_key": "OCPNODE-2269"
      }
    },
    "epics": {
      "OCPNODE-3039": {
        "summary": "Tech Preview Support BYOPKI for image verification in OCP",
        "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPNODE-2841": {
        "summary": "Remove Cgroup v1 from OCP in 4.19",
        "description": "OCP/Telco Definition of Done Epic Goal Remove the support for cgroup v1 in 4.19 Why is this important? Without dependant components like systemd, RHCOS moving away from cgroups v1 it is important for the node to make this move as well. Scenarios As a system administrator I would like to make sure my cluster doesn't use cgroup v1 from 4.19 onwards Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPNODE-2506": {
        "summary": "GA User Namespaces",
        "description": "Epic Goal Prepare user namespaces for GA by enhancing SCC support and testing Why is this important? Enable nested containers use cases and enhance security Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPNODE-2619": {
        "summary": "Move ClusterImagePolicy, ImagePolicy to v1",
        "description": "OCP/Telco Definition of Done Epic Goal This epic tracks the work needed to move the ClusterImagePolicy API from v1alpha1 to v1 in OCP 4.19. It does not include any new feature requests, which will be tracked by other epics-just the API upgrade process, CI jobs and related tasks. The workflow to move to GA: v1 types in o/api client-go to generate v1 Add v1 manifest to payload and update MCO to v1 APIs update featuregate to default enabled add the v1 manifest to the payload remove the v1alpha1 manifest from the payload Wait 1-2 weeks - feature promotion in o/api And do those all in order. For 3 here, those PRs that need to be simul-merged Why is this important? Moving the ClusterImagePolicy API to a stable version v1 and announcing that OpenShift now supports Sigstore verification are key steps in helping customers strengthen their software supply chain. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPNODE-2269": {
        "summary": "Dev Preview Support BYOPKI for image verification in OCP",
        "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Cloud": {
    "stories": {
      "OCPCLOUD-2895": {
        "summary": "AWS Add ElasticFabricAdapter to conversion library",
        "description": "Background We implemented `ElasticFabricAdapter` upstream. We will get this feature in openshift after the linked rebase card merges. We need to update the conversion library to convert the new NetworkInterfaceType field between MAPI and CAPPI. Steps Add conversion for NetworkInterfaceType to the conversion library Stakeholders Cluster Infra Definition of Done NetworkInterfaceType is being converted Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2809"
      },
      "OCPCLOUD-2860": {
        "summary": "Core CAPI should support machine to node annotations propagation",
        "description": "User Story As a user I want to be able to add annotations to my machines, and have them propagate to the nodes. This will allow me to use the labels for other tasks e.g selectors. Background _Currently, MAPI supports propagating labels from machines to nodes, but CAPI does not. When we move to CAPI we will lose this feature._ _See Relevant upstream issues: Steps Understand why the discrepancy exists Determine how much work it would be for the NodeLink controller to copy the annotations Chat with upstream to see if the idea of unrestricted annotation propagation through some mechansim is palletable. Come back to the group and decide a course of action. Stakeholders Our users, who currently have this feature. Definition of Done - Code is implemented to synchronize annotations from a CAPI Machine to a Node - Our manifests for CAPI include the \"--additional-sync-machine-annotations=.\" argument. The fully generated manifests are at",
        "epic_key": "OCPCLOUD-2706"
      },
      "OCPCLOUD-2716": {
        "summary": "Core Handle Machine owner references translation between MAPI and CAPI",
        "description": "Background Presently, the mapi2capi and capi2mapi code cannot handle translations of owner references. We need to be able to map CAPI/MAPI machines to their correct CAPI/MAPI MachineSet/CPMS and have the owner references correctly set. This requires identifying the correct owner and determining the correct UID to set. This will likely mean extending the conversion utils to be able to make API calls to identify the correct owners. Owner references for non-MachineSet types should still cause an error. Steps Add a client to the conversion util constructors (members of the conversion structs?) OR handle this outside of the conversion library? Work out the correct way to convert MachineSet/CPMS owner references between namespaces Stakeholders Cluster Infra Definition of Done Owner references are correctly converted between MAPI and CAPI machines Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2713": {
        "summary": "AWS Handle credentials secret conversion to CAPI",
        "description": "Background In MAPA, we provide a way for a user to specify the credentials to use for creating and managing AWS resources. This secret comes from a CredentialsRequest and is created either by CCO, or manually, with the name `aws-cloud-credentials`. Any Machine/MachineSet referencing these credentials is effectively using the \"default\". In CAPA, the default is to us the cluster identity ref to work out what to do. When not specified, it will fallback to using the controllers role, which we populate today using a credentials request. Therefore, any Machine using the default in MAPA/CAPA, has an option to be converted across. Where we then have an issue, is converting non-standard credentials. If any user has created a non-standard credential, we must set a static identity| that would then be used for the entire cluster. We must work out how to message about this/how to handle this. Initially, we can block the conversion and suggest a KCS to allow the user to set the AWSCluster IdentityRef, once the identity ref is configured, we can ignore the credentials secret. Steps Implement detection and conversion of the default credentials secret as described above Add logic to detect non-default credentials and return an appropriate error message Create a KCS to explain the steps necessary to use a custom credential Stakeholders Cluster Infra Definition of Done Credentials secrets are converted/users are told what to do/how to convert across Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2809"
      },
      "OCPCLOUD-2712": {
        "summary": "AWS Understand security groups differences in MAPA/CAPA",
        "description": "Background When converting CAPI2MAPI, we convert CAPA's `AdditionalSecurityGroups` into the security groups for MAPA. While this looks correct, there are also fields like `SecurityGroupOverrides` which when present currently, would cause an error. We need to understand how security groups work today in MAPA, compare that to CAPA, and be certain that we are correctly handling the conversion here. Is CAPA doing anything else under the hood? Is it currently applying extra security groups that are standard that would otherwise cause issues? Steps Understand how security groups work in CAPA and MAPA Determine if our current conversion of security groups is appropriate and understand the role of securityGroupOverrides Update documentation/make appropriate changes to the security groups conversion based on the above findings. Stakeholders Cluster infra Definition of Done We are confident that converted machines behave correctly with respect to the security group configuration. Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2809"
      },
      "OCPCLOUD-2648": {
        "summary": "Handle deletion mechanics of MAPI/CAPI migration machine sync controller",
        "description": "Background To ensure higher level objects continue to operate as expected (cluster-autoscaler,mhc) we define a synchronisation of deletions for MAPI and CAPI mirrors. The behaviours outlined below will ensure that the resources continue to operate as expected and should be implemented in both the Machine and MachineSet controllers. Behaviours Ensure sync.machine.openshift.io/finalizer is present on both copies of mirrored resources Propagate deletionTimestamp from authoritative resource to non-authoritative Propagate deletionTimestamp if non-authoritative and has an owner reference (eg Machine owned by MachineSet) Remove sync finalizer from both resources when authoritative deletion finalizer is removed (start with non-authoritative) Steps Implement finalizer addition and removal in Machine and MachineSet synchronization controllers based on behaviours outlined above Stakeholders Cluster Infra Definition of Done Deletion of MAPI/CAPI resources is synchronised as defined above Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2647": {
        "summary": "Implement CAPI to MAPI Machine conversion",
        "description": "Background To enable CAPI MachineSets to still mirror MAPI MachineSets accurately, and to enable MAPI MachineSets to be implemented by CAPI MachineSets in the future, we need to implement a way to convert CAPI Machines back into MAPI Machines. These steps assume that the CAPI Machine is authoritative, or, that there is no MAPI Machines. Behaviours If no Machine exists in MAPI But the CAPI Machine is owned, and that owner exists in MAPI Create a MAPI Machine to mirror the CAPI Machine MAPI Machines should set authority to CAPI on create If a MAPI Machine exists Convert infrastructure template from InfraMachine to providerSpec Update spec and status fields of MAPI Machine to reflect CAPI Machine On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement conversion based on the behaviours outlined above using the CAPI to MAPI conversion library Stakeholders Cluster Infra Definition of Done When a CAPI MachineSet scales up and is mirrored in MAPI, the CAPI Machine gets mirrored into MAPI Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2645": {
        "summary": "Implement MAPI to CAPI Machine conversion",
        "description": "Background For the Machine controller, we need to implement a forward conversion, converting the MachineAPI Machine to ClusterAPI. This will involve creating the CAPI Machine if it does not exist, and managing the Infrastructure Machine. This card covers the case where MAPI is currently authoritative. Behaviours Create Cluster API mirror if not present CAPI mirror should be paused on create Names of mirror should be 1:1 with original Manage InfraMachine creation by converting MAPI providerSpec InfraMachine should be named based on the name of the Machine (to mirror CAPI behaviour) InfraMachine should have appropriate owner references and finalizers created Ensure CAPI Machine spec and status overwritten with conversion from MAPI Ensure Labels/Annotations copied from MAPI to CAPI On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement behaviours described above to convert MAPI Machines to CAPI Machines using conversion library Stakeholders Cluster Infra Definition of Done MAPI Machines create paused CAPI mirrors Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2564": {
        "summary": "Implement migration controller to handle authority transitions",
        "description": "Background describes a process in which the handover of the authoritative API is transitioned between Machine API and Cluster API. This process involves checking for a synchonized condition on the Machine API resource and relying on the synchronized generation and the Paused condition on both Machine API and Cluster API resoucres. A control loop should be built to handle just the transitional logic. Keeping the control loop separate will make this logic easy to reason about and easy to test. The actual sync of resources will be handled in a separate, isolated loop. It will: For each resource that supports the authoritative API Wait for spec.authoritativeAPI to be different to status.authoritativeAPI (user initiated a migration) Set the status.authoritativeAPI to migrating Wait for the previous authoritative API resource to report Paused condition true Check the synchronized condition and synchronizedGeneration fields are up to date Move the status authoritativeAPI to match the spec If the synchronized generation is not up to date, wait until the sync loop updates it Behaviours Watch Machines/MachineSets for .spec.authoritativeAPI not equal .status.authoritativeAPI Should set .status.authoritativeAPI to match .spec.authoritativeAPI when .status.authoritativeAPI is empty Exit here Check Synchronized condition on MAPI resource True, exit if not Move status.authoritativeAPI to Migrating If moving away from CAPI, pause the CAPI resource Wait on Paused condition True on old authoritative resource Check Syncrhonized condition and .status.synchronizedGeneration are up to date on MAPI resource, exit if not Error if condition False Requeue later if synchronizedGeneration not up to date Add appropriate Finalizer to new authoritative API Requeue here until we observe this in cache Remove Finalizer from old authoritative API Requeue here until we observe this in cache Move status.authoritativeAPI to match spec.authoritativeAPI and reset status.synchronizedGeneration If moving to CAPI, unpause the CAPI resource Steps Create a new control loop in the Cluster-CAPI-Operator repo Implement the logic as described above Add tests to test the transitional behaviour using envtest Stakeholders Cluster Infra Definition of Done Sync loop for migration is implemented and tested Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2500": {
        "summary": "Update CAS to recognize both upstream and openshift scale from zero annotations",
        "description": "User Story As a developer, in order to deprecate the old annotations, we will need to carry both for at least one release cycle. Updating the CAO to apply the upstream annotations, and the CAS to accept both (preferring upstream), will allow me to properly deprecate the old annotations. Background to help the transition to the upstream scale from zero annotations, we need to have the CAS recognize both sets of annotations, preferring the upstream, for at least one release cycle. this will allow us to have a window of deprecation on the old annotations. Steps update CAS to recognize both annotations add a unit test to ensure prioritization works properly Stakeholders openshift eng Definition of Done CAS can recognize both sets of annotations Docs n/a Testing unit testing for priority behavior",
        "epic_key": "OCPCLOUD-2136"
      },
      "OCPCLOUD-2202": {
        "summary": "Each cluster should have a Cluster object in the openshift-cluster-api namespace",
        "description": "Background The CAPI operator should ensure that, for clusters that are upgraded into a version of openshift supporting CAPI, that a Cluster object exists in the openshift-cluster-api namespace with the name as the infratructure ID of the Cluster. The cluster spec should be populated with the reference to the infrastructure object and the status should be updated to reflect that the control plane is initialized. Steps Extend the existing cluster| controller to manage the Cluster resource within CAPI operator Ensure that on supported platforms it populates a Cluster object for the cluster Add documentation to the CAPI operator to describe the controller and its operation Add testing to track the operation of the controller Ensure the controller does not interfere with Cluster resources that were not created by it Stakeholders Cluster Infra Definition of Done When I install a tech preview cluster, I should be able to `oc get cluster -n openshift-cluster-api` and have a result returned without any action on my part Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2634"
      },
      "OCPCLOUD-2880": {
        "summary": "Implement MachineSet label selector and Machine Label MAPI-CAPI Conversion",
        "description": "Background MAPI MachineSets have label selector to machine Label mapping. There are CAPI equivalents for these labels. During MachineSet/Machine conversion we should convert to the MAPI/CAPI equivalent. {code:java} machine.openshift.io/cluster-api-cluster: clusterID --- cluster.x-k8s.io/cluster-name: clusterID{code} {code:java} machine.openshift.io/cluster-api-machine-type: role machine.openshift.io/cluster-api-machine-role: role --- node-role.kubernetes.io/role: \"\" {code} {code:java} machine.openshift.io/cluster-api-machineset: clusterID-role-region --- cluster.x-k8s.io/set-name:clusterID-role-region {code} Steps Look into upstream code to determine if there are any possible values for the above labels that we have not accounted for. Determine if there is difference between MAPI type and role label and if they can be consolidated into one CAPI label. Add the conversion to the conversion library. Stakeholders Cluster Infra Definition of Done Conversion library converts between MAPI-CAPI for labels an Label selector on machines and machinesets Docs Add docs requirements for this card Testing Add unit tests",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2824": {
        "summary": "Configure rebasebot periodics to run on cluster api repositories",
        "description": "User Story As a developer I want rebasebot to periodically update cluster api repositories. Background Created this card to track merging my PR that was hit with few blockers. We ran the required rebases as rehearsal jobs bud did not merge the PR. Steps Move capi manifest gen hook script from the open PR to cluster-capi-operator Configure latests source fetching after implemented in rebasebot (OCPCLOUD-2757) Merge Stakeholders Cluster Infra Definition of Done Cluster api rebases run as periodic ci job",
        "epic_key": "OCPCLOUD-2593"
      },
      "OCPCLOUD-2718": {
        "summary": "AWS Handle no MAPI ebs volumesize",
        "description": "Background VolumeSize on the block device mapping spec in MAPA is currently optional (and if is not set we send an empty value to AWS and let it choose for us), where it is required and a minimum of 8gb in CAPA. We need to determine an appropriate behaviour for when the value is unset. Steps Check historically on the installer to see what value it typically has set (in the AMI, and if that changed overtime) Determine an appropriate minimum size for the root volume in OpenShift When not set, default the CAPA volume size to an appropriate value based on the above Adjust conversion logic based on the above Stakeholders Cluster Infra Definition of Done Machines with no volume size in MAPI can be converted to CAPI Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2809"
      },
      "OCPCLOUD-2680": {
        "summary": "Core CAPI should support machine to node label propagation",
        "description": "User Story As a user I want to be able to add labels/taints/annotations to my machines, and have them propagate to the nodes. This will allow me to use the labels for other tasks e.g selectors. Background _Currently, MAPI supports propagating labels from machines to nodes, but CAPI does not. When we move to CAPI we will lose this feature._ _See Relevant upstream issues: Steps Understand why the discrepancy exists Determine how much work it would be for the NodeLink controller to copy the labels Chat with upstream to see if the idea of unrestricted label propagation through some mechansim is palletable. Come back to the group and decide a course of action. Stakeholders Our users, who currently have this feature. Definition of Done - Code is implemented upstream to sync labels from a Machine to a Node - Our manifests include the \"--additional-sync-machine-labels=.\" argument. The generated manifests are at",
        "epic_key": "OCPCLOUD-2706"
      },
      "OCPCLOUD-2644": {
        "summary": "Implement MAPI to CAPI MachineSet conversion",
        "description": "Background For the MachineSet controller, we need to implement a forward conversion, converting the MachineAPI MachineSet to ClusterAPI. This will involve creating the CAPI MachineSet if it does not exist, and managing the Infrastructure templates. This card covers the case where MAPI is currently authoritative. Behaviours Create Cluster API mirror if not present CAPI mirror should be paused on create Names of mirror should be 1:1 with original Manage InfraTemplate creation by converting MAPI providerSpec InfraTemplate naming should be based on hash to be able to deduplicate InfraTemplate naming should be based on parent resources InfraTemplate should have ownerReference to CAPI MachineSet If template has changed, remove ownerReference from old template. If no other ownerReferences, remove template. Should be identifiable as created by the sync controller (annotated?) Ensure CAPI MachineSet spec and status overwritten with conversion from MAPI Ensure Labels/Annotations copied from MAPI to CAPI On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement MAPI to CAPI conversion by leveraging library for conversion and applying above MachineSet level rules Stakeholders Cluster Infra Definition of Done When a MAPI MachineSet exists, a CAPI MachineSet is created and kept up to date if there are changes Docs Add docs requirements for this card Testing Explain testing that will be added",
        "epic_key": "OCPCLOUD-2120"
      },
      "OCPCLOUD-2642": {
        "summary": "Setup OCP build of Azure Service Operator",
        "description": "User Story The Cluster API provider Azure has a deployment manifest that deploys Azure service operator from mcr.microsoft.com/k8s/azureserviceoperator:v2.6.0 image. We need to set up OpenShift builds of the operator and update the manifest generator to use the OpenShift image. Background Azure have split the API calls out of their provider so that they now use the service operator. We now need to ship service operator as part of the CAPI operator to make sure that we can support CAPZ. Steps -Request for the ASO repo to be created and build in Openshift- -Set up release repo configuration and basic testing for ASO repo- -Set up ART build for ASO repo- Fill out prodsec survey ( Update the manifest generator in cluster-capi-operator to replace the image with our image stream. Manifest generator should know how to filter parts of ASO so that we only ship the parts of Azure care about Create manifests for deploying the subset of required ASO that we need Stakeholders Cluster Infrastructure Definition of Done CAPZ deploys OpenShift version of ASO Docs Testing"
      }
    },
    "epics": {
      "OCPCLOUD-2809": {
        "summary": "MAPI/CAPI Feature Parity (AWS) (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2706": {
        "summary": "MAPI/CAPI Feature Parity (Core) (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2780": {
        "summary": "Support AWS Capacity Blocks for ML in MAPI",
        "description": "OCP/Telco Definition of Done Epic Goal MAPI supports CapacityReservations, but it has the missing functionality that supports AWS capacity bloks. Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2120": {
        "summary": "Implement Migration core for MAPI to CAPI (Tech Preview)",
        "description": "OCP/Telco Definition of Done Epic Goal Create the core/common tooling needed to enable the migration designed in OCPCLOUD-1578 To allow providers to individually migrate from MAPI to CAPI Implementation plan in Why is this important? We need to build out the core so that development of the migration for individual providers can then happen in parallel Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2136": {
        "summary": "Update autoscaling annotations to accommodate upstream keys",
        "description": "Epic Goal Update the scale from zero autoscaling annotations on MachineSets to conform with the upstream keys, while also continuing to accept the openshift specific keys that we have been using. Why is this important? This change makes our implementation of the cluster autoscaler conform to the API that is described in the upstream community. This reduces the mental overhead for someone that knows kubernetes but is new to openshift. This change also reduces the maintenance burden that we carry in the form of addition patches to the cluster autoscaler. By changing our controllers to understand the upstream annotations we are able to remove extra patches on our fork of the cluster autoscaler, making future maintenance easier and closer to the upstream source. Scenarios A user is debugging a cluster autoscaler issue by examining the related MachineSet objects, they see the scale from zero annotations and recognize them from the project documentation and from upstream discussions. The result is that the user is more easily able to find common issues and advice from the upstream community. An openshift maintainer is updating the cluster autoscaler for a new version of kubernetes, because the openshift controllers understand the upstream annotations, the maintainer does not need to carry or modify a patch to support multiple varieties of annotation. This in turn makes the task of updating the autoscaler simpler and reduces burden on the maintainer. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Scale from zero autoscaling must continue to work with both the old openshift annotations and the newer upstream annotations. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - OpenShift code and tests merged: link to meaningful PR or GitHub Issue DEV - OpenShift documentation merged: link to meaningful PR or GitHub Issue DEV - OpenShift build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - OpenShift documentation merged: link to meaningful PR please note, the changes described by this epic will happen in OpenShift controllers and as such there is no \"upstream\" relationship in the same sense as the Kubernetes-based controllers."
      },
      "OCPCLOUD-2634": {
        "summary": "(Infrastructure) Cluster generation for Cluster API platforms",
        "description": "OCP/Telco Definition of Done Epic Goal To add support for generating Cluster and Infrastructure Cluster resources on Cluster API based clusters Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2889": {
        "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2882": {
        "summary": "GCP - Add support to deploy Confidential VMs using AMD SEV-SNP",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using AMD SEV-SNP technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using SEV-SNP technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OCPCLOUD-2593": {
        "summary": "Rebasebot: lifecycle hooks",
        "description": "Epic Goal Rebasebot| needs to support customizable system for running repository specific tooling before/during/afeter rebase. This is primarily required for automatic rebases of CAPI provider repositories. Other uses for this feature are expected. Why is this important? Further automation of our rebase process will allow us to focus more on development instead of maintenance. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Application Platform Engineering": {
    "stories": {
      "OAPE-163": {
        "summary": "As a developer, I want to add e2e tests for MachineNamePrefix when the field is reset",
        "epic_key": "OAPE-142"
      },
      "OAPE-148": {
        "summary": "As a developer, I want to promote MachineNamePrefix feature to default featureset",
        "description": "Promote the _CPMSMachineNamePrefix_ featuregate to default featureset and ensure that the gate is enabled by default.",
        "epic_key": "OAPE-142"
      },
      "OAPE-147": {
        "summary": "As a developer, I want to add e2e tests for OnDelete update strategy",
        "description": "Add E2E tests (both pre-submit and periodic) for MachineNamePrefix with OnDelete update strategy",
        "epic_key": "OAPE-142"
      },
      "OAPE-126": {
        "summary": "Include CPMSMachineNamePrefix feature-gate name in e2e tests",
        "description": "Include `CPMSMachineNamePrefix` feature gate name in e2e tests. This is required for sippy tool to filter the e2e tests specific to this featuregate. xRef:",
        "epic_key": "OAPE-142"
      },
      "OAPE-96": {
        "summary": "As a developer, I want to bump o/api into o/kubernetes repo to update API godoc",
        "description": "As a developer, I want to bump o/api into o/kubernetes repo to update API godoc added in OAPE-94",
        "epic_key": "OAPE-26"
      },
      "OAPE-94": {
        "summary": "As a developer, I want to update the API godoc to document that manual intervention is required for using externalCertificate field",
        "description": "Update API godoc to document that manual intervention is required for using {{{}.spec.tls.externalCertificate{}}}. Something simple like: \"The Router service account needs to be granted with read-only access to this secret, please refer to openshift docs for additional details.\"",
        "epic_key": "OAPE-26"
      },
      "OAPE-92": {
        "summary": "As a developer, I want to promote the feature to default featureset",
        "description": "Promote the _RouteExternalCertificate_ featuregate to default featureset and ensure that the gate is enabled by default.",
        "epic_key": "OAPE-26"
      },
      "OAPE-91": {
        "summary": "As a developer, I want to add E2E tests for the entire feature",
        "description": "Add E2E tests for the entire feature in _openshift/origin_ to ensure feature stability.",
        "epic_key": "OAPE-26"
      },
      "OAPE-78": {
        "summary": "As a developer, I want to add techpreview periodic jobs for all cloud providers to signal feature stabilization",
        "description": "To be able to gather test data for this feature, we will need to introduce tech preview periodics, so we need to duplicate each of and add the techpreview configuration. It's configured as an env var, so copy each job, add the env var, and change the name to include -techpreview as a suffic env: FEATURE_SET: TechPreviewNoUpgrade",
        "epic_key": "OAPE-16"
      },
      "OAPE-19": {
        "summary": "As a developer, I want to support prefix name formats to control plane machines via CPMS-operator",
        "description": "Utilize the new field added in openshift/api and add the implementation so that custom name (prefix) formats can be assigned to Control Plane Machines via CPMS. All the changes should be behind the feature gate. This prefix will supersede the current usage of the control plane label and role combination we use today The names must still continue to be suffixed with charsidx as this is important to the operation of CPMS Add unit tests and E2E tests.",
        "epic_key": "OAPE-16"
      },
      "OAPE-18": {
        "summary": "As a developer, I want to vendor openshift/api changes into cpms-operator",
        "description": "Bump openshift/api to vendor machineNamePrefix field and CPMSMachineNamePrefix feature-gate into cpms-operator",
        "epic_key": "OAPE-16"
      }
    },
    "epics": {
      "OAPE-142": {
        "summary": "GA Ability to assign custom name formats to Control Plane Machines via CPMS",
        "description": "OCP/Telco Definition of Done Epic Goal Placeholder to track GA activities for OAPE-16 feature. Why is this important? ... Scenarios ... Acceptance Criteria Moving the feature to default feature set. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "OAPE-26": {
        "summary": "GA Support router to load secrets",
        "description": "Placeholder to track GA work for CFE-811"
      },
      "OAPE-16": {
        "summary": "TP Ability to assign custom name formats to Control Plane Machines via CPMS",
        "description": "Epic Goal Provide a new field to the CPMS that allows to define a Machine name prefix This prefix will supersede the current usage of the control plane label and role combination we use today The names must still continue to be suffixed with chars-idx as this is important to the operation of CPMS Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. DEV - Downstream code and tests merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "Network Observability": {
    "stories": {
      "NETOBSERV-2023": {
        "summary": "Implement a quickstart for netobserv operator",
        "description": "OCP Console provides quickstarts as CRD to add items under the help menu Those quickstarts can be linked to the getting started card on the dashboard overview page The quickstart should contain - the steps to install and configure netobserv - an overview of the resources usage and the capabilities",
        "epic_key": "NETOBSERV-1940"
      },
      "NETOBSERV-2029": {
        "summary": "Implement a CLI Download in OCP Core",
        "description": "Similarly to the quickstart, the console also allow to create a CR to add a CLI download section: We should add by default in OCP a CR about netobserv CLI",
        "epic_key": "NETOBSERV-1940"
      }
    },
    "epics": {
      "NETOBSERV-1940": {
        "summary": "Console plugin configuration and status panel",
        "description": "Epic Goal Create a new view in netobserv console plugin to configure FlowCollector and get some status information. This must address some of the limitations related to the generic OLM install form, which lacks of flexibility in organizing the UI elements to offer a good UX. This may also include a resource footprint calculator that would help users taking more informed decisions before installing the FlowCollector (nice to have - might be a follow-up and/or a separate tool) Finally, once the FlowCollector is installed, some basic status information will be provided such as: Current FC status (components readiness etc.) FC Status warnings (same warnings as in the validation webhook) Extra warnings / recommendations, such as recommending to install Kafka depending on the number of nodes LokiStack status / readiness when relevant A FlowMetrics section with list of installed metrics + estimated cardinality + actual cardinality Links to netobserv dashboards Why is this important? Make it easier, less intimidating, to configure: despite installation being well documented, having a good UX for guiding should make users happier. Current OLM form is too limited. Also, all the options offered are often seen as intimidating. Remove uncertainty before installation: many users are afraid of the potential cost in resource footprint and want to anticipate it. A calculator will help reduce/remove the uncertainty. Increase visibility of issues: some issues might remain unnoticed, such as configuration warnings or high metrics cardinality. We want to make them more visible."
      }
    }
  },
  "Network Edge": {
    "stories": {
      "NE-2017": {
        "summary": "Enable GatewayAPIController feature gate in Default feature set",
        "description": "The goal of this story is to enable the GatewayAPIController feature gate| in the Default feature set. Currently, this feature gate is enabled only in the DevPreview and TechPreview feature sets. This should be the final implementation story in the epic, ensuring that all required implementation tasks are completed beforehand. Additionally, before enabling GatewayAPIController in the Default feature set, we must have at least five origin tests in place, as required (see for implementation of the tests).",
        "epic_key": "NE-1942"
      },
      "NE-2008": {
        "summary": "Add GRPCRoute tests to compliance testing",
        "epic_key": "NE-1117"
      },
      "NE-1994": {
        "summary": "Verify that Istio's manual deployment feature is not enabled",
        "description": "Add a test to cluster-ingress-operator's E2E tests to verify that Istio is configured not to allow manual deployment.",
        "epic_key": "NE-1933"
      },
      "NE-1970": {
        "summary": "Defining FeatureGate e2e tests: GatewayAPIController featuregate",
        "description": "After reviewing the existing GWAPI e2e tests| for GatewayAPIController featuregate and QE new designed cases in Polarion, we will cover below tests in Origin e2e: 1 OSSM installation succeed (check related resources) and default gatewayclass is accepted. 2 can create and delete custom gatewayclass (delete custom should not affect default and istiod-openshift-gateway) 3 Gateway object could be created (check LoadBalancer service, DNSRecords) 4 HTTPRoute object could be created (using separated gateway) 5 (negative) The OSSM related resource gets recreated after delete it (Subscription, Istio) 6 (negative) The Gateway related resource gets recreated after delete it (LoadBalancer service, DNSRecords...)",
        "epic_key": "NE-1942"
      },
      "NE-1969": {
        "summary": "Updates to CIO logic for Gateway API CRD Management",
        "description": "What? On OCP 4.19 onward we will ensure the Gateway API CRDs are present a specific version with its own feature gate which will default to true. If we can not ensure the CRDs are present at the expected version we will mark the cluster degraded. Why? See the description of NE-1898. How? The Cluster Ingress Operator (CIO)",
        "epic_key": "NE-1898"
      },
      "NE-1953": {
        "summary": "Validating Admission Policy for Gateway API CRD Management",
        "description": "What? The purpose of this task is to provide API validation on OCP that blocks upgrades to Gateway API CRDs from all entities except the platform itself. Why? See the description of NE-1898. How? We will use a Validating Admission Policy (VAP) Blocking in the VAP should occur at the group level, meaning only the CIO is capable of creating or changing any CRDs across the entire group at any version. As such this VAP will block access to ALL Gateway API CRDs, not just the ones we use (GatewayClass, Gateway, HTTPRoute, GRPCRoute, ReferenceGrant). Note that this means experimental APIs (e.g. TCPRoute, UDPRoute, TLSRoute) and older versions of APIs (e.g. v1beta1.HTTPRoute) are restricted as well from creation/modification. The effect should be that only the standard versions of GatewayClass, Gateway, HTTPRoute, GRPCRoute and ReferenceGrant (at the time of writing, these fully represent the standard APIs| are present and nobody can modify those, or deploy any others. This VAP should be deployed alongside the CIO manifests, such that it is deployed along with the CIO itself. Prior Art Example of a VAP restricting actions to a single entity: Helpful Links Here's where the current operator manifests can be found:",
        "epic_key": "NE-1898"
      },
      "NE-1934": {
        "summary": "Bump to OSSM 3.0",
        "description": "Update cluster-ingress-operator to install OSSM 3.0.",
        "epic_key": "NE-1933"
      },
      "NE-1907": {
        "summary": "GWAPI Implement OSSM Subscription Pinning",
        "epic_key": "NE-1817"
      },
      "NE-1870": {
        "summary": "Submit PR to fix OWNERS files in openshift/origin",
        "epic_key": "NE-1865"
      },
      "NE-1790": {
        "summary": "Enable Dynamic Configuration Manager",
        "description": "The goal of this user story is to combine the code from the smoke test user story into an implementation PR. Since multiple gaps were discovered a feature gate will be needed to ensure stability of OCP before the feature can be enabled by default.",
        "epic_key": "NE-879"
      },
      "NE-2009": {
        "summary": "Enable GatewayAPI feature gate in Default feature set",
        "description": "The goal of this story is to enable the GatewayAPI feature gate This should be the final implementation story in the epic, ensuring that all required implementation tasks are completed beforehand. Additionally, before enabling GatewayAPI in the Default feature set, we must have at least five origin tests in place, as required.",
        "epic_key": "NE-1898"
      },
      "NE-1968": {
        "summary": "Defining FeatureGate e2e tests: GatewayAPI featuregate",
        "description": "see thread: and the tests would be covered in Origin are: Verify Gateway API CRDs and esnure required CRDs should already be installed Verify Gateway API CRDs and ensure existing CRDs can not be deleted Verify Gateway API CRDs and ensure existing CRDs can not be updated Verify Gateway API CRDs and ensure CRD of standard group can not be created Verify Gateway API CRDs and ensure CRD of experimental group is not installed Verify Gateway API CRDs and ensure CRD of experimental group can not be created",
        "epic_key": "NE-1942"
      },
      "NE-1957": {
        "summary": "Test DNS Records creation for Gateways with unique and overlapping hostnames",
        "description": "Test how cluster ingress operator handles DNS record creation when multiple Gateways are created with unique and overlapping hostnames, to ensure the correct DNS resolution. Test cases: - Create multiple Gateways that have listeners with the same hostname, as well as with differing hostnames and test DNS flow. Expectation: DNS should create separate DNSRecords for each unique hostname. - Create a Gateway with Listeners that don't have a hostname: Expectation: No DNSRecord should be created. Requests with empty hostname value are processed as a fallback, after the least specific matches are exhausted. - Gateway listener trying to claim a fully qualified hostname (abc.apps.DNS config base domain) that would match a .apps.DNS config base domain ingress controller wildcard. Which endpoint tries to serve the request to abc.apps.DNS config base domain? Expectation: DNSRecord for abc.apps.DNS config base domain gets created. Traffic for the most specific match should be resolved to the gateway. - DNSRecords handling when the associated Gateway is deleted or hostname was deleted or modified on the Gateway. Expectation: DNSRecords associated with the removed Gateway also get deleted.",
        "epic_key": "NE-1769"
      },
      "NE-1954": {
        "summary": "Implement GatewayAPI Controller featuregate",
        "description": "What? Add a new featuregate for OSSM installation, and move OSSM installation from the existing GatewayAPI feature gate to the new separate featuregate, so we have one featuregate only for CRDs and one featuregate only for installing OSSM. This will help us with staging component releases.",
        "epic_key": "NE-1898"
      },
      "NE-1936": {
        "summary": "Bump cluster-ingress-operator to Kubernetes 1.32 for 4.19",
        "description": "Description of problem The openshift/cluster-ingress-operator repository| vendors k8s.io/ v0.31.1. OpenShift 4.19 is based on Kubernetes 1.32. Version-Release number of selected component (if applicable) 4.19. How reproducible Always. Steps to Reproduce Check Actual results The k8s.io/ packages are at v0.31.1. Expected results The k8s.io/ packages are at v0.32.0 or newer.",
        "epic_key": "NE-1935"
      },
      "NE-1908": {
        "summary": "Add instructions for keepalived-ipfailover image testing"
      },
      "NE-1871": {
        "summary": "Update Gateway API Feature Gate to include Tech Preview",
        "description": "Update the existing feature gate to enable Gateway API in clusters with either the DevPreviewNoUpgrade or TechPreviewNoUpgrade feature set.",
        "epic_key": "NE-1747"
      },
      "NE-1277": {
        "summary": "Add Gateway API to must-gather results",
        "description": "GWAPI and istio logs are not in the must-gather reports. Add Gateway API resources and possibly OSSM resources to the operator's relatedObjects field.",
        "epic_key": "NE-1117"
      }
    },
    "epics": {
      "NE-1942": {
        "summary": "Operator and Origin E2E tests for the gateway controller and CRD life-cycle management e2e testing automation",
        "description": "Use cases: As a developer I would like to test for unacceptable failures that exist in the Gateway API with Ingress product. This Epic is a place holder for stories regarding e2e and unit tests that are missing for old features and to determine whether OSSM 3.x TP2 bugs affect us before they are fixed in GA. There is already one epic for DNS and test cases should be added for any new features in the release. Write and run test cases that are currently missing."
      },
      "NE-1117": {
        "summary": "Gateway controller implementation",
        "description": "Epic Goal Add Gateway API via Istio Gateway implementation as GA in future release Problem: As an administrator, I would like to securely expose cluster resources to remote clients and services while providing a self-service experience to application developers. GA: A feature is implemented as GA so that developers can issue an update to the Tech Preview MVP and: can no longer change APIs without following a deprecating or backwards compatibility process. are required to fix bugs customers uncover must support upgrading the cluster and your component provide docs provide education to CEE about the feature must also follow Red Hat's support policy for GA Why is this important? Reduces the burden on Red Hat developers to maintain IngressController and Route custom resources Brings OpenShift ingress configuration more in line with standard Kubernetes APIs Demonstrates Red Hat\u2019s leadership in the Kubernetes community. Scenarios ... Acceptance Criteria Gateway API and Istio Gateway are in an acceptable standing for GA Istio Gateway installation without sidecars enabled Decision completed on whether a new operator is required, especially for upgrade and status reports Decision completed on whether Ingress-Gateway (or Route-Gateway) translation is needed Enhancement Proposals, Migration details, Tech Enablement, and other input for QA and Docs as needed API server integration, Installation, CI, E2E tests, Upgrade details, Telemetry as needed TBD Dependencies (internal and external) OSSM release schedule aligned with OpenShift's cadence, or workaround designed ...tbd Previous Work (Optional): Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "NE-1933": {
        "summary": "Bump to OSSM 3.0.0",
        "description": "Epic Goal The logic in cluster-ingress-operator that installs and configures OSSM 2.y should be updated to install and configure OSSM 3.y. Why is this important? The GA release of the OpenShift Gateway API feature will be based on OSSM 3.y. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (+) Priority is set by engineering. - (+) Epic must be Linked to a Parent Feature. - (-) Target version+ must be set. - (+) Assignee must be set. - (+) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown. - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) 1. OSSM 3.0 (currently in Tech Preview). Previous Work 1. NE-1105. Open questions 1. Will any further changes be required between OSSM 3.0 Tech Preview and OSSM 3.0 GA? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "NE-1898": {
        "summary": "CRD Lifecycle Management for Gateway API",
        "description": "Overview Gateway API in upstream Kubernetes. OpenShift Service Mesh (OSSM) Microshift and OpenShift AI OCP will be fully in charge of managing the life-cycle of the Gateway API enacts a process called \"CRD Management Succession\" to ensure the transfer of control occurs safely, which includes multiple pre-upgrade checks and CIO startup checks. Acceptance Criteria If not present the Gateway API CRDs should be deployed at the install-time of a cluster, and management thereafter handled by the platform Any existing CRDs not managed by the platform should be removed, or management and control transferred to the platform Only the platform can manage or make any changes to the Gateway API CRDs, others will be blocked Documentation about these APIs, and the process to upgrade to a version where they are being managed needs to be provided Cross-Team Coordination The organization as a whole needs to be made aware of this as new projects will continue to pop up with Gateway API support over the years. This includes (but is not limited to) OSSM Team (Istio) Connectivity Link Team (Kuadrant) MicroShift Team OpenShift AI Team (KServe) Importantly our cluster infrastructure work with Cluster API (CAPI) OCPCLOUD-2114 Investigate lifecycle of Cluster API APIs within OpenShift"
      },
      "NE-1817": {
        "summary": "Pin OSSM Subscription to a Compatible Version for Gateway API Support",
        "description": "Template: Networking Definition of Planned Epic Goal: Guarantee a compatible OSSM version is installed for Gateway API Why is this important? The ingress operator manages OSSM operator and Istio configuration for Gateway API support, but these resources can change in future releases in potentially incompatible ways. In order to guarantee that the OSSM and Istio versions are compatible with the ingress operator in a given OpenShift release, the ingress operator should install a known good version, rather than the latest. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Needs to be OSSM 3.x.x+ so that we're more up to date with upstream Istio ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. NE-1326: Investigate OSSM subscription so we can minimize surprise interoperability issues Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "NE-1865": {
        "summary": "Tech Debt Fix OWNERS files in openshift/origin",
        "description": "Add a NID alias to OWNERS_ALIASES and update the OWNERS file in test/extended/router and add OWNERS file to test/extended/dns"
      },
      "NE-879": {
        "summary": "Enable the dynamic config manager",
        "description": "Goal To make the current implementation of the HAProxy config manager should not be used to reduce the impact of the feature. - Limit dynamic server allocation -- Set the maximum number of dynamic servers| to a minimal value to prevent high resource consumption. - Provide customer opt-out -- Offer customers a handler to opt out of the default config manager implementation."
      },
      "NE-1260": {
        "summary": "Operator CI job for the upstream Gateway API conformance test suite",
        "description": "The team agrees, we should be running the upstream GWAPI conformance tests, as they are readily available and we are an integration product with GWAPI. We need to answer these questions asked at the March 23, 2023 GWAPI team meeting: Would it make sense to do it as an optional job in the cluster-ingress-operator? Is OSSM running the Gateway Conformance test in their CI? Review what other implementers do with conformance tests to understand what we should do (Do we fork the repo? Clone it? Make a new repo?)"
      },
      "NE-1769": {
        "summary": "Operator E2E tests for gateway DNS management",
        "description": "Use cases: As a customer I would like to understand how and when DNS records are created for my Gateway API resources. As a developer I would like to fix any issues that are not acceptable, or document those that cannot be resolved. See Enhancement Proposal - new controller to manage dns records for gateway listeners and Write and run unit test cases to find answers to the following about the current Gateway API DNS reconciliation: Does it work with multiple Gateways? Create multiple Gateways that have listeners with same hostname, as well as with differing hostnames and test DNS flow. What happens for a Gateway with Listeners that don't have a hostname? -Does it ignore Services that are not associated with the Gateway controller?- What happens if a Gateway listener tries to claim a name (abc.apps.example.com) that would match a .apps.example.com ingress controller wildcard. Which endpoint tries to serves the request to abc.apps.example.com? Other important factors TBD. Acceptance Criteria: new unit test cases, documentation, and update to enhancement document if needed."
      },
      "NE-1935": {
        "summary": "Update components to use Kubernetes 1.32 packages",
        "description": "Epic Goal Bump vendored Kubernetes packages (k8s.io/api, k8s.io/apimachinery, k8s.io/client-go, etc.) to v0.32.0 or newer version. Why is this important? Keep vendored packages up to date. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to ToDo status - (+) Priority+ is set by engineering - (x) Epic must be Linked to a Parent Feature - (-) Target version must be set - (+) Assignee must be set - (x) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated -Release Technical Enablement - Provide necessary release enablement details and documents.- Dependencies (internal and external) 1. Other vendored dependencies (such as openshift/api and controller-runtime) may also need to be updated to Kubernetes 1.32. Previous Work (Optional) 1. NE-1875. Open questions None. Done Checklist CI - CI is running, tests are automated and merged. -Release Enablement link to Feature Enablement Presentation- DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue -DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue- DEV - Downstream build attached to advisory: link to errata -QE - Test plans in Polarion: link or reference to Polarion- -QE - Automated tests merged: link or reference to automated tests- -DOC - Downstream documentation merged: link to meaningful PR-"
      },
      "NE-1747": {
        "summary": "Graduate \"GatewayAPI\" featuregate to TechPreviewNoUpgrade",
        "description": "As a developer, I need a featuregate to develop behind so that the Gateway API work does not impact other development teams until tests pass. The featuregate is currently in the DevPreviewNoUpgrade featureset. We need to graduate it to the TechPreviewNoUpgrade featureset to give us more CI signal and testing. Ultimately the featuregate needs to graduate to GA (default on) once tests pass so that the feature can GA. See also: - - -"
      }
    }
  },
  "OpenShift Top Level Product Strategy": {
    "description": "Holds all top-level Market Problems, Features, and some Epics that were committed during release product planning.",
    "features": {
      "OCPPLAN-7878": {
        "summary": "NetEdge - Maintainability and Debugability & Tech Backlog",
        "description": "tldr: three basic claims, the rest is explanation and one example We cannot improve long term maintainability solely by fixing bugs. Teams should be asked to produce designs for improving maintainability/debugability. Specific maintenance items (or investigation of maintenance items), should be placed into planning as peer to PM requests and explicitly prioritized against them. While bugs are an important metric, fixing bugs is different than investing in maintainability and debugability. Investing in fixing bugs will help alleviate immediate problems, but doesn't improve the ability to address future problems. You (may) get a code base with fewer bugs, but when you add a new feature, it will still be hard to debug problems and interactions. This pushes a code base towards stagnation where it gets harder and harder to add features. One alternative is to ask teams to produce ideas for how they would improve future maintainability and debugability instead of focusing on immediate bugs. This would produce designs that make problem determination, bug resolution, and future feature additions faster over time. I have a concrete example of one such outcome of focusing on bugs vs quality. We have resolved many bugs about communication failures with ingress by finding problems with point-to-point network communication. We have fixed the individual bugs, but have not improved the code for future debugging. In so doing, we chase many hard to diagnose problem across the stack. The alternative is to create a point-to-point network connectivity capability. this would immediately improve bug resolution and stability (detection) for kuryr, ovs, legacy sdn, network-edge, kube-apiserver, openshift-apiserver, authentication, and console. Bug fixing does not produce the same impact. We need more investment in our future selves. Saying, \"teams should reserve this\" doesn't seem to be universally effective. Perhaps an approach that directly asks for designs and impacts and then follows up by placing the items directly in planning and prioritizing against PM feature requests would give teams the confidence to invest in these areas and give broad exposure to systemic problems. ---- Relevant links: Documentation: Edge Diagnostics Scratchpad the SDN team's diagnostic guide. Linux Performance OpenShift Router Reload Technical Overview on Access. How to collect worker metrics to troubleshoot CPU load, memory pressure and interrupt issues and networking on worker nodes in OCP 4 on Mojo, results from OpenShift scalability testing. Scalability and performance OCP 3.11 documentation about the manual performance configuration that was possible in OCP 3. Timing web requests with cURL and Chrome some useful tcpdump commands. OpenShift SDN - Networking design document for improved status condition reporting. Observability tips for HAProxy analysis using tshark. The PCP Book: A Complete Documentation of Performance Co-Pilot brief guide to using SystemTap on RHCOS. Troubleshooting throughput issues Red Hat Enterprise Linux Network Performance Tuning Guide (PDF) a diagnostic built into the kube-apiserver operator. Diagnostic tools: dropwatch to check NIC configuration. iovisor/bcc: BCC - Tools for BPF-based Linux IO analysis, networking, monitoring, and more to gather timing information about HTTP/HTTPS connections. route-monitor a programmable packet generator. OpenTracing node-problem-detector by Brendan Gregg. DTrace SystemTap cheatsheet (PDF) kubectl plugin for tcpdump & Wireshark. ironcladlou/ditm network diagnostic and visualization tool. ali a general stress-loading tool (CPU, filesystem, network, ...). mb is an example of diagnosing DNS latency/timeouts. BZ1829779 Investigation is an example of diagnosing misconfigured DNS for an external LB. Debugging network stalls on Kubernetes| from the GitHub Blog, about diagnosing Kubernetes performance issues related to ksoftirqd."
      }
    }
  },
  "OpenShift Monitoring": {
    "stories": {
      "MON-4207": {
        "summary": "Bump prometheus-operator to v0.81.0 downstream",
        "description": "Bump prometheus-operator to v0.81.0 downstream"
      },
      "MON-4126": {
        "summary": "Add fallbackScrapeProtocol to ScrapeClass",
        "description": "Add fallbackScrapeProtocol to ScrapeClass Prometheus-Operator And have it set downstream. (no need to wait for a prom-operator release, we can cherry-pick it one merged, that would be considered an extra test for the change)",
        "epic_key": "MON-4103"
      },
      "MON-3866": {
        "summary": "Create separate metrics client cert for metrics server",
        "description": "For the issue we had identified that we need to have separate metrics client cert for metrics server but for that we need to add approver for metrics-server |",
        "epic_key": "MON-4098"
      }
    },
    "epics": {
      "MON-4103": {
        "summary": "Prometheus 3 integration",
        "description": "Ref:"
      },
      "MON-4043": {
        "summary": "Configuring external Alertmangers with proxy_url",
        "description": "Proposed title of this feature request Configuring external Alertmangers with proxy_url What is the nature and description of the request? Currently , there is no way to set the proxy_url when adding external alertmanagers instances. Why does the customer need this? (List the business requirements) Customer would like to add external alermanger instance on a disconnected cluster, so proxying the prometheus-alertmanager connection is needed. List any affected packages or components. Prometheus"
      },
      "MON-4098": {
        "summary": "Metrics Server Post GA 2",
        "description": "This epic is to track stories that are not completed in MON-3865"
      }
    }
  },
  "Machine Config Operator": {
    "stories": {
      "MCO-1648": {
        "summary": "Add MCN to must-gather",
        "description": "With the GA of MCN, we will need to have the objects in must-gathers for debugging. Done when: MachineConfigNode objects are included in must-gathers|",
        "epic_key": "MCO-836"
      },
      "MCO-1645": {
        "summary": "Remove `replace` line in `go.mod` of MCO repo",
        "description": "To simultaneously merge the V1 MCN API and updates to the MCN origin tests Once the simultaneous merges are complete and the API is properly bumped in the client-go repo (MCO-1644), the `replace` statements should be removed and the API and client-go versions should point to the merged commits in the openshift/machine-config-operator repo. Done when: PR pointing to the correct API and client-go commits is created and merged in the `openshift/machine-config-operator` repo Cleanup mentioned in this| comment is completed",
        "epic_key": "MCO-836"
      },
      "MCO-1635": {
        "summary": "Add runbook for HighOverallControlPlaneMemory alert",
        "description": "MCO will send an alert when a node for 1 hour, when all control plane node have extremely high memory usage The alerts describes the following summary: - Memory utilization across all control plane nodes is high, and could impact responsiveness and stability. description: - Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd may be slow to respond. To fix this, increase memory of the control plane nodes. It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for HighOverallControlPlaneMemory alert Created runbook link is accessible to cluster admin with HighOverallControlPlaneMemory alert",
        "epic_key": "MCO-111"
      },
      "MCO-1615": {
        "summary": "Implement node degraded functionality in MCN conditions",
        "description": "Currently, the MachineConfigNode object does not clearly reflect when a node is in a degraded state. Rather, it continues showing the MCN conditions that were last updated during the failed node upgrade. An example of a node failing an upgrade in the \"AppliedFilesAndOS\" phase can be seen here: {code:java} Name: ip-10-0-1-244.ec2.internal Namespace: Labels: none Annotations: none API Version: machineconfiguration.openshift.io/v1alpha1 Kind: MachineConfigNode Metadata: Creation Timestamp: 2025-03-24T12:40:07Z Generation: 3 Owner References: API Version: v1 Kind: Node Name: ip-10-0-1-244.ec2.internal UID: 7137be14-1e41-40a7-91ff-50da0c5693f6 Resource Version: 91058 UID: 9445d162-b0ea-407a-9a82-bba4db7d78db Spec: Config Version: Desired: rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Node: Name: ip-10-0-1-244.ec2.internal Pool: Name: worker Status: Conditions: Last Transition Time: 2025-03-24T12:40:11Z Message: All pinned image sets complete Reason: AsExpected Status: False Type: PinnedImageSetsProgressing Last Transition Time: 2025-03-24T15:58:47Z Message: Update is Compatible. Reason: UpdateCompatible Status: True Type: UpdatePrepared Last Transition Time: 2025-03-24T15:59:47Z Message: Updating the Files and OS on disk as a part of the in progress phase Reason: AppliedFilesAndOS Status: Unknown Type: UpdateExecuted Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the UpdatePostActionComplete phase Reason: NotYetOccurred Status: False Type: UpdatePostActionComplete Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: Uncordoned Node as part of completing upgrade phase Reason: Uncordoned Status: False Type: UpdateComplete Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: In desired config . Resumed normal operations. Reason: Resumed Status: False Type: Resumed Last Transition Time: 2025-03-24T15:58:47Z Message: Update Compatible. Post Cfg Actions : Drain Required: true Reason: UpdatePreparedUpdateCompatible Status: True Type: UpdateCompatible Last Transition Time: 2025-03-24T15:59:45Z Message: Drained node. The drain is complete as the desired drainer matches current drainer: drain-rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Reason: UpdateExecutedDrained Status: True Type: Drained Last Transition Time: 2025-03-24T15:59:47Z Message: Applying files and new OS config to node. OS will not need an update. SSH Keys will not need an update Reason: UpdateExecutedAppliedFilesAndOS Status: Unknown Type: AppliedFilesAndOS Last Transition Time: 2025-03-24T15:58:52Z Message: Cordoned node. The node is reporting Unschedulable = true Reason: UpdateExecutedCordoned Status: True Type: Cordoned Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the RebootedNode phase Reason: NotYetOccurred Status: False Type: RebootedNode Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the ReloadedCRIO phase Reason: NotYetOccurred Status: False Type: ReloadedCRIO Last Transition Time: 2025-03-24T15:58:46Z Message: Node ip-10-0-1-244.ec2.internal needs an update Reason: Updated Status: False Type: Updated Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: UnCordoned node. The node is reporting Unschedulable = false Reason: UpdateCompleteUncordoned Status: False Type: Uncordoned Last Transition Time: 2025-03-24T12:40:11Z Message: All is good Reason: AsExpected Status: False Type: PinnedImageSetsDegraded Config Version: Current: rendered-worker-cb3673914e9994a198f0a92079c46ffc Desired: rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Observed Generation: 4 Events: none {code} As can be seen, this does not clearly show that something went wrong with the update. Instead it looks like the upgrade is still proceeding. This impacts our ability to use MCN to power other functionality and will likely lead to customer confusion. A `MachineConfigNodeNodeDegraded` status condition was added as part of the MCN API updates in MCO-1543. This story involves implementing the functionality to populate this condition on a node degrade. Some bugs, including OCPBUGS-44290 and OCPBUGS-52828, have been opened due to issues resulting from MCN not reporting node degradation clearly. Further, MCN clearly reporting on node degraded is needed to power the functionality for MCO-1228, which is part of the status reporting GA. Done when: MCN clearly reports node degrade statuses using the `MachineConfigNodeNodeDegraded` condition",
        "epic_key": "MCO-836"
      },
      "MCO-1594": {
        "summary": "Update tests in origin to use explicit opt-out option",
        "description": "The origin tests should be: updated to use the new API from disable the ownerref test as we no longer plan to degrade in that manner in the default-on behavior This should land before MCO-1584| lands.",
        "epic_key": "MCO-1361"
      },
      "MCO-1584": {
        "summary": "Implement boot image updates by default on GCP and AWS",
        "description": "This will have to be implemented after lands. The default opt-in should only take place if no boot image configuration currently exists on the cluster.",
        "epic_key": "MCO-1361"
      },
      "MCO-1579": {
        "summary": "Bump ignition to v2.20.0",
        "description": "This story covers all the needed work from the code side that needs to be done to support the 3.5 ignition spec. To support 3.5 we need to, from a high level perspective: Bump the ignition dependency to v2.20.0, that contains the 3.5 types. Switch all imports that points to 3.4 to point to github.com/coreos/ignition/v2/config/v3_4/types. Create the conversion logic and update the existing ones: convertIgnition34to22 changes to convertIgnition35to22 convertIgnition22to34 changes to convertIgnition22to35 create convertIgnition35to34 Update UTs to reflect above changes and to cover the new function. Done When: The code points to the new ignition release The code uses ignition 3.5 as the default version The UT tests are updated to match the changes for the already existing code plus the new conversion functions",
        "epic_key": "MCO-1234"
      },
      "MCO-1544": {
        "summary": "API 2/6 Adapt MCO code to use MCN\u2019s updated v1alpha1 API",
        "description": "The second step in GAing the MCN API is pulling the updated v1alpha1 API worked on in MCO-1543 into the MCO code. This will allow for testing the final API design in the MCO before the API is graduated to V1. Done when: V1alpha1 API for MCN is pulled into the MCO (openshift/machine-config-operator) Code references and MCN functionality are updated according to API changes Tests are updated and passing Requires: MCO-1543 (Create finalized v1alpha1 MCN API)",
        "epic_key": "MCO-836"
      },
      "MCO-1543": {
        "summary": "API 1/6 Create finalized v1alpha1 MCN API",
        "description": "The first step in GAing the MCN API is finalizing the v1alpha1 API. This will allow for testing of the final API design before the API is graduated to V1. Since there are a fair amount of changes likely to be made for the MCN API, making our changes in v1alpha1 first seems to follow the API team\u2019s preference| of V1 API graduations only having minor changes. Done when: V1alpha1 API for MCN is finalized The MCN API fields are all properly documented All design decisions are appropriately documented",
        "epic_key": "MCO-836"
      },
      "MCO-1522": {
        "summary": "API 2/4 Adapt MCO code to use PIS\u2019s GA API",
        "description": "The second step in GAing PinnedImageSets is adapting the MCO code (in openshift/machine-config-operator) to use the V1 (GA) API created in MCO-1521. Done when: MCO code is pointing to the GA PIS API Code references and functionality are updated according to API changes Requires: MCO-1521 (Create GA PIS API)",
        "epic_key": "MCO-1258"
      },
      "MCO-1521": {
        "summary": "API 1/4 Create GA PIS API",
        "description": "The first step in GAing PinnedImageSets is creating a V1 (GA) API for the feature. A related API PR to use as a reference for the expected work can be found here| Done when: The PIS API properly documents all fields included in the GA API Any design decisions/field changes are appropriately documented/tracked The PIS V1 API PR is reviewed and merged into master of openshift/api",
        "epic_key": "MCO-1258"
      },
      "MCO-1519": {
        "summary": "API 4/6 Adapt MCO & origin code to use MCN\u2019s V1 API",
        "description": "The fourth step in GAing the MCN API is pulling the V1 API created in MCO-1518 into the MCO & origin code. Related PRs to use as references for the expected work can be found here for the MCO repo Done when: MCO code is pointing to the V1 MCN API Origin code is pointing to the V1 MCN API Code references and functionality are updated according to the API needs Requires: MCO-1518 (Create GA MCN API) This involves a simultaneous merge of the MCO, origin, and API",
        "epic_key": "MCO-836"
      },
      "MCO-1509": {
        "summary": "Inject /etc/containers into OCL build pod from rendered MachineConfig",
        "description": "To make the OCL build process more consistent as well as enabling it to work in a disconnected environment, we should inject the contents of the /etc/containers directory into the builder pod. These files arre managed by the container-runtime-config controller.path' /etc/containers/registries.conf /etc/crio/crio.conf.d/00-default /etc/machine-config-daemon/policy-for-old-podman.json /etc/containers/policy.json $ oc get mc/99-master-generated-registries -o yaml grep \"/etc/containers\" /etc/containers/storage.conf /etc/containers/registries.conf /etc/containers/policy.json /etc/containers/registries.d/sigstore-registries.yaml{noformat} As an implementation detail, the most straightforward way to do this would be to have the BuildRequest object may need to copy those files into place or adjust an SELinux context before starting the build, since that was needed for the /etc/pki/entitlements functionality to work. Although that might also not be necessary since these files were passed through to the build context whereas /etc/containers is more for configuring Buildah itself. Unknowns at the time of this writing: Should the ControllerRuntimeConfig| be considered as part of this? Is /etc/containers/storage.conf appropriate to inject into the builder pod? Done When: The contents of /etc/containers are mounted into the builder pod. The unknowns are resolved and additional files, if any, are injected into the builder pod.",
        "epic_key": "MCO-1507"
      },
      "MCO-1501": {
        "summary": "Support Custom MCPs in MCN",
        "description": "The machine config node functionality should fully support custom machine config pools. Investigation from MCO-1299 highlights that custom MCPs are not supported. This story will be done when the following is complete: Custom pools should be reflected in the \"POOLNAME\" column when a user runs {code:java} $ oc get machineconfignode {code} Users should be able to check the status of a specific, custom MCP using {code:java} $ oc get machineconfignodes $(oc get machineconfignodes -o json select(.spec.pool.name==\"pool_name\")|.metadata.name') {code} MCNs (as seen when running oc describe machineconfignode/node-name) should reflect the custom pools a node is a part of under Spec.Pool.Name",
        "epic_key": "MCO-836"
      },
      "MCO-1492": {
        "summary": "Add Runbook for SystemMemoryExceedsReservation",
        "description": "MCO will send an alert when a node for 15 minutes, a specific node is using more memory than is reserved. The alerts describes the following \"summary: \"Alerts the user when, for 15 minutes, a specific node is using more memory than is reserved\" description: \"System memory usage of \\ $value | humanize on \\ $labels.node exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased ( when running nodes with high numbers of pods (either due to rate of change or at steady state).\"\" It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for SystemMemoryExceedsReservation alert Created runbook link is accessible to cluster admin with SystemMemoryExceedsReservation alert",
        "epic_key": "MCO-111"
      },
      "MCO-1485": {
        "summary": "Boot Image Controller should attempt to upgrade stub ignition to spec 3",
        "description": "The boot image controller should should ensure `-user-data` stub secrets are at least spec 3. This requires the cert management work to land first. To ensure maximum coverage and minimum risk, we will only attempt to upgrade stub secrets that are currently spec 2. While we could potentially upgrade all stubs to the newest(which at the moment is 3.4.0) supported by the MCO, this may cause issues like for some boot images that only support early spec 3 ignition(certain older boot images can only do 3.0.0 and 3.1.0 ignition). Newer boot images can support all spec 3 stubs, so to preserve scaling ability as much as we can, we'll leave spec 3 stubs as is for the moment.",
        "epic_key": "MCO-1361"
      },
      "MCO-1482": {
        "summary": "machine-config ClusterOperator stays Upgradeable=True as new nodes are added",
        "description": "Implementing RFE-3017. As a bare Story, without a Feature or Epic, because I'm trying to limit the amount of MCO-side paperwork required to get my own RFE itch scratched. As a Story and not a NO-ISSUE pull, because OCP QE had a bit of trouble handling mco4637| when I went NO-ISSUE on that one, and I think this might be worth a 4.19 release note."
      },
      "MCO-1457": {
        "summary": "Clean up bootstrap MCS CA & TLS cert objects for management",
        "description": "The CA/cert generated by the installer is not currently managed and also does not preserve the signing key; so the cert controller we are adding in the MCO(leveraged from library-go), throws away everything and starts fresh. Normally this happens fairly quickly so both the MCS and the -user-data secrets are updated together. However, in certain cases(such as agent based installations) where a bootstrap node joins the cluster late, it will have the old CA from installer, and unfortunately the MCS will have a TLS cert signed by the new CA - resulting in invalid TLS cert errors. To account for such cases, we have to ensure the first CA embedded in any machine is matching the format expected by the cert controller. To do this, we'll have to do the following in the installer: -Have the bootstrap MCC generate the CA/TLS cert using the cert controller, and populate them into the right places(this card)- -Make changes in the installer to remove the creation of the CA/cert, since the bootstrap MCC will now handle this( Template out all root-ca artifacts in the format expected by the library-go cert controller. This would involve adding certain annotations on the artifacts(with respect to validity of the cert and some other ownership metadata) The root CA signing key is currently discarded by the installer, so this will have to be a new template in the installer.",
        "epic_key": "MCO-1208"
      },
      "MCO-1451": {
        "summary": "Merge \"Zack's Scripts\", clean up hack scripts",
        "epic_key": "MCO-1298"
      },
      "MCO-1449": {
        "summary": "Add Runbook for MCDPivotError",
        "description": "MCD will send an alert when a node failes to pivot to another MachineConfig. This could prevent an OS upgrade from succeeding. The alert contains the information on what logs to look for The alerts describes the following \"Error detected in pivot logs on \\ $labels.node , upgrade may be blocked. For more details: oc logs -f -n \\ $labels.namespace \\ $labels.pod -c machine-config-daemon \" It is possible that admin may not be able to interpret exact action to be taken after looking at MCD pod logs. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for MCDPivotError alert Created runbook link is accessible to cluster admin with MCDPivotError alert",
        "epic_key": "MCO-111"
      },
      "MCO-1443": {
        "summary": "Graduate MOSB/MOSC API to v1",
        "description": "To make OCL ready for GA, the first step would be graduating the MCO's APIs from v1alpha1 to v1. This requires changes in the openshift/api repo.",
        "epic_key": "MCO-1316"
      },
      "MCO-1437": {
        "summary": "Inherit from global pull secret if baseImagePullSecret field is not specified",
        "description": "As a follow up to the one field we identified that is best updated pre-GA is to make the baseImagePullSecret optional. The builder pod should have the logic to fetch from baseImagePullSecret if the user does not specify this via a MachineOSConfig object.",
        "epic_key": "MCO-1316"
      },
      "MCO-1416": {
        "summary": "Separate OCL e2e tests into new test suite",
        "description": "The original scope of this task is represented across this story & MCO-1494. With OCL GA'ing soon, we'll need a blocking path within our e2e test suite that must pass before a PR can be merged. This story represents the first stage in creating the blocking path: Migrate the tests from e2e-gcp-op-techpreview into a new test suite called e2e-ocl. This can be done by moving the tests in the MCO repo from the test/e2e-techpreview folder to a new test/e2e-ocl folder. There might be some minor cleanups such as fixing duplicate function names, etc. but it should be fairly straightforward to do. Make a new e2e-gcp-op-ocl job to call the newly created e2e-ocl test suite. This test should first be added as optional for 4.19 so it can be stability tested before it is made blocking for 4.18 & 4.19. This will require a PR to the openshift/release repo to call the new test for 4.19 & master. This should be a pretty straightforward config change.",
        "epic_key": "MCO-828"
      },
      "MCO-1165": {
        "summary": "Regression BuildController should have a rebuild function",
        "description": "REGRESSION We need to reinvent the wheel for triggering rebuild functionality and the rebuild mechanism as pool labeling and annotation is no longer a favorable way to interact with layered pools There are a few situations in which a cluster admin might want to trigger a rebuild of their OS image in addition to situations where cluster state may dictate that we should perform a rebuild. For example, if the custom Dockerfile changes or the machine-config-osimageurl changes, it would be desirable to perform a rebuild in that case. To that end, this particular story covers adding the foundation for a rebuild mechanism in the form of an annotation that can be applied to the target MachineConfigPool. What is out of scope for this story is applying this annotation in response to a change in cluster state (e.g., custom Dockerfile change). Done When: BuildController is aware of and recognizes a special annotation on layered MachineConfigPools (e.g., {{{}machineconfiguration.openshift.io/rebuildImage{}}}). Upon recognizing that a MachineConfigPool has this annotation, BuildController will clear any failed build attempts, delete any failed builds and their related ephemeral objects (e.g., rendered Dockerfile / MachineConfig ConfigMaps), and schedule a new build to be performed. This annotation should be removed when the build process completes, regardless of outcome. In other words, should the build success or fail, the annotation should be removed. optional BuildController keeps track of the number of retries for a given MachineConfigPool. This can occur via another annotation, e.g., machineconfiguration.openshift.io/buildRetries=1 . For now, this can be a hard-coded value (e.g., 5), but in the future, this could be wired up to an end-user facing knob. This annotation should be cleared upon a successful rebuild. If the rebuild is reached, then we should degrade.",
        "epic_key": "MCO-828"
      },
      "MCO-81": {
        "summary": "MCD: emit earlier events to warn about failing drains",
        "description": "In newer versions of OCP, we have changed our draining mechanism to only fail after 1 hour. This also means that the event which captures the failing drain was also moved to the failure at the 1hr mark. Today, upgrade tests oft fail with timeouts related to drain errors (PDB or other). There exists no good way to distinguish what pods are failing and for what reason, so we cannot easily aggregate this data in CI to tackle issues related to PDBs to improve upgrade and CI pass rate. If the MCD, upon a drain run failure, emits the failing pod and reason (PDB, timeout) as an event, it would be easier to write a test to aggregate this data. Context in this thread:",
        "epic_key": "MCO-1298"
      },
      "MCO-1646": {
        "summary": "Remove `replace` line in `go.mod` of origin repo",
        "description": "To simultaneously merge the V1 MCN API and updates to the MCN origin tests Once the simultaneous merges are complete and the API is properly bumped in the client-go repo (MCO-1644), the `replace` statements should be removed and the API and client-go versions should point to the merged commits in the openshift/origin repo. Done when: origin PR pointing to the correct API and client-go commits is created and merged",
        "epic_key": "MCO-836"
      },
      "MCO-1590": {
        "summary": "Add explicit opt-out and Status field for ManagedBootImages API",
        "description": "Based on discussion on the enhancement, we have decided that we'd like to add an explicit opt-out option and a status field for the ManagedBootImages knob in the MachineConfiguration object. More context here:",
        "epic_key": "MCO-1361"
      },
      "MCO-1587": {
        "summary": "Add runbook for ExtremelyHighIndividualControlPlaneMemory alert",
        "description": "MCO will send an alert when a node for 45 minutes, when a control plane node has extremely high memory usage The alerts describes the following \"summary: - Extreme memory utilization per node within control plane nodes is extremely high, and could impact responsiveness and stability. description: - The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests especially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.\" It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for ExtremelyHighIndividualControlPlaneMemory alert Created runbook link is accessible to cluster admin with ExtremelyHighIndividualControlPlaneMemory alert",
        "epic_key": "MCO-111"
      },
      "MCO-1558": {
        "summary": "MOSB & Image Pruning",
        "description": "When user deletes a MOSB, delete the built image associated with it from the image registry.",
        "epic_key": "MCO-1552"
      },
      "MCO-1537": {
        "summary": "Add runbook for MCDRebootError alert",
        "description": "MCC sends drain alert when a node fails to reboot in a span of 5 minutes This is to make sure that admin takes appropriate action if required by looking at the pod logs. Alert contains the information on where to look for the logs. Example alert looks like: Reboot failed on \\ $labels.node , update may be blocked. For more details: oc logs -f -n \\ $labels.namespace \\ $labels.pod -c machine-config-daemon It is possible that admin may not be able to interpret exact action to be taken after looking at MCC pod logs. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for MCDRebootError alert Created runbook link is accessible to cluster admin with MCDRebootError alert",
        "epic_key": "MCO-111"
      },
      "MCO-1523": {
        "summary": "API 3/4 Create 5 tests in openshift/origin for GA readiness signal",
        "description": "The third step in GAing PinnedImageSets is creating at least five tests in openshift/origin. These tests will act as a GA readiness signal and build confidence in the feature\u2019s functionality. More information on the requirements for the tests can be found here| Done when: At least five tests are defined for PIS The tests meet the criteria specified in the API requirements Requires: MCO-1521 (Create GA PIS API) MCO-1522 (Adapt MCO code to use PIS\u2019s GA API)",
        "epic_key": "MCO-1258"
      },
      "MCO-1520": {
        "summary": "API 5/6 Create 5 tests in openshift/origin for GA readiness signal",
        "description": "The fifth step in GAing the MCN API is creating at least five tests in openshift/origin. These tests will act as a GA readiness signal and build confidence in the feature's functionality. More information on the requirements for the tests can be found here (Custom MCP support in MCN) Can be worked on in parallel to MCO-1519 (Adapt MCO code to use MCN\u2019s GA API)",
        "epic_key": "MCO-836"
      },
      "MCO-1518": {
        "summary": "API 3/6 Create V1 MCN API",
        "description": "The third step in GAing the MCN API is creating a V1 API for the feature. A related API PR to use as a reference for the expected work can be found here| Done when: V1 API for MCN is finalized The MCN API fields are all properly documented All design decisions are appropriately documented Requires: MCO-1543 (Create finalized v1alpha1 MCN API)",
        "epic_key": "MCO-836"
      },
      "MCO-1515": {
        "summary": "Pick up openshift/kubernetes 1.32 rebase updates",
        "description": "User or Developer story As a MCO developer, I want to pick up the openshift/kubernetes updates for the 1.32 k8s rebase to track the k8s version as rest of the OpenShift 1.32 cluster. Engineering Details Update the go.mod, go.sum and vendor dependencies pointing to the kube 1.32 libraries. This includes all direct kubernetes related libraries as well as openshift/api , openshift/client-go, openshift/library-go and openshift/runtime-utils Acceptance Criteria: All k8s.io related dependencies should be upgraded to 1.32. openshift/api , openshift/client-go, openshift/library-go and openshift/runtime-utils should be upgraded to latest commit from master branch All ci tests must be passing",
        "epic_key": "MCO-1488"
      },
      "MCO-1470": {
        "summary": "Update helper binaries with latest changes",
        "description": "In we are migrating my helper binaries into the MCO repository. I had to make changes to several of my helpers in the original repository are incorporated into the MCO repository versions of the relevant helper binaries.",
        "epic_key": "MCO-531"
      },
      "MCO-643": {
        "summary": "Implement a path in the controller to manage user-data secrets",
        "description": "The machinesets in the machine-api namespace reference a user-data secret (per pool and can be customized) which stores the initial ignition stub configuration pointing to the MCS, and the TLS cert. This today doesn't get updated after creation. The MCO now has the ability to manage some fields of the machineset object as part of the managed bootimage work. We should extend that to also sync in the updated user-data secrets for the ignition tls cert. The MCC should be able to parse both install-time-generated machinesets as well as user-created ones, so as to not break compatibility. One way users are using this today is to use a custom secret + machineset to do non-MCO compatible ignition fields, for example, to partition disks for different device types for nodes in the same pool. Extra care should be taken not to break this use case",
        "epic_key": "MCO-1208"
      },
      "MCO-466": {
        "summary": "Improve erroring where there is a bootstrap/in-cluster master config mismatch",
        "description": "Today the MCO bootstraps with a bootstrap MCC/MCS to generate and serve master configs. When the in-cluster MCC comes up, it then tries to regen the same MachineConfig via the in-cluster MCs at the time. This often causes a drift and for the install to fail. See and for more context. For the most recent occurrence of this, see: Early on this helped us see differences between bootstrap and in-cluster behaviour more easily, but we do have the bootstrap machineconfig on-disk on the masters. In theory, we should just be able to use that directly and attempt to consolidate the changes. In the case of a drift, instead of failing, we can consider doing an immediate update instead to the latest version.",
        "epic_key": "MCO-1298"
      },
      "MCO-113": {
        "summary": "Preserve previous boot MCD logs",
        "description": "In OCP 4.7 (?) and before, you were able to see the MCD logs of the previous container post upgrade. Now it seems that we no longer do in newer versions. I am not sure if this is a change in kube pod logging behaviour, how the pod gets shutdown and brought up, or something in the MCO. This however makes it relatively hard to debug in newer versions of the MCO, and in numerous bugs we could not pinpoint the source of the issue since we no longer have necessary logs. We should find a way to properly save the previous boot MCD logs if possible.",
        "epic_key": "MCO-1298"
      }
    },
    "epics": {
      "MCO-836": {
        "summary": "GA Machine Config Node",
        "description": "This epic describes the work required to GA a minimal viable version of the Machine Config Node feature to enable the subsequent GAing of the Pinned Image Sets feature. The GAing of status reporting as well as any further enhancements for the Machine Config Node feature will be tracked in MCO-1506. Related Items: Original MCN/State Reporting Enhancement Pinned Image Sets Enhancement 4.19 backport SBAR| Doc work tracked in OSDOCS-14404 Done when: MCN API is GAed MCN functionality is consistent across all MCPs (default & custom) and both clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing The team is confident that the state of MCN is robust enough to support the GAing of Pinned Image Sets"
      },
      "MCO-111": {
        "summary": "Actionable Error Messaging",
        "description": "The error propagation is generally speaking not 1-to-1. The operator status will generally capture the pool status, but the full error from Controller/Daemon does not fully bubble up to pool/operator, and the journal logs with error generally don\u2019t get bubbled up at all. This is very confusing for customers/admins working with the MCO without full understanding of the MCO\u2019s internal mechanics: The real error is hard to find The error message is often generic and ambiguous The solution/workaround is not clear at all Using \"unexpected on-disk state\" as an example, this can be caused by any amount of the following: An incomplete update happened, and something rebooted the node The node upgrade was successful until rpm-ostree, which failed and atomically rolled back The user modified something manually Another operator modified something manually Some other service/network manager overwrote something MCO writes Etc. etc. Since error use cases are wide and varied, there are many improvements we can perform for each individual error state. This epic aims to propose targeted improvements to error messaging and propagation specifically. The goals being: De-ambigufying different error cases with the same message Adding more error catching, including journal logs and rpm-ostree errors Propagating full error messages further up the stack, up to the operator status in a clear manner Adding actionable fix/information messages alongside the error message With a side objective of observability, including reporting all the way to the operator status items such as: Reporting the status of all pools Pointing out current status of update/upgrade per pool What the update/upgrade is blocking on How to unblock the upgrade Approaches can include: Better error messaging starting with common error cases De-ambigufying config mismatch Capturing rpm-ostree logs from previous boot, in case of osimageurl mismatch errors Capturing full daemon error message back to pool/operator status Adding a new field to the MCO operator spec, that attempts to suggest fixes or where to look next, when an error occurs Adding better alerting messages for MCO errors Options"
      },
      "MCO-1361": {
        "summary": "Opt-out updated bootimage for GCP and AWS",
        "description": "This epic will encompass work required to switch boot image updates on GCP to be opt-out."
      },
      "MCO-1234": {
        "summary": "Bump ignition to spec 3.5",
        "description": "Once ignition spec 3.5 stablizes, we should switch to using spec 3.5 as the default in the MCO to enable additional features in RHCOS. (example: needs 3.5)"
      },
      "MCO-1258": {
        "summary": "GA Pin and pre-load images",
        "description": "This epic describes the work required to GA the Pinned Image Sets feature. Related Documentation: Pinned Image Sets Enhancement Design Review Document Comment with helpful info on how to use PIS| Done when: Pinned Image Set API is GAed (here is the API in tech preview: ) Pinned Image Set functionality is consistent for clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing e2e testing: create PIS for custom pool and run garbage collection and check if PIS remain on node add PIS to custom pool and check if they have been successfully added add PIS to standard pool and check if they have been successfully added add invalid PIS and check if MCN has degraded in standard pool add invalid PIS and check if MCN has degraded in custom pool"
      },
      "MCO-1507": {
        "summary": "On Cluster Layering Disconnected Support",
        "description": "Post GA On Cluster Build Enhancement work"
      },
      "MCO-1208": {
        "summary": "Manage the MCS ignition-ca cert",
        "description": "Spun out of This aims to capture the work required to rotate the MCS-ignition CA + cert. Original description copied from MCO-668: Today in OCP there is a TLS certificate generated by the installer | which is called \"root-ca\" but is really \"the MCS CA\". A key derived from this is injected into the pointer Ignition configuration under the \"security.tls.certificateAuthorities\" section, and this is how the client verifies it's talking to the expected server. If this key expires (and by default the CA has a 10 year lifetime), newly scaled up nodes will fail in Ignition (and fail to join the cluster). The MCO should take over management of this cert, and the corresponding user-data secret field, to implement rotation. Reading: - There is a section in the customer facing documentation that touches on this: - There's a section in the customer facing documentation for this: that needs updating for clarification. - There's a pending PR to openshift/api: - Also see old (related) bug: - This is also separate to which describes the management of kubelet certs"
      },
      "MCO-1298": {
        "summary": "Tech debt 4.18",
        "description": "These are items that the team has prioritized to address in 4.18."
      },
      "MCO-1316": {
        "summary": "On-Cluster Layering - upgrades and integrations",
        "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
      },
      "MCO-828": {
        "summary": "On-Cluster Layering GA",
        "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
      },
      "MCO-1552": {
        "summary": "On Cluster Layering: Address Image Pruning",
        "description": "Done When: We implement a solution that mitigates the node disruptions when applying Machine Configs. Notes: MVP: when a MachineOSBuild object is deleted, the corresponding image in the registry should also be deleted, whether internal or external Mid term: have good documentation around user-driven prunes, and what can/cannot be pruned Mid term: have an opt-in/opt-out mechanism for auto pruning, where builds not in use and x versions old are deleted automatically Nice to have: tag latest (and maybe latest-1) builds for pools, so the user can easily refer to the latest build and know what not to prune Nice to have: have user-pruned images automatically reflect back on the MachineOSBuild Long term: user defined prune logic"
      },
      "MCO-1488": {
        "summary": "Update MCO dependencies to Kubernetes 1.32",
        "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that MCO uses to v1.29 which will keep it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? Uncover any possible issues with the openshift/kubernetes rebase before it merges. MCO continues using the latest kubernetes/OpenShift libraries and the kubelet, kube-proxy components. MCO e2e CI jobs pass on each of the supported platform with the updated components. Acceptance Criteria All stories in this epic must be completed. Go version is upgraded for MCO components. CI is running successfully with the upgraded components against the 4.18/master branch. Dependencies (internal and external) ART team creating the go 1.31 image for upgrade to go 1.31. OpenShift/kubernetes repository downstream rebase PR merge. Open questions: Do we need a checklist for future upgrades as an outcome of this epic?- yes, updated below. Done Checklist Step 1 - Upgrade go version to match rest of the OpenShift and Kubernetes upgraded components. Step 2 - Upgrade Kubernetes client and controller-runtime dependencies (can be done in parallel with step 3) Step 3 - Upgrade OpenShift client and API dependencies Step 4 - Update kubelet and kube-proxy submodules in MCO repository Step 5 - CI is running successfully with the upgraded components and libraries against the master branch."
      },
      "MCO-531": {
        "summary": "General \"tech debt\" items that don't have a home yet",
        "description": "Background This is intended to be a place to capture general \"tech debt\" items so they don't get lost. I very much doubt that this will ever get completed as a feature, but that's okay, the desire is more that stores get pulled out of here and put with feature work \"opportunistically\" when it makes sense. Goal If you find a \"tech debt\" item, and it doesn't have an obvious home with something else (e.g. with MCO-1 if it's metrics and alerting) then put it here, and we can start splitting these out/marrying them up with other epics when it makes sense."
      }
    }
  },
  "OpenShift Image Registry": {
    "stories": {
      "IR-522": {
        "summary": "Allow registry to run in new regions without code changes",
        "description": "We want to allow the image registry to run in new AWS regions without requiring a manual intervention in the code every time a new region pops up. As we can see here| every time a new region is added we need to manually add it to the list of known regions as well. This is required because the upstream project uses an AWS client that isn't receiving these new regions automatically.",
        "epic_key": "IR-513"
      }
    },
    "epics": {
      "IR-513": {
        "summary": "Support for new AWS regions without code changes",
        "description": "Epic Goal Streamline onboarding of new AWS regions in OCP managed services like ROSA or OSD by removing the need for code changes in the cluster image registry operator Why is this important? Every time AWS is launching a new region the deployment of OpenShift will fail with the Cluster Image Registry Operator failing to reconcile until the newly added region is hard coded into the operator's controller This blocks installer and QE teams from development, testing and rollout of new region support for OCP managed services on AWS Disabling the integrated registry is currently not supported by many of our managed services Acceptance Criteria A new AWS region should not require any code changes to CIRO for the operator to reconcile the request for required cloud infrastructure to run the image registry Backports to all currently supported OCP versions Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Hosted Control Plane": {
    "stories": {
      "HOSTEDCP-2262": {
        "summary": "Prototype having our own class instead of VAP for shared ownership",
        "description": "While VAP is ok for implementing shared ownership it has some drawbacks. e.g. it forces us to change the builtin CEL rules of the API for required fields which is a maintenance burden and error prone. Besides it doesn't gives control to future api pivots we might need to execute to satisfy business needs. E.g. expose a field for dual stream support which requires picking a different ami, e.g expose a field for kubeletconfig that let us include that in the payload generation... we should be ready to pivot to have our own class which exposes only a subset of the upstream and have a controller which just renders the upstream one",
        "epic_key": "CNTRLPLANE-414"
      },
      "HOSTEDCP-2257": {
        "summary": "Vendor karpenter CRDs",
        "description": "User Story: As a (user persona), I want to be able to: create/update Karpenter resources directly without dealing with unstructured types so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Karpenter CRDs are vendored in Hypershift code Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CNTRLPLANE-414"
      },
      "HOSTEDCP-2256": {
        "summary": "CPO Refactor: Components should be deleted when predicate changes to false",
        "description": "User Story: As a (user persona), I want to be able to: As an external dev I want to be able to add new components to the CPO easily As a core dev I want to feel safe when adding new components to the CPO As a core dev I want to add new components to the CPO with our copy/pasting big chunks of code introduced a new abstraction to be used by ControlPlane components. However, when a component or a sub-resources predicate changes to false, the resources are not removed from the cluster. All resources should be deleted from the cluster. docs:",
        "epic_key": "CNTRLPLANE-308"
      },
      "HOSTEDCP-2255": {
        "summary": "E2E coverage for custom tolerations",
        "description": "QE has testing for this which detected OCPBUGS-43357 but we should make our own test and verify this in our e2e as well"
      },
      "HOSTEDCP-2234": {
        "summary": "Implement automated machine approval for karpenter instances",
        "description": "User Story: As a (user persona), I want to be able to: Instances created by karpenter can automatically become Nodes so that I can achieve Reduce operational burden. Acceptance Criteria: Description of criteria: For CAPI/MAPI driven machine management the cluster-machine-approver uses the machine.status.ips to match the CSRs. In karpenter there's no Machine resources We'll need to implement something similar. Some ideas: -- Explore using the nodeClaim resource info like status.providerID to match the CSRs -- Store the requesting IP when the ec2 instances query ignition and follow similar comparison criteria than machine approver to match CSRs -- Query AWS to get info and compare info to match CSRs (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "HOSTEDCP-2222"
      },
      "HOSTEDCP-2231": {
        "summary": "Replace Deprecated Cloud Connection with IBM Transit Gateway",
        "description": "User Story: As a (user persona), I want to be able to: Successfully create a PowerVS Hypershift cluster. The connection between VPC and PowerVS Instance should be through Transit Gateway. All cloud connection related checks should no longer be present. so that I can achieve Successfully transitioned from the deprecated IBM Cloud Connection to the IBM Transit Gateway, ensuring continued compatibility and improved network connectivity in the codebase. Acceptance Criteria: Description of criteria: Update the documentation replacing cloud connection with transit gateway. Do, mention the past usage of cloud connection and the reason of the update. Identify and refactor code dependencies on the deprecated IBM Cloud Connection. Ensure seamless integration with IBM Transit Gateway by conducting end-to-end testing. Collaborate with stakeholders to confirm compliance and resolve any issues during implementation.",
        "epic_key": "CNTRLPLANE-425"
      },
      "HOSTEDCP-2120": {
        "summary": "e2e testing automation: Add a Mechanism to Label all Pods in the Control Plane Namespace",
        "epic_key": "HOSTEDCP-2004"
      },
      "HOSTEDCP-1971": {
        "summary": "test to capture HO updates causing nodePool rollouts",
        "description": "test to capture HO updates causing nodePool reboots",
        "epic_key": "CNTRLPLANE-398"
      },
      "HOSTEDCP-2249": {
        "summary": "Move from karpenter nodepool to programatic generate userdata",
        "description": "Move from karpenter nodepool to programatic generate userdata",
        "epic_key": "CNTRLPLANE-414"
      },
      "HOSTEDCP-2237": {
        "summary": "Implement auto approval for serving CSRs for Karpenter",
        "description": "User Story: As a (user persona), I want to be able to: Instances created by karpenter can automatically become Nodes so that I can achieve Reduce operational burden. Acceptance Criteria: Description of criteria: introduced a new controller to implement auto approval for kubelet client CSRs. We need to extend to also approve serving CSRs since they are not auto approved by the cluster-machine-approver Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "HOSTEDCP-2222"
      },
      "HOSTEDCP-2075": {
        "summary": "Add missing API validation and docs for HostedCluster",
        "description": "User Story: As a (user persona), I want to be able to: Capability 1 Capability 2 Capability 3 so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CNTRLPLANE-424"
      }
    },
    "epics": {
      "HOSTEDCP-2222": {
        "summary": "Implement automated machine approval for karpenter instances",
        "description": "Goal Instances created by karpenter can automatically become Nodes Why is this important? Reduce operational burden. Scenarios For CAPI/MAPI driven machine management the cluster-machine-approver uses the machine.status.ips to match the CSRs. In karpenter there's no Machine resources We'll need to implement something similar. Some ideas: - Explore using the nodeClaim resource info like status.providerID to match the CSRs - Store the requesting IP when the ec2 instances query ignition and follow similar comparison criteria than machine approver to match CSRs - Query AWS to get info and compare info to match CSRs ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "HOSTEDCP-2226": {
        "summary": "Implement shared ownership of karpenter CRs in guest cluster",
        "description": "Goal As a service provider I the cluster admin to only manipulate fields of the nodeclass API that won't impact the service ability to operate, e.g. userdata and ami can't be changed. As a service provider I want to be the solely authoritative source of truth to set input that impacts the ability to operate AutoNode. Why is this important? The way we implement this will have UX implications for cluster admin which has direct impact on customer satisfaction. Scenarios We decided to start by using validating admission policies to implement ownership of ec2NodeClass fields. So we can restrict fields crud to a particular service account. This has some caveats: - If a field that the service own is required in the API, we need to let the cluster admin to set it on creation even though we'll clobber it by reconciling a controller. To mitigate this we might want to change the upstream CEL validations of the ec2NodeClass API - The raw userdata is exposed to the cluster admin via ec2NodeClass.spec.userdata - Since we enforce the values for userdata and ami via controller reconciliation there's potential room for race conditions If using validating policies for this proves to be satisfactory we'll need to consider alternatives, e.g: - Having an additional dedicated CRD for openshiftnodeclass that translates into the ec2NodeClass and completely prevent the cluster admin from interacting with the latter via vap. - having our own class similar to eks so we can fully manage the operational input in the backend. ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "HOSTEDCP-2004": {
        "summary": "Add a Mechanism to Label all Pods in the Control Plane Namespace",
        "description": "Goal Hypershift has a mechanism for Labeling Control Plane Pods Cluster service should be able to set the label for a given hosted cluster Why is this important? As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. See Documentation: Scenarios Given a subscriptionID of \"1d3378d3-5a3f-4712-85a1-2485495dfc4b\", there needs to be the following label on all pods hosted on behalf of the customer: {code:yaml} kubernetes.azure.com/managedby: sub_1d3378d3-5a3f-4712-85a1-2485495dfc4b{code} Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "HOSTEDCP-2220": {
        "summary": "Build and merge a HCP + Karpenter feature gated prototype",
        "description": "Goal Codify and enable usage of a prototype for HCP working with karpetner management side. Why is this important? A first usable version is critical to democratize knowledge and develop internal feedback. Acceptance Criteria Deploying a cluster with --auto-node results in karpenter running management side, the CRDs and a default ec2NodeClass installed within the guest cluster ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Etcd": {
    "stories": {
      "ETCD-717": {
        "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.19",
        "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.19",
        "epic_key": "ETCD-715"
      },
      "ETCD-709": {
        "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.18",
        "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.18",
        "epic_key": "ETCD-707"
      },
      "ETCD-726": {
        "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.21",
        "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.21",
        "epic_key": "ETCD-725"
      },
      "ETCD-677": {
        "summary": "Configure removal of etcd container from static pod manifest",
        "description": "For TNF we need to replace our currently running etcd after the installation with the one that is managed by pacemaker. This allows us to keep the following benefits: installation can stay the way it is upgrade paths are not blocked we can keep existing operator functionality around (cert rotation, defrags etc) pacemaker can control etcd without any other interference AC: on a (later specified) signal, we roll out a static pod revision without the etcd container",
        "epic_key": "ECOPROJECT-2053"
      }
    },
    "epics": {
      "ETCD-715": {
        "summary": "Rebase openshift/etcd to 3.5.19",
        "description": "rebase etcd to 3.5.19"
      },
      "ETCD-707": {
        "summary": "Rebase openshift/etcd to 3.5.18",
        "description": "rebase etcd to 3.5.18"
      },
      "ETCD-725": {
        "summary": "Rebase openshift/etcd to 3.5.21",
        "description": "rebase etcd to 3.5.21"
      }
    }
  },
  "OpenShift Installer": {
    "stories": {
      "CORS-3937": {
        "summary": "Static validations",
        "description": "system-assigned identity cannot be set on compute nodes user-assigned identity on compute nodes must == 1 assigned identity must match type assigned identity name is a valid guid",
        "epic_key": "CORS-3883"
      },
      "CORS-3919": {
        "summary": "Update Cluster Infra Manifest",
        "description": "User Story: As a (user persona), I want to be able to: Pass the custom endpoints to all cluster components Capability 2 Capability 3 so that I can achieve All cluster components should use the same api endpoints. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Fill out the API infra config with the custom endpoints when the user has supplied them via the install-config Bring in the API changes as a vendor update. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3918": {
        "summary": "Pass custom endpoints to CAPG",
        "description": "User Story: As a (user persona), I want to be able to: Pass the custom endpoints to CAPG Capability 2 Capability 3 so that I can achieve Use the custom endpoints upstream Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Update the CAPG version that the installer uses Update the CRD for CAPG Pass the endpoints (when user entered the applicable ones) to CAPG through the Cluster Spec This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3917": {
        "summary": "Validate custom endpoints",
        "description": "User Story: As a (user persona), I want to be able to: Ensure that the custom endpoints are valid before use. Reach the endpoints provided. Capability 3 so that I can achieve Ensuring that the custom endpoint connectivity are not the reason for any installation issues. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3908": {
        "summary": "Use custom endpoints in cloud-provider-gcp",
        "description": "User Story: As a (user persona), I want to be able to: Use custom endpoints through out all cluster components Capability 2 Capability 3 so that I can achieve Using the same api endpoints through the cluster Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: {code:java} providers/gce/gce_fake.go: service, err := compute.NewService(context.Background(), option.WithoutAuthentication()) providers/gce/gce.go: service, err := compute.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: serviceBeta, err := computebeta.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: serviceAlpha, err := computealpha.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: containerService, err := container.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) {code} Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3906": {
        "summary": "Use custom endpoints in MAPI for GCP",
        "description": "User Story: As a (user persona), I want to be able to: Use the custom endpoints in MAPI that were set in the installer. Override the endpoints for: compute tagging so that I can achieve Using the same custom endpoints for all of the services in the cluster. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: The services with endpoints to be overwritten can be found: pkg/cloud/gcp/actuators/services Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3873": {
        "summary": "Place ingress LBs on specific subnets",
        "description": "User Story: The installer will apply the specified IngressControllerLB subnets to the default IngressController's spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.classicLoadBalancer.subnets or ...networkLoadBalancer.subnets field (based on platform.aws.lbType) in the manifest generated by the generateDefaultIngressController function. Acceptance Criteria: Description of criteria: Ingress LB is placed on specific subnet(s) as specified by user (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: For sanity checking, you can run openshift-install create manifests and check the cluster-ingress-.yaml manifests If no roles are specified in the installconfig, no subnets are supplied in the manifest (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3872": {
        "summary": "Place machines only in clusternode subnets (and bootstrap node in bootstrap subnet)",
        "description": "User Story: Machines should only be placed in subnets specified as clusternode. Bootstrap node should be on subnet with bootstrap node role. Acceptance Criteria: Description of criteria: See above (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: This needs to be specified in both the CAPI-created machines Better link for MAPI machines: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3869": {
        "summary": "Static validations (pkg/types)",
        "description": "User Story: Static validations (no API connection required) Acceptance Criteria: Description of criteria: -The subnet IDs must be valid:- -Start with subnet-- -Length is exactly 24 characters- The subnet IDs must not include duplicates Maximum of 10 IngressController Subnets Validation For a subnet that has defined roles Roles must be of supported types (i.e. from a set of defined roles) Roles must not be duplicate. This and check naturally validates that a subnet can only have max 5 roles EdgeNode cannot be combined with any other roles ClusterNode, IngressControllerLB, ControlPlaneExternalLB (if cluster is external), and ControlPlaneInternalLB must be assigned to at least 1 subnet A subnet cannot have both role ControlPlaneExternalLB and ControlPlaneInternalLB If the cluster is internal, ControlPlaneExternalLB must not be assigned to any subnets.| Some validations are extracted from API validation (i.e. the intstaller does not handle CEL at this time) and Xvalidation markers (i.e. defined in the enhancement proposal). (optional) Out of Scope: Validations that require access to the AWS API will go in pkg/asset (different card) Engineering Details: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3868": {
        "summary": "Deprecate installconfig.platform.aws.subnets",
        "description": "User Story: As an openshift-install user I want to be able to continue to use aws.subnets during deprecation (a warning will show) As an openshift developer, I only want a single code path for subnets (via upconversion) Acceptance Criteria: Description of criteria: Validation that both fields are not simultaneously specified When aws.subnets is specified, it's upconverted into aws.vpc.subnets Existing pkg/types/aws/Subnets type is renamed to DeprecatedSubnets Remove all (or as many possible) usages of DeprecatedSubnets, replaced with the new vpc.Subnets field We may need to keep usage of DeprecatedSubnets for certain validations Warning when using deprecated field (optional) Out of Scope: . Engineering Details: Conversion package: Review how subnets are used in and whether any changes/refactoring is needed",
        "epic_key": "CORS-3440"
      },
      "CORS-3864": {
        "summary": "Integrate CAPZ changes",
        "description": "Once CAPZ changes are integrated into upstream or our fork, we need to vendor those to the installer.",
        "epic_key": "CORS-3272"
      },
      "CORS-3861": {
        "summary": "Refactor resource group creation/reconciliation to be handled by CAPZ",
        "description": "Currently RG creation is handled by the installer SDK. It can and should be handled by CAPZ so that we have less code to maintain and do not need to handle separate configurations for Azure & Azure Stack",
        "epic_key": "CORS-3272"
      },
      "CORS-3855": {
        "summary": "Hybrid SRE: Remove ARO build-flag in openshift-installer",
        "description": "User Story: As a openshift developer, I want to be able to: Find all the places that the IsARO() function is used and either remove or rework the code so that it is no longer used. Figure out how ARO is using managed identities with IsARO() and rework the code so that it is not ARO specific. so that I can achieve Understanding of what work needs to be done. Acceptance Criteria: Description of criteria: Documentation of the required work.",
        "epic_key": "CORS-3489"
      },
      "CORS-3854": {
        "summary": "Hybrid SRE: Add support to enable boot diagnostics option at installation time in Azure",
        "description": "User Story: As a openshift developer, I want to be able to: Enable boot diagnostics on worker nodes. Determine if setting it enabled by default is okay. so that I can achieve Better debugging. Acceptance Criteria: Description of criteria: Documentation of the work that needs to be done.",
        "epic_key": "CORS-3490"
      },
      "CORS-3843": {
        "summary": "Add Tech Preview Feature gate to Installer",
        "description": "User Story: As a (user persona), I want to be able to: Add the Tech Preview Feature Gate to the installer for custom endpoints Validate the custom endpoints feature gate in the installer Capability 3 so that I can achieve An installer feature gate to ensure users know that this feature is not yet slated for release Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3842": {
        "summary": "Add GCP Endpoint Tech Preview Feature to API",
        "description": "User Story: As a (user persona), I want to be able to: Add the feature to API Add tech preview tags for the feature Capability 3 so that I can achieve Protect installs using this feature. The feature will touch many aspects of openshift Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3835": {
        "summary": "Add Endpoints to GCP Platform in Install Config",
        "description": "User Story: As a (user persona), I want to be able to: Enter the custom endpoints via the install config Capability 2 Capability 3 so that I can achieve Initiate an install where the custom endpoints for GCP APIs can be used. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: The user enters the data into the install-config. The data is validated. Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3637": {
        "summary": "OWNERS files for platform providers",
        "description": "Most of the platform subdirectories don't have OWNERS files we should add the aliases for everything that's missing backport to 4.16",
        "epic_key": "CORS-3623"
      },
      "CORS-3960": {
        "summary": "Remove Terraform (!)",
        "description": "User Story: When Terraform is no longer used, I want to quit building any terraform artifacts, images, or ci jobs. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate."
      },
      "CORS-3959": {
        "summary": "Installer Hook Azure Stack Provisioning",
        "description": "User Story: All the installer hook (non-CAPZ) provisioning for Azure stack Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3272"
      },
      "CORS-3936": {
        "summary": "Support creating clusters with AWS Public IP",
        "description": "User Story: As a (user persona), I want to be able to: Create IPI AWS clusters with Public IP-only option (aka. NAT-less deployment) with managed-VPC so that I can achieve significant cost saving for OpenShift CI Currently, openshift-installer supports \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" environment, which will skip creating NAT Gateway. However, the above env will make openshift-installer skip the entire creation of VPC, which means, the VPC must be manually created beforehand. Our goal is to modify the behaviour of \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" to allow omitting \"subnets\" section in install config and let the openshift-installer to create the VPC and Internet Gateway (in replacement of NAT Gateway). Acceptance Criteria: Description of criteria: Passing \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" without \"subnets\" section in install config will not report: {noformat} level=error msg=failed to fetch Master Machines: failed to load asset \"Install Config\": failed to create install config: platform.aws.subnets: Required value: subnets must be specified for public-only subnets clusters {noformat} (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate."
      },
      "CORS-3911": {
        "summary": "Use custom endpoints in GCP PD CSI Driver",
        "description": "User Story: As a (user persona), I want to be able to: Use custom endpoints through out all cluster components Capability 2 Capability 3 so that I can achieve Using the same api endpoints through the cluster Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: {code:java} pkg/gce-cloud-provider/compute/gce.go: service, err := computebeta.NewService(ctx, computeOpts...) pkg/gce-cloud-provider/compute/gce.go: service, err := compute.NewService(ctx, computeOpts...) pkg/gce-cloud-provider/compute/gce_test.go: service, _ := compute.NewService(ctx, computeOpts...){code} Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3907": {
        "summary": "Use custom endpoints in the cluster ingress operator",
        "description": "User Story: As a (user persona), I want to be able to: Use the same custom endpoints (when applicable) through out all cluster components Capability 2 Capability 3 so that I can achieve Allowing users to override api endpoints in the cluster components. The DNS api is found here. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. pkg/dns/gcp/provider.go {code:java} dnsService, err := gdnsv1.NewService(context.TODO(), option.WithCredentialsJSON(config.CredentialsJSON), option.WithUserAgent(config.UserAgent)) {code} (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-2389"
      },
      "CORS-3871": {
        "summary": "Place API LBs in specified subnets",
        "description": "User Story: The installer will configure the ControlPlaneInternalLB subnets in the spec.controlPlaneLoadBalancer.subnets field of the CAPA AWSCluster object, while the ControlPlaneExternalLB subnets will be set in the spec.secondaryControlPlaneLoadBalancer.subnets field. Acceptance Criteria: Description of criteria: Loadbalancers are created in the subnets specified in the install config. (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: CAPA type: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3870": {
        "summary": "AWS API Validations",
        "description": "User Story: I want to run pre-flight checks against the AWS API for the subnets I provided in the install config. Acceptance Criteria: Description of criteria: All Subnets Belong to the Same VPC Validation Consistent Cluster Scope with IngressControllerLB Subnets Validation Reject BYO VPC Installations that Contain Untagged Subnets Reject Duplicate AZs (optional) Out of Scope: Engineering Details: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3867": {
        "summary": "Allow users to specify subnets with roles in install config",
        "description": "User Story: As an openshift-instlal user I want to be able to specify AWS subnets with roles. Acceptance Criteria: Description of criteria: Installconfig has installconfig.platform.aws.vpc.subnets field vpc.subnets conforms to API defined in the enhancement godoc/oc explain text is written and generated (see explain docs| (optional) Out of Scope: Validations will be handled in a different card. Engineering Details: API for installconfig field is defined in the type for subnets is: id - string roles - slice of SubnetRoles the list of subnet roles is defined in the enhancement. they include ClusterNode, EdgeNode, ControlPlaneExternalLBSubnetRole, etc... (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3440"
      },
      "CORS-3825": {
        "summary": "Add azure disk nvme controller support",
        "description": "User Story: As a (user persona), I want to be able to: Capability 1 Capability 2 Capability 3 so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "CORS-3771"
      },
      "CORS-2508": {
        "summary": "Unused AWS action DescribeAutoScalingGroups",
        "description": "The OpenShift installer includes a autoscaling:DescribeAutoScalingGroups IAM permission which I believe its not used and carried over from something that may have existed in Hive a long time ago or maybe not at all. Reference installer commit they too don't see it. Done criteria: - Test creating a cluster with ROSA (Hive) - Create enabling auto scaling - Destroy cluster and ensure everything was cleaned up - Validate with CloudTrail that the IAM call was not used"
      }
    },
    "epics": {
      "CORS-3883": {
        "summary": "Azure: Remove Automatic Identity Creation",
        "description": "OCP/Telco Definition of Done Epic Goal Remove automatic (opinionated) creation (and attachment) of identities to Azure nodes Allow API to configure identities for nodes Why is this important? Creating and attaching identities to nodes requires elevated permissions The identities are no longer required (or used) so we can reduce the required permissions Scenarios Users want to do a default ipi install that just works without the User Access Admin role Users want to BYO user-assigned identity (requires some permissions) Users want to use a system assigned identity Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3927": {
        "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3923": {
        "summary": "GCP - Add support to deploy Confidential VMs using AMD SEV-SNP",
        "description": "Epic Goal Add support to deploy Confidential VMs on GCP using AMD SEV-SNP technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using SEV-SNP technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-2389": {
        "summary": "OpenShift Installer to support Private Google Access to GCP endpoints",
        "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
      },
      "CORS-3440": {
        "summary": "Add ability to choose ingress controller subnets at installation",
        "description": "OCP/Telco Definition of Done Epic Goal Add the ability to choose subnets for IngressControllers with LoadBalancer-type Services for AWS in the Installer. This install config should be applies to the default IngressController and all future IngressControllers (the design is similar to installconfig.platform.aws.lbtype Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3272": {
        "summary": "Provision Azure Stack Infra with CAPI",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3489": {
        "summary": "Hybrid SRE: Remove ARO build-flag in openshift-installer",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3490": {
        "summary": "Hybrid SRE: Add support to enable boot diagnostics option at installation time in Azure",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3623": {
        "summary": "Technical debt for 4.19",
        "description": "Epic Goal This epic includes tasks the team would like to tackle to improve our process, QOL, CI. It may include tasks like updating the RHEL base image and vendored assisted-service. Why is this important? We need a place to add tasks that are not feature oriented. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORS-3278": {
        "summary": "Replace Terraform with CAPI Provider for IBM Cloud",
        "description": "Epic Goal Replace Terraform infrastructure and machine (bootstrap, control plane) provisioning with CAPI-based approach."
      },
      "CORS-3771": {
        "summary": "Azure - Add support for Dxv6 machine series",
        "description": "Epic Goal Dlsv6 Dsv6 Why is this important? ARO will need to support Dxv6 instance types supported. These are currently in preview Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Core Networking": {
    "stories": {
      "CORENET-5786": {
        "summary": "ux improvement: provide decent error msg if the provided physical network name is invalid",
        "description": "We want to throw a decent error msg to the user (seen when describing the pod) in case the network to which to pod is being attached features an invalid (missing) bridge mapping name - i.e. the physical network name points to a mapping that doesn't exist.",
        "epic_key": "CORENET-5358"
      },
      "CORENET-5751": {
        "summary": "origin: use l2bridge binding for virt suite",
        "description": "We're still using the passt binding, which prevents us from checking the TCP connection persistence. We should start using the correct binding - l2bridge - which will allow us to QE the real binding our customers are using, and will also allow us to improve our test coverage, since we would be able to align w/ our upstream tests.",
        "epic_key": "CORENET-5649"
      },
      "CORENET-5587": {
        "summary": "API Port CUDN e2e tests to openshift/origin",
        "description": "Port e2e tests of CUDN introduced on U/S by",
        "epic_key": "CORENET-4931"
      },
      "CORENET-5575": {
        "summary": "Add placeholder GA tests for persistent IPs feature on openshift conformance tests"
      },
      "CORENET-5481": {
        "summary": "Fix security issues with CNO IPSec certificate signing",
        "description": "In CNO we have an approver that signs certs automatically, without checking any identity information. We should modify this to require that the certificate request contains the kubelet certificate (issued separately) to ensure the identity of the client is an openshift node. We should not just hand out certificates to anyone who asks for them.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-5479": {
        "summary": "UDN API Make IPAM options for explicit in the API; cater to common use cases",
        "description": "See the UDN Sync Meeting notes: In our current UDN API, subnets field is mandatory always for primary role and optional for secondary role. This is because users are allowed to have a pure L2 without subnets for secondary networks. However, in the future if we want to add egress support on secondary networks, we might need subnets... CNV has many different use cases: For UDPNs, we always need subnets for L2 and L3 why not make them optional and let users get default values? - drawback is loosing visibility and this podsubnet now conflicting with other internal subnets and customer in their ignorange have the oopsy stage, we have seen this in plenty with joinsubnets already For UDSNs, we may or maynot have the need for IPAM, today this subnets field is optional, but then when we do need subnets we cannot set default values here so its icky. This card tracks the design changes to the API and the code changes needed to implement this. See for details.",
        "epic_key": "CORENET-4931"
      },
      "CORENET-5389": {
        "summary": "VM/Pod using OVN localnet network unable to access its own host IP/Ingress",
        "description": "Description of problem: VM or Pod utilizing the OVN localnet network is unable to access its own host IP or the ingress service when both are hosted on the same node. However, the VM/Pod can successfully ping other compute nodes and communicate with external networks. Observed Results: VM/Pod = VM/Pod's Host: Not working VM/Pod = Ingress (same host): Not working VM/Pod = Other Hosts: Works VM/Pod = Ingress (different host): Works External = VM/Pod: Works VM/Pod = External Networks: Works While the issue involves the inability to access the host from the VM, our goal is to access the ingress of the hosting cluster. Version-Release number of selected component (if applicable): 4.16 How reproducible: 100% Steps to Reproduce: 1. Create nncp object with localnet: apiVersion: nmstate.io/v1 kind: NodeNetworkConfigurationPolicy metadata: annotations: description: localnet1 network is mapped to the br-ex bridge (OVN) name: localnet1-nncp spec: desiredState: ovn: bridge-mappings: - bridge: br-ex localnet: localnet1 state: present nodeSelector: node-role.kubernetes.io/worker: '' 2. Create NetworkAttachmentDefinition object apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: localnet1-network spec: config: - \\{ \"name\": \"localnet1-network\", \"mac\": \"XX:XX:XX:XX:88:05\", \"ips\": \"XX.XX.XX.11/24\" } labels: app: test-pod spec: containers: - name: test-pod image: docker.io/library/alpine:latest command: \"/bin/sleep\", \"10000\" ~~~ 4. Attempt to access the host IP or ingress service located on the same host node. Hypervisor (hosting Pod) IP: XX.XX.XX.70 Hypervisor (Another worker in cluster) IP: XX.XX.XX.71 Ping to own host fails ping XX.XX.XX.70 PING XX.XX.XX.70 (XX.XX.XX.70): 56 data bytes ^C --- XX.XX.XX.70 ping statistics --- 8 packets transmitted, 0 packets received, 100% packet loss Ping to other worker in environment works / ping XX.XX.XX.71 -c 1 PING XX.XX.XX.71 (XX.XX.XX.71): 56 data bytes 64 bytes from XX.XX.XX.71: seq=0 ttl=42 time=0.199 ms Actual results: VM/Pod using OVN localnet network unable to access its own host IP/Ingress Expected results: VM/Pod using OVN localnet network should be able to access Ingress Additional info: Affected Platforms: Is it an internal RedHat testing failure.",
        "epic_key": "CORENET-5878"
      },
      "CORENET-5374": {
        "summary": "openshift/origin: only provision workloads when network creation has started"
      },
      "CORENET-5371": {
        "summary": "Evaluate improvements to ovs-monitor-ipsec/libreswan",
        "description": "If opportunistic IPSec is a dead-end we need to investigate whether or not further improvements to what we have is possible. Some ideas are: Improve ovs-monitor-ipsec logic to reconcile. Ilya is already working on this: Check if there is a native API solution where we can talk to libreswan, instead of forking bash commands. strongswan enables dynamic config file watching, consult with IPSec team if it is possible for that same capability to be added to libreswan. Then we eliminate the need to manage ipsec connections in ovs-monitor-ipsec, and can simply update the config file. Examine using a different ipsec control plane (maybe strongswan) instead of libreswan.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-5241": {
        "summary": "Enable support to enable OVN-Kubernetes BGP in CNO",
        "description": "CNO should deploy the new RouteAdvertisements OVN-K CRD. When the OCP API flag to enable BGP support in the cluster is set, CNO should enable support on OVN-K through a CLI arg.",
        "epic_key": "CORENET-4947"
      },
      "CORENET-5080": {
        "summary": "Make CNO to react for Machine Config Pool status",
        "description": "The CNO rolls out ipsec mc plugin for rolling out IPsec for the cluster, but it doesn't really check master and work role machine config pools status to confirm if that's successfully installed in the cluster nodes. Hence CNO should be made to listen for MachineConfigPool status object updates and set network operator condition accordingly based on ipsec mc plugin rollout status.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-4974": {
        "summary": "L2 NetworkPolicy Support NetworkPolicies on Primary UDNs",
        "description": "We want to do Network Policies not MultiNetwork POlicies",
        "epic_key": "CORENET-4931"
      },
      "CORENET-4056": {
        "summary": "Improve ipsec tests",
        "epic_key": "CORENET-5361"
      },
      "CORENET-821": {
        "summary": "Whereabouts Downstream Merge",
        "description": "DS merge for fast range fixes and dep bumps"
      },
      "CORENET-710": {
        "summary": "Whereabouts fast ranges: Get fast ranges working in the CNO",
        "description": "The CNO should deploy the fast ranges CRDs We need to add this CRD And it'll be deployed with the CNO in here: Goal: Enable the perf/scale team to use Whereabouts fast ranges (as well as other interested parties, such as telco)",
        "epic_key": "CORENET-664"
      },
      "CORENET-5914": {
        "summary": "API Replicate CRD changes to CNO",
        "description": "Following extensions of CUDN CRD to support Localnet topology, the CRD changes should be replicated to CNO in order to make them available on OCP.",
        "epic_key": "CORENET-5358"
      },
      "CORENET-5856": {
        "summary": "Update RA API with new universal network selector",
        "epic_key": "CORENET-5350"
      },
      "CORENET-5743": {
        "summary": "CNCC 1.32 Kube Rebase",
        "epic_key": "CORENET-5635"
      },
      "CORENET-5721": {
        "summary": "SGW Add support for Layer-2 UDNs",
        "description": "The main difficulty of supporting L2 UDN is not having a node-specific pod network subnet to advertise with that node as next hop: L2 UDNs subnet is cluster wide. One of the ideas was to advertise /32 pod specific routes but there are concerns on the scalability of that. The other idea is to advertise the whole L2 UDN subnet with the selected nodes as next hop and let multi-path take care of the rest. With this alternative there is acceptance that this might not always route the traffic on the most optimum path. This effort entails adding support for it in cluster manager route advertisement controller, and mimic the existing support of L3 zone and node network controllers for L2 as well. This includes upstream testcases that should basically mimic L3 existing test cases which add some level of dependency with SDN-5712.",
        "epic_key": "CORENET-5350"
      },
      "CORENET-5711": {
        "summary": "CNO 1.32 Kube rebase",
        "epic_key": "CORENET-5635"
      },
      "CORENET-5678": {
        "summary": "CNO: update MNP CRD",
        "epic_key": "CORENET-5645"
      },
      "CORENET-5666": {
        "summary": "Downstream Merge of OVN-Kubernetes 1.32 rebase",
        "epic_key": "CORENET-5635"
      },
      "CORENET-5581": {
        "summary": "Add ipsec upgrade ci job as mandatory lane",
        "description": "The e2e-aws-ovn-ipsec-upgrade job is currently an optional job and always_run: false because the job not reliable and success rate is so low. This must be made as mandatory CI lane after fixing its relevant issues.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-5524": {
        "summary": "Revert libreswan-4.6 change in openshift/os repo",
        "description": "Revert libreswan-4.6 change in openshift/os repo made via once final fixes in libreswan for bug| is ready to be consumed in openshift.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-5387": {
        "summary": "Set the NetworkSegmentation FG on persistent IPs conformance tests using primary UDNs"
      },
      "CORENET-5382": {
        "summary": "Restore default libreswan version in ovnk image",
        "description": "The SDN-5480 gets libreswan4.6 into ovnk image to use stable libreswan version which fixes connection/timeout/crash issues. But we must restore libreswan to default version in ovnk image once these issues are addressed in any of forthcoming RHEL release.",
        "epic_key": "CORENET-5361"
      },
      "CORENET-5253": {
        "summary": "Downstream e2e CI tests for PodNetwork Advertisement",
        "epic_key": "CORENET-5350"
      },
      "CORENET-5115": {
        "summary": "monitoringL2/L3 Open default network ports on UDN pods via users's request through pod annotations",
        "epic_key": "CORENET-4931"
      },
      "CORENET-4481": {
        "summary": "Whereabouts perf scale: Re-enable opt-in for the node slice controller.",
        "description": "We made a compromise to automatically enable this for now in order to move forward with perf/scale testing as a short term item. While this shouldn't have a large impact on cluster performance in general (even when installed without use), it should still be optional to use. See also:",
        "epic_key": "CORENET-664"
      },
      "CORENET-454": {
        "summary": "temp logging change for debugging release",
        "description": "temp for bot"
      },
      "CORENET-365": {
        "summary": "Whereabouts Downstream Merge"
      }
    },
    "epics": {
      "CORENET-5358": {
        "summary": "Universal connectivity: Localnet 4.19",
        "description": "Template: Networking Definition of Planned Epic Goal Provide quality user experience for customers connecting their Pods and VMs to the underlying physical network through OVN Kubernetes localnet. Why is this important? This is a continuation to It covers the UDN API for localnet and other improvements Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (/) Priority+ is set by engineering - (/) +Epic must be Linked to a +Parent Feature+ - (/) Target version+ must be set - (/) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated This must be done downstream too Release Technical Enablement - Provide necessary release enablement details and documents. OVN Kubernetes secondary networks with the localnet topology can be created through ClusterUserDefinedNetworks When possible, user input is validated and any configuration issue is shown on the UDN. Alternatively some issues can be shown on CNI ADD events on Pod -Definition of these networks can be changed even if there are Pods connected to them. When that happens, the UDN is marked as degraded until all the \"old\" pods are gone. The mutable fields should be: MTU, VLAN, physnet name- For cases where a user incorrectly set their MTU, VLAN, or physnet name, there is a clear and foolproof flow describing how to correct this mistake. A single \"bridge-mappings\" \"localnet\" can be referenced from multiple different UDNs The default MTU set for localnet is 1500 Pod requesting UDN without a VLAN is able to connect to services running on the host's network ({-}stretch) The \"physnet\" mapping is a \"supported API\" and available to users - so they can connect to the machine network without a need to configure a custom bridge-mapping{-} we should just always request user to configure the mapping themselves, until we understand all the implications of non-NORMAL mode on br-ex and how it works with local access / bondings / ... (stretch) Scheduling is managed by the platform - if a UDN requests a localnet (as in bridge-mappins.localnet), the Pod requesting this UDN will be only scheduled on a node with this resource available. This can use the same mechanism as the SR-IOV operator - combination of device plugins and \"k8s.v1.cni.cncf.io/resourceName\" annotation ... IPAM is not in the scope of this epic. See RFE-6947. Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORENET-5878": {
        "summary": "Universal connectivity: Localnet 4.20",
        "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORENET-4947": {
        "summary": "4.18 OVN Kubernetes support for BGP as a routing protocol",
        "description": "Epic Goal OVN Kubernetes support for BGP as a routing protocol. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORENET-664": {
        "summary": "Tech Preview Whereabouts: Performance and scale considerations",
        "description": "Epic Goal Address performance and scale issues in Whereabouts IPAM CNI Why is this important? Whereabouts is becoming increasingly more popular for use on workloads that operate at scale. Whereabouts was originally built as a convenience function for a handful of IPs, however, more and more customers want to use whereabouts in scale sitatuions. Notably, for telco and ai/ml scenarios. Some ai/ml scenarios launch a large number of pods that need to use secondary networks for related traffic. Supporting Documents Upstream collaboration outline| Acceptance Criteria Both original allocation mode, and fast_ranges mode work without user intervention (e.g. backwards compatible) Reconciler still works with fast_ranges fast_ranges: 50% reduction in allocation time over original allocation method"
      },
      "CORENET-5350": {
        "summary": "4.20 GA OVN Kubernetes support for BGP as a routing protocol: On-Prem",
        "description": "Epic Goal Left over from 4.18 (potentially BGP+UDN, egress IP) perf/scale UX fixes (ovnk specific API) Enabling subset of nodes selected for BGP advertisement with pod network (requirement from customers) Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORENET-5635": {
        "summary": "Rebase Kube version to 1.32 in repos maintained by the SDN team",
        "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "CORENET-5645": {
        "summary": "Support EndPort in MultiNetworkPolicy",
        "description": "Template: Networking Definition of Planned Epic Goal Add support for endPort field in multinetworkpolicy. The API change is merged upstream time to make d/s update. The support tracked in this epic is only for ovn-kubernetes, multi-netpol implementation will catch up later. It is known that MNP lacks some validations that networkpolicy has due to being a core API. It needs additional discussion (and potentially breaking changes) to decide whether MNP should also introduce similar validation. Opened a bug Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Virtualization": {
    "stories": {
      "CNV-59853": {
        "summary": "Update old console classes",
        "description": "Followup to: Also Catalog - Template catalog is visually broken in kubevirt-ui"
      },
      "CNV-58647": {
        "summary": "DEV: Networking: PatternFly Modal deprecated in PF6",
        "description": "update PF modal",
        "epic_key": "CNV-57714"
      },
      "CNV-57648": {
        "summary": "upstream documentation for descheduler integration with hcp nodepools",
        "description": "In the hypershift upstream documentation, outline how the de-scheduler can be used to continually redistribute VMs in a nodepool when clumping of VMs occur after live migration.",
        "epic_key": "CNV-41958"
      },
      "CNV-55987": {
        "summary": "DEV: Aggregate the alerts in the Alerts page by alert name and severity",
        "description": "Goal Description As an Admin I want to have an easy view of the alerts that are firing in my cluster. If I have the same alert that fires many time it is very hard to identify the issues. We can simplify the existing Alerts page to make it much clearer by the following quick fix: 1. Aggregate the alerts by alert name and severity 2. For each aggregated line add the \"Total alerts number\" 3. When pressing on the aggregated line it can be expanded with the list of alerts 4.Optional - Add the namespace label to the expanded list of alerts, where each alert can have a different namespace. Note: Not all alerts have this label. Initial mockup by ~fkargbo: Acceptance Criteria Given we have several alerts of the same name and severity When we go to the Alerts page and view the alerts Then We would see a single line for each \"alert name\" and \"severity\", the number of times its shown and I can click on the line to expend the line and get the full list of alerts of that name and severity. User Stories High-Level goal-based user story, with context. \"As a VM owner/cluster administrator, I want to Achieve Some Goal, so that Some Reason/Context.\" another user story Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes Any additional details or decisions made/needed",
        "epic_key": "CNV-54399"
      },
      "CNV-58320": {
        "summary": "DEV: Remove Console classes in Networking UI",
        "epic_key": "CNV-58313"
      },
      "CNV-56305": {
        "summary": "DEV: PatternFly6 Upgrade NETWORK UI",
        "description": "Goal Upgrade CNV make use of PatternFly6 User Stories Notes Any additional details or decisions made/needed",
        "epic_key": "CNV-56300"
      }
    },
    "epics": {
      "CNV-57714": {
        "summary": "PatternFly Modal deprecated in PF6",
        "description": "PatternFly Modal was deprecated in PF6, we should replace this component with a new Modal. This issue is a followup to updating to PF6:"
      },
      "CNV-41958": {
        "summary": "HCP KubeVirt VM Enhanced Topology Spread",
        "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints."
      },
      "CNV-54399": {
        "summary": "Aggregate the alerts in the Alerts page by alert name and severity",
        "description": "Goal Description As an Admin I want to have an easy view of the alerts that are firing in my cluster. If I have the same alert that fires many time it is very hard to identify the issues. We can simplify the existing Alerts page to make it much clearer by the following quick fix: 1. Aggregate the alerts by alert name and severity 2. For each aggregated line add the \"Total alerts number\" 3. When pressing on the aggregated line it can be expanded with the list of alerts 4.Optional - Add the namespace label to the expanded list of alerts, where each alert can have a different namespace. Note: Not all alerts have this label. Initial mockup by ~fkargbo: Acceptance Criteria Given we have several alerts of the same name and severity When we go to the Alerts page and view the alerts Then We would see a single line for each \"alert name\" and \"severity\", the number of times its shown and I can click on the line to expend the line and get the full list of alerts of that name and severity. User Stories High-Level goal-based user story, with context. \"As a VM owner/cluster administrator, I want to Achieve Some Goal, so that Some Reason/Context.\" another user story Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes Any additional details or decisions made/needed"
      },
      "CNV-46603": {
        "summary": "UI for OVN Kubernetes: Primary user-defined networks",
        "description": "Goal Primary used-defined networks can be managed from the UI and the user flow is seamless. User Stories As a cluster admin, I want to use the UI to define a ClusterUserDefinedNetwork, assigned with a namespace selector. As a project admin, I want to use the UI to define a UserDefinedNetwork in my namespace. As a project admin, I want to be queried to create a UserDefinedNetwork before I create any Pods/VMs in my new project. As a project admin running VMs in a namespace with UDN defined, I expect the \"pod network\" to be called \"user-defined primary network\", and I expect that when using it, the proper network binding is used. As a project admin, I want to use the UI to request a specific IP for my VM connected to UDN. UX doc Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes The user-defined networks design, including the API, is available here:"
      },
      "CNV-58313": {
        "summary": "Remove Console classes (prefixed with co-, ocs-, odc-) in plugins",
        "description": "Remove Console classes in Kubevirt, Network and NMState UI plugins. - remove those, which have / will have their styling removed Details also on Slack:"
      },
      "CNV-56300": {
        "summary": "PatternFly6 Upgrade NETWORK UI",
        "description": "Goal Upgrade NETWORK UI make use of PatternFly6 User Stories Notes Any additional details or decisions made/needed"
      }
    }
  },
  "Cluster Integration and Delivery": {
    "stories": {
      "CLID-321": {
        "summary": "As a user of a disconnected cluster, I would like to identify easily the custom resources generated by oc-mirror applied to the cluster"
      },
      "CLID-310": {
        "summary": "As an oc-mirror user, I want to be able to skip signature mirroring",
        "description": "For users that might not have their policy.json and/or registries.d correctly configured, one might want to skip signature verification and mirroring completely. This story doesn' t provide a granular way (per image) way to skip signature mirroring. This story only provides a way to enable/disable signature mirroring as a whole. We need to also verify the behavior behind the existing command line arg secure-policy We need to at least ask PM if other parameters related to signature configuration found in skopeo/podman should also be available in oc-mirror. Ex: {code:java} // This is what skopeo uses to not verify signatures --insecure-policy run the tool without any policy check // This is what skopeo uses to set different locations for policy.json and registries.d --policy string Path to a trust policy file --registries.d DIR use registry configuration files in DIR (e.g. for container signature storage) // This is what skopeo uses to stop copying signatures --remove-signatures Do not copy signatures from SOURCE-IMAGE // these shouldn't be needed. --sign-by FINGERPRINT Sign the image using a GPG key with the specified FINGERPRINT --sign-by-sigstore PATH Sign the image using a sigstore parameter file at PATH --sign-by-sigstore-private-key PATH Sign the image using a sigstore private key at PATH --sign-identity string Identity of signed image, must be a fully specified docker reference. Defaults to the target docker reference. --sign-passphrase-file PATH Read a passphrase for signing an image from PATH {code}",
        "epic_key": "CLID-289"
      },
      "CLID-309": {
        "summary": "As an oc-mirror user, I want cosign signature tags to be mirrored alongside images during disk to mirror workflow",
        "description": "Acceptance criteria When mirroring from disk to mirror, with an imageSetConfig containing an additional signed image, from an archive that was previously verified to contain the signatures corresponding to that image, the mirror registry should contain all signature tags corresponding to the mirored image",
        "epic_key": "CLID-289"
      },
      "CLID-308": {
        "summary": "As an oc-mirror user, I want cosign signature tags to be incrementally saved to archives during mirror to disk workflow",
        "description": "Acceptance criteria when performing a mirror to disk with an empty working-dir, and an imagesetconfig containing a signed additional image, the archive generated contains the signature manifest AND the blobs corresponding to that manifest when performing a mirror to disk with an existing working-dir, and the same imagesetconfig as a previous run from a previous day, the archive doesn't contain signatures that were included in a previous archive",
        "epic_key": "CLID-289"
      },
      "CLID-303": {
        "summary": "Removal of the selected bundles feature",
        "epic_key": "CLID-302"
      },
      "CLID-246": {
        "summary": "Helm chart signature support",
        "description": "Currently helm chart support in v2 does not mirror and verify signatures, this user story is to implement the mirroring of the signatures and the verification of them.",
        "epic_key": "CLID-289"
      },
      "CLID-347": {
        "summary": "Create defaults configs for signature mirroring/verification",
        "description": "In operating systems (OS) where the registries.d and policy.json does not include our internal registries and the field use-sigstore-attachment: true, it is necessary to have a default embedded in oc-mirror. For oc-mirror cache: {code:java} docker: localhost:55000: use-sigstore-attachments: true{code} For customer regitry (only an example of a registry running on localhost:6000 below) {code:java} docker: localhost:6000: use-sigstore-attachments: true{code} For the release images: {code:java} docker: quay.io: use-sigstore-attachments: true{code} For operator catalog and bundles: {code:java} docker: registry.access.redhat.com: use-sigstore-attachments: true lookaside: {code:java} docker: registry.redhat.io: use-sigstore-attachments: true lookaside: Reference about containers/image policy.json/registries.d:",
        "epic_key": "CLID-289"
      },
      "CLID-307": {
        "summary": "As an oc-mirror user, I want cosign signature tags to be mirrored alongside images during mirror to mirror workflow",
        "description": "Acceptance criteria Signed additional images have all corresponding signature tags available in the mirror registry -Signed catalog images have all corresponding signature tags available in the mirror registry- Signed Operator images (and related) have all corresponding signature tags available in the mirror registry Signed Release images have all corresponding signature tags available in the mirror registry Signed Release payload images have all corresponding signature tags available in the mirror registry A manifest list image shall have a signature tag for the manifest list, and 1 signature tag for each manifest included in the manifest list.",
        "epic_key": "CLID-289"
      },
      "CLID-301": {
        "summary": "cli: add option to change location of the cache",
        "description": "Currently the location of the cache directory can be set via the environment variable `OC_MIRROR_CACHE`. The only problem is that the env var is not easily discoverable by users. It would be better if we had a command line option (e.g `--cache-dir dir`) which is discoverable via `--help`."
      }
    },
    "epics": {
      "CLID-289": {
        "summary": "As a user I would like to mirror the signatures of the container images",
        "description": "-Open Questions:- -Verifying Third-Party Image Signatures: Support verifying the authenticity and integrity of the non-Red Hat (third-party) image signatures using the public keys.- -Question 1: How complex would it be to allow users to specify the location of their public keys in the configuration file or pass them as arguments?- -Question 2: Is it oc-mirror going to copy the certificate/public key as a resource to the cluster resources folder and ask the customer to apply them?- -Question 3: How about certificates?- Catalog images signatures: scenario when we rebuild the catalog Question 1: The signature of the catalog rebuilt is not like the original one since we changed the image completely, how is it going to work? Is the cluster going to fail because the signature is not the one expected? Support the future OCI 1.1 referrer-based approach: Question 1: Is the container image prioritizing this implementation on their side? Do we already have the Jira issue about this implementation?"
      },
      "CLID-302": {
        "summary": "Bundles feature removal",
        "description": "There was a selected bundle feature on v2 that needs to be removed in 4.18 because of the its risk. An alternative solution is required to unblock one of our customers."
      }
    }
  },
  "OpenShift CFE": {
    "stories": {
      "CFE-1167": {
        "summary": "As a developer, I want to add a new field to openshift/api",
        "description": "Provide a new field to the CPMS that allows to define a Machine name prefix",
        "epic_key": "OAPE-16"
      },
      "CFE-1168": {
        "summary": "As a developer, I want to add a new feature gate in openshift/api",
        "description": "Define a new feature gate in openshift/api for this feature so that all the implementation can be safe guarded behind this gate.",
        "epic_key": "OAPE-16"
      }
    }
  },
  "OpenShift Cloud Credential Operator": {
    "stories": {
      "CCO-647": {
        "summary": "Enable readOnlyRootFilesystem on all pods",
        "description": "Enable readOnlyRootFilesystem on all of the cloud-credential-operator pods. This will require reverting prior changes that caused the tls-ca-bundler.pem to be mounted in a temporary location and then moved to the default location as part of the cloud-credential-operator pod's command.",
        "epic_key": "CCO-385"
      },
      "CCO-631": {
        "summary": "Upgrade to Kubernetes 1.32",
        "description": "As a developer, I want to upgrade the Kubernetes dependencies to 1.32 to ensure compatibility with the OpenShift cluster",
        "epic_key": "CCO-627"
      },
      "CCO-629": {
        "summary": "Update vendor dependencies to latest",
        "description": "As a developer, I want to update all go dependencies: to reduce the risk of security vulnerabilities to reduce the risk of incompatibility when handling urgent updates such as CVEs Note: As this is the first time we are doing this, we should provide best-effort here by upgrading the dependencies that have no conflict, and creating cards for the ones that need more effort. Those cards can be linked to the 4.20 Regular Maintenance epic if/when created so we can better plan for the additional effort.",
        "epic_key": "CCO-628"
      },
      "CCO-626": {
        "summary": "Log diff on CredentialsRequest status change",
        "description": "When the CCO updates a CredentialsRequest's status, the current logs are not clear on what's changing: {code:none} time=\"2024-12-05T21:44:49Z\" level=info msg=\"status has changed, updating\" controller=credreq cr=openshift-cloud-credential-operator/aws-ebs-csi-driver-operator secret=openshift-cluster-csi-drivers/ebs-cloud-credentials {code} We should make it possible to get the CCO to log the diff it's trying to push, even if that requires bumping the operator's log level to debug. That would make it easier to understand hotloops like OCPBUGS-47505."
      }
    },
    "epics": {
      "CCO-385": {
        "summary": "readOnlyRootFilesystem should be explicitly to true and if required to false for security reason",
        "description": "_1. Proposed title of this feature request_ openshift-cloud-credential-operator - readOnlyRootFilesystem should be explicitly to true and if required to false for security reason _2. What is the nature and description of the request?_ According to security best practice, it's recommended to set readOnlyRootFilesystem: true for all containers running on kubernetes. Given that openshift-cloud-credential-operator does not set that explicitly, it's requested that this is being evaluated and if possible set to readOnlyRootFilesystem: true or otherwise to readOnlyRootFilesystem: false with a potential explanation why the file-system needs to be write-able. _3. Why does the customer need this? (List the business requirements here)_ Extensive security audits are run on OpenShift Container Platform 4 and are highlighting that many vendor specific container is missing to set readOnlyRootFilesystem: true or else justify why readOnlyRootFilesystem: false is set. _4. List any affected packages or components._ openshift-cloud-credential-operator"
      },
      "CCO-627": {
        "summary": "Upgrade to Kubernetes 1.32",
        "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that CCO uses to v1.32 which keeps it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? To make sure that Hive imports of other OpenShift components do not break when those rebase To avoid breaking other OpenShift components importing from CCO. To pick up upstream improvements Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) Kubernetes 1.32 is released Previous Work (Optional): Similar previous epic CCO-595 Done Checklist CI - CI is running, tests are automated and merged."
      },
      "CCO-628": {
        "summary": "4.19 Regular Maintenance",
        "description": "Epic Goal Update all golang dependencies Ensure periodic CI jobs for new version Sunset periodic CI jobs for version(s) no longer supported Why is this important? To ensure we are using latest vendor code To reduce security vulnerabilities in vendor code To ensure we are regularly testing the latest version To reduce costs from testing old, unsupported versions. Scenarios ... Acceptance Criteria ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Autoscaling": {
    "stories": {
      "AUTOSCALE-193": {
        "summary": "use aws-karpenter-provider-aws image in HCP autonode",
        "description": "Currently, the HCP autonode implementation uses an upstream aws image for karpenter-aws-provider: We need to be able to propagate the current karpenter-provider-aws image associated with the release payload into the HCP autonode karpenter deployment once we lifecycle the image. The hardcoded image \"public.ecr.aws/karpenter/controller:1.0.7\" is at version 1.0.7, while our fork was forked off a few minor versions after that. We also need to resolve any bugs/breaking changes that arise when using the new image with autonode.",
        "epic_key": "AUTOSCALE-36"
      },
      "AUTOSCALE-163": {
        "summary": "The most basic e2e to gate upcoming work",
        "description": "Set up a mininum presubmit gate",
        "epic_key": "AUTOSCALE-28"
      },
      "AUTOSCALE-159": {
        "summary": "Implement HCP karpenter deletion logic",
        "description": "As an OpenShift HCP management cluster admin, when I delete a HCP cluster with autonode on, I would also want to make sure any provisioned nodes are removed from the infrastructure. We need to make sure Karpenter related objects are not blocking the deletion or would result in resource leakage. This story should capture the flow described in and implements cascading deletion login in the HCP Karpenter operator. This should also include e2e test(s) and relevant unit tests which make sure a HCP cluster with autonode on gets successfully deleted.",
        "epic_key": "AUTOSCALE-31"
      },
      "AUTOSCALE-127": {
        "summary": "create release job to exercise e2e tests for karpenter-provider-aws",
        "epic_key": "AUTOSCALE-36"
      },
      "AUTOSCALE-112": {
        "summary": "add test for basic scale out",
        "description": "if possible we might be able to reuse upstream tests, based on decisions from PODAUTO-323.",
        "epic_key": "AUTOSCALE-28"
      },
      "AUTOSCALE-44": {
        "summary": "e2e testing automation: Implement AutoNode upgrades via karpenter drift/consolidation",
        "description": "As a developer I want to make sure the Hypershift hosted cluster upgrades induce karpenter drift and the RHCOS version and Kubernetes version of the node match the versions of the release payload that we were upgrading the hosted cluster to. I also want to make sure the workloads on the old node are moved onto the new node. We've figured out that karpenter drift with hypershift cluster upgrades works already as part of AUTOSCALE-1, now we have to e2e ci test it. Since this is a feature that tests node upgrades inside a guest cluster, we can only test it from the hypershift side. We would probably stick the test here: We need to create a hypershift e2e test (or if possible, integrate it into the existing test) that follows some flow like this: creates a hosted cluster that points to n-1 release version (by e2eutil.NewHypershiftTest) creates and scales a workload that starts a karpenter node and schedules the workload take note of the kubernetes version and the rhcos version of the node upgrades the hosted cluster release image to version n wait until the new nodeclaim and node are created and schedulable make sure the k8s versions and rhcos version match the expected version of release image n wait until the previous workload is scheduled to the new node, fail if it doesn't within a timeout",
        "epic_key": "AUTOSCALE-23"
      }
    },
    "epics": {
      "AUTOSCALE-36": {
        "summary": "Mirror karpenter-aws repo and include it in OCP Payload",
        "description": "Goal Mirror the karpenter aws source code within openshift GH org Agree and automate a rebase cadence Include the image build within the OCP payload. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "AUTOSCALE-28": {
        "summary": "Setup e2e testing for techpreview API and add e2e for autoNode via karpenter",
        "description": "Goal Have a CI pipeline that runs HO with --tech-preview Add e2e test for autoNode via karpenter that validates: all manifest are created as expected management and guest side. Karpenter is able to autoprovision and remove compute. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "AUTOSCALE-31": {
        "summary": "Implement HCP karpenter deletion",
        "description": "Goal The goal of this epic is to support deletion of Karpenter provisioned nodes and their corresponding instances, when a HyperShift HostedCluster is torn down. The end goal is that all infrastructure backed instances are automatically fully removed from their infrastructure when deletion of the HostedCluster is finished. A subgoal includes allowing metrics, alerts, and events to be emitted during teardown. This epic is a part of the strategic feature work for OpenShift AutoNode: Why is this important? This is important because a user will expect all related resources corresponding to a HostedCluster is deleted when it is torn down. We need to specially care for Karpenter instances since they are being provisioned outside of the cluster's environment and being registered with the cluster afterwards. That means we will need to delete them from the infrastructure during teardown, without potentially leaking resources. It is also important that deletion deadlocks are minmized so that users are not stuck during deletion for an excessive amount of time. Additionally, metrics, events, and alerts will allow cluster-admins to diagnose any potential problems related to Karpenter/AutoNode during the tear down phase, and allow them to safely deprovision the cluster. Scenarios A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster. A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster, but the deletion is timed out due to some issue in the deletion process. Acceptance Criteria Dev - Deletion implementation has been merged, and metrics, alerts, events, etc. have been added. Dev - Upstream docs are merged that include document the deletion process, and steps to debug a stuck/failed deletion CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented (created hypershift-hosted cluster with AutoNode on, create some workloads, delete hosted cluster, make sure karpenter provisioned instances are deleted from infrastructure) Release Technical Enablement - Must have TE slides Dependencies (internal and external) None Previous Work (Optional): None Open questions: None for now. Some questions were covered by this spike: Done Checklist CI - CI is running, tests are automated and merged. link to tests in openshift/release Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: N/A"
      },
      "AUTOSCALE-23": {
        "summary": "Implement AutoNode upgrades via karpenter drift/consolidation",
        "description": "Goal Define upgrade criteria for Karpenter Nodes (E.g. follow the control plane, this can be configurable at the HC level so the services can make their choice) Implement it relying on native Drift and Consolidation. Why is this important? Reduce operational burden Scenarios With AutoNode via karpenter the Service is authoritative to manage upgrades of karpenter Nodes. We need to agree on 1..N criteria/strategies. Possibly expose them in the HC API and let them be driven via Drift/Consolidation. Known caveats: in the current prototype everytime the ignition token is rotated would cause drift as a side effect. We'll need to either make it configurable or somehow transparent for drift ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift Authentication": {
    "stories": {
      "AUTH-541": {
        "summary": "Structured authentication configuration for the KAS pods",
        "description": "The CAO and KAS-o both need to work and enable structured authentication configuration for the KAS static pods. CAO: - a controller tracks the auth CR for auth type OIDC - generates structured auth config object and serializes it into a configmap - syncs the configmap into openshift-config KAS-o: - a config observer tracks the auth CR for type OIDC - syncs the auth configmap from openshift-config into openshift-kube-apiserver and enables the `--authentication-config` CLI arg for the KAS pods - the auth-metadata and webhook-authenticator config observers remove their resources and CLI args accordingly - a revision controller syncs that configmap into a static file",
        "epic_key": "CNTRLPLANE-80"
      },
      "AUTH-555": {
        "summary": "Merge kube-rbac-proxy v0.18.2 into downstream",
        "description": "What Merge upstream kube-rbac-proxy v0.18.2 into downstream. Why We don't want to log the tokens even if the verbosity is set to high",
        "epic_key": "CNTRLPLANE-882"
      }
    }
  },
  "Agent-based Installer for OpenShift": {
    "stories": {
      "AGENT-1159": {
        "summary": "Internal docs update",
        "description": "Update the internal installer documentation to reflect the changes on the agent services",
        "epic_key": "AGENT-408"
      },
      "AGENT-1151": {
        "summary": "Use internal appliance registry",
        "description": "Recently the appliance allowed using an internal registry (see Modify the script to use that (instead of the external one), and test the installation workflow.",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1150": {
        "summary": "Move setup-agent-tui.sh into ignition file",
        "description": "Currently the builder script embeds the agent-setup-tui.service in the ignition files, but the script directly in the ISO. For consistency, also the script should be placed inside the ISO ignition",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1137": {
        "summary": "Allow using nightlies/CI release payloads for testing",
        "description": "The current builder script must be able to ingest a nightly/CI release payload for local and CI testing. This also means that the appliance tool should be able to mirror the images from a nightly/CI payload (except maybe for the operators)",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1119": {
        "summary": "Add assisted UI to ove builder script",
        "description": "The ove builder script must include any required assisted UI artifact into the generated ISO",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1118": {
        "summary": "Add agent TUI to ove builder script",
        "description": "The ove builder script should include all the necessary agent TUI artifacts within the generated ISO",
        "epic_key": "AGENT-1086"
      },
      "AGENT-464": {
        "summary": "Expand TUI with a form that asks whether the node should be node0 or not",
        "description": "User Story: As an admin, I want to be able to: Have an interactive generic installation image that I can use for all nodes. Since it is a single image for all the nodes, I need to be able to select on boot whether the node is node0 (and future master) or a regular node. Have the TUI checks take into account whether the node is node0 or not to perform additional checks (like connectivity check to the rendenzvous IP) so that I can achieve Interactive installation with a single image Acceptance Criteria: Description of criteria: A dialog is presented on boot asking whether this node should be the one that controls the installation (node0) On regular nodes additional connectivity checks are performed towards rendezvous IP TUI writes Node0 configuration so the blocked node0 services can proceed (after network configuration and registry checks) Engineering Details: There is a PoC of this dialog in Final dialog in use with ABI: (?) This does not require a design proposal. (?) This does not require a feature gate.",
        "epic_key": "AGENT-387"
      },
      "AGENT-1188": {
        "summary": "Make ISO USB-bootable",
        "description": "Since the ISO containing the entire release image is {_}HUGE{_}, many users will likely want to copy it to a USB drive rather than mounting it as virtualmedia through the BMC. USB mass storage devices require a master boot record to be bootable, unlike optical drives which use the El Torito ISO9660 extension to make them bootable. To turn add an MBR to an El Torito ISO to make it bootable in either mode, we can do: {code:bash} isohybrid --uefi agent.x86_64.iso {code} Based on how CoreOS does this when building the RHCOS live ISO, it's possible that this is only available on x86-64:",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1154": {
        "summary": "Internal documentation",
        "description": "Add a README file in the builder script folder to document internally its usage",
        "epic_key": "AGENT-1086"
      },
      "AGENT-1114": {
        "summary": "TUI checks connectivity to rendezvous node",
        "description": "User Story: As a (user persona), I want to be able to: enter the rendezvous host ip in the agent_tui so that I can achieve have the agent_tui check the current host has connectivity to the rendezvous host an error dialog is displayed if there is no connectivity Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
        "epic_key": "AGENT-387"
      },
      "AGENT-1113": {
        "summary": "Add create interactive-disconnected-ignition sub command to openshift-installer",
        "description": "This sub-command will be used to generate the ignition file based on the interactive disconnected workflow. This command will be invoked by the builder script (currently within the appliance tool) for supporting generating the ISO. It will also consume, in future, the eventual (portion of) install configuration that the user will provide via the connected UI (above the sea level)",
        "epic_key": "AGENT-1086"
      },
      "AGENT-537": {
        "summary": "OpenShift installer on demand generation of just the certificates and credentails for installation",
        "description": "In order to perform interactive installation, we need to expose the OpenShift installer generation of certificates, Kubeadmin password kubeconfig This can either be done by exposing it as some subcommand of openshift-install or exposing the relevant functions in some of the installer golang packages. This needs to be called by a new endpoint in assisted service that REST can call to trigger the generation",
        "epic_key": "AGENT-408"
      },
      "AGENT-467": {
        "summary": "TUI shows WebUI URLs once the assisted service is available",
        "description": "User Story: As an admin, I want to be able to: See the WebUI available URLs (if there are multiple addresses, it could be multiple URLs) in the TUI once all the checks have passed and the backend services are running so that I can achieve Successful connection to the WebUI to continue with the interactive installation Acceptance Criteria: Description of criteria: The WebUI dialog updates to show the URLs in a prominent place for the user to be aware that they can already connect to proceed with interactive installation (the URLs, if multipe, should probably be sorted putting the addresses that have default gateway first) Engineering Details: (?) This does not require a design proposal. (?) This does not require a feature gate.",
        "epic_key": "AGENT-408"
      }
    },
    "epics": {
      "AGENT-408": {
        "summary": "GUI backend services",
        "description": "Epic Goal Have a friendly graphical user to perform interactive installation that runs on node0 Why is this important? Allows the WebUI to run in _Agent based installation_ where we can only count on node0 to run it Provides a familiar (close to SaaS) interface to walk through the first cluster installation Interactive installation takes us closer to having generated images that serve multiple first cluster installations Scenarios As an admin, I want to generate an ISO that I can send to the field to perform a friendly, interactive installation Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) Assisted-Service WebUI needs an _Agent based installation_ wizard Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "AGENT-1086": {
        "summary": "OVE release image generation",
        "description": "Epic Goal Setup a workflow to generate an ISO that will contain all the relevant pieces to install an OVE cluster Why is this important? As per OCPSTRAT-1874, the user must be able to install into a disconnected environment an OVE cluster, with the help of a UI, and without requiring explicitly to setup an external registry Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous work: Dependencies (internal and external) ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "AGENT-387": {
        "summary": "Interactively configure the rendezvous address",
        "description": "Epic Goal Allow the user to select a host to be Node 0 interactively after the booting the ISO. On each host the user would be presented with a choice between two options: Select this host as the rendezvous host (it will become part of the control plane) The IP address of the rendezvous host is: Enter IP (If the former option is selected, the IP address should be displayed so that it can be entered in the other hosts.) Why is this important? Currently, when using DHCP the user must determine which IP address is assigned to at least one of the hosts prior to generating the ISO. (OpenShift requires infinite DHCP leases anyway, so no extra configuration is required but it does mean trying to manually match data with an external system.) AGENT-385 would extend a similar problem to static IPs that the user is planning to configure interactively, since in that case we won't have the network config to infer them from. We should permit the user to delay collecting this information until after the hosts are booted and we can discover it for them. Scenarios In a DHCP network, the user creates the agent ISO without knowing which IP addresses are assigned to the hosts, then selects one to act as the rendezvous host after booting. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) AGENT-7 Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      },
      "AGENT-587": {
        "summary": "Add Nutanix platform integration",
        "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "On Prem Networking": {
    "stories": {
      "OPNET-629": {
        "summary": "Improve HAProxy monitor robustness",
        "description": "This is a followup to to make the monitor resilient in all configurations, including things like 5 node control planes. Instead of relying on a longer fall time, we can just let HAProxy report its own ability to reach any backend, which means under ordinary circumstances this check will never fail. There will no longer be any issue with pathologically bad call chains where we happen to hit backends that are down but haven't been detected yet.",
        "epic_key": "OPNET-579"
      }
    },
    "epics": {
      "OPNET-579": {
        "summary": "Unplanned work for 4.18",
        "description": "Template: Networking Definition of Planned Epic Goal Track work that needs to happen in 4.18 but was not part of the original planning. Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
      }
    }
  },
  "OpenShift BuildConfig": {
    "stories": {
      "OCPBUILD-174": {
        "summary": "Bring openshift/builder Contributor Docs Up to Date",
        "description": "User Story As a developer looking to contribute to OCP BuildConfig I want contribution guidelines that make it easy for me to build and test all the components. Background Much of the contributor documentation for openshift/builder is either extremely out of date or buggy. This hinders the ability for newcomers to contribute. Approach Document dependencies needed to build openshift/builder from source. Update \"dev\" container image for openshift/builder so teams can experiment locally. Provide instructions on how to test \"WIP Pull Request\" process \"Disable operators\" mode. Red Hatter instructions: using cluster-bot Acceptance Criteria New contributors can compile openshift/builder from GitHub instructions New contributors can test their code changes on an OpenShift instance Red Hatters can test their code changes with cluster-bot."
      }
    }
  },
  "OpenShift API Server": {
    "epics": {
      "API-1689": {
        "summary": "Create TLS artifacts registry",
        "description": "In order to keep track of existing certs/CA bundles and ensure that they adhere to requirements we need to have a TLS artifact registry setup. The registry would: have a test which automatically collects existing certs/CA bundles from secrets/configmaps/files on disk have a test which collects necessary metedata from them (from cert contents or annotations) ensure that new certs match expected metadata and have necessary annotations on when a new cert is added Ref: API-1622"
      }
    }
  }
}