{
    "OpenShift Workloads": {
        "epics": {
            "WRKLDS-1449": {
                "summary": "Upgrade to Kubernetes 1.31",
                "description": "Epic Goal Drive the technical part of the Kubernetes 1.31 upgrade, including rebasing openshift/kubernetes repositiry and coordination across OpenShift organization to get e2e tests green for the OCP release. Why is this important? (mandatory) OpenShift 4.18 cannot be released without Kubernetes 1.31 Scenarios (mandatory) Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\" PRs: Retro: Kube 1.31 Rebase Retrospective Timeline (OCP 4.18)| Retro recording:",
                "GITHUB": [
                    {
                        "id": "2141",
                        "type": "pullRequest",
                        "title": "WRKLDS-1449: Update to Kubernetes v1.31.3",
                        "body": "/assign @atiratree /hold should we merge after branch cut"
                    }
                ]
            }
        }
    },
    "Subscription Watch": {
        "stories": {
            "SWATCH-3413": {
                "summary": "Support ACM recording rule in SWATCH",
                "description": "Note once ACM validates recording rule we can utilize it in SWATCH like below. Me and ~lburnett0 are still debating whether to set this query in swatch-metrics application.yaml in rosa, default or acm. For org 5691294 We have provided few options to ACM team on changing the recording rule. Since SWATCH isn't the appropriate place to maintain each product's custom business logic to compute the values of usage to be metered. We may not need to do the following but, we are still ironing out with ACM: {code:java} topk(1, max(acm_capacity_effective_cpu_cores) by (_id) on(_id) group_right min_over_time(ocm_subscription{product=~\"moa-hostedcontrolplane\", external_organization=\"5691294\", support=~\"Premium)) {code} Result {code:java} {_id=\"17277f6d-f760-4949-af1a-1d93983639d5\", account=\"1xwWrms9ndnGhveUGg5iitacj3D\", billing_marketplace=\"aws\", billing_marketplace_account=\"515966534054\", billing_model=\"marketplace\", class=\"Unknown\", cloud_account_id=\"515966534054\", disconnected=\"false\", display_name=\"hcp-poc\", ebs_account=\"1082951\", email_domain=\"rbs.co.uk\", external_organization=\"5691294\", has_csm=\"false\", has_tam=\"true\", instance=\"uhc-acct-mngr-green-metrics.uhc-production.svc:8080\", job=\"uhc-acct-mngr-green-metrics\", managed=\"true\", metered_by_rh=\"true\", namespace=\"uhc-production\", organization=\"1Np3zXRXH2QrF3jaIJ7dIzc8khP\", product=\"moa-hostedcontrolplane\", prometheus=\"openshift-customer-monitoring/ams\", receive=\"true\", rhit_web_user_id=\"51367488\", risk=\"5\", service=\"uhc-acct-mngr-green-metrics\", support=\"Premium\", tenant_id=\"FB870BF3-9F3A-44FF-9BF7-D7A047A52F43\"} Value: 56 {code}",
                "GITHUB": [
                    {
                        "id": "554",
                        "type": "pullRequest",
                        "title": "SWATCH-3413: Add acm_capacity_effective_cpu_cores:sum recording rule",
                        "body": "A recording rule needs to aggregate worker nodes under the hub cluster so swatch can associate the metric with the hub's billing account information. Create a new recording rule that uses the existing one, and adds the \"sum\" logic on top of it."
                    }
                ]
            }
        }
    },
    "OpenShift Storage": {
        "stories": {
            "STOR-2319": {
                "summary": "Remove cns-migration CLI tool",
                "description": "We have decided to remove cns-migration CLI tool for now.",
                "epic_key": "STOR-2301",
                "GITHUB": [
                    {
                        "id": "286",
                        "type": "pullRequest",
                        "title": "STOR-2319: Remove code for CNS migration tool"
                    }
                ]
            },
            "STOR-2285": {
                "summary": "Add e2e for running volume group snapshot tests",
                "description": "We need to make sure that we have e2e tests in Openshift that exercise this feature.",
                "epic_key": "STOR-2265",
                "GITHUB": [
                    {
                        "id": "2254",
                        "type": "pullRequest",
                        "title": "STOR-2285: Update group snapshot test rules",
                        "body": "Update group snapshot test rules to be in sync with openshift-hack/e2e/annotate. We want group snapshot tests disabled only with csi-hostpath test driver. That one needs a special config - a change in the csi-hostpath CSI driver yaml files + a feature gate enabled. The others (namely csi-hostpath-groupsnapshot) should be enabled. That one has the yaml file change + `OCPFeatureGate:VolumeGroupSnapshot`. With this PR, I can see: ``` ./k8s-tests-ext list OCP CSI Volumes Driver: csi-hostpath-groupsnapshot OCPFeatureGate:VolumeGroupSnapshot Testpattern: (delete policy) volumegroupsnapshottable Feature:volumegroupsnapshot VolumeGroupSnapshottable should create snapshots for multiple volumes in a pod Suite:openshift/conformance/parallel Suite:k8s\", ``` Which is what I want. Before this PR, the command returns empty output and no group snapshots are tested."
                    },
                    {
                        "id": "2232",
                        "type": "pullRequest",
                        "title": "STOR-2285: UPSTREAM: carry: Add volume group snapshot test driver",
                        "body": "Upstream enables volume group snapshots by editing yaml files in a shell script 1. We can't use this script in openshift-tests. Create a brand new, OCP specific test driver based on csi-driver-hostpath, only with the --feature-gate=VolumeGroupSnapshot on the external-snapshotter command line. The tests have a tag `OCPFeatureGate:VolumeGroupSnapshot` to run them only in TechPreviewNoUpgrade CI jobs. Also explicitly skip the o/k test in `k8s-e2e-` CI jobs, that job does not interpret `OCPFeatureGate:xyz` We will need to carry this whole patch until the feature graduates to enabled-by-default, hopefully within 4.19 cycle. I've chosen to create brand new files in this carry patch, so it can't conflict with the existing ones in a future rebase. 1:"
                    }
                ]
            },
            "STOR-2263": {
                "summary": "Chore: update csi-driver-smb to the latest release",
                "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "360",
                        "type": "pullRequest",
                        "title": "STOR-2263: correct smb csi driver test manifest",
                        "body": "- Upstream fix - - Correct the smb csi driver test manifest(actually the driver does not support `nodeExpansion `), follow up of ."
                    }
                ]
            },
            "STOR-2257": {
                "summary": "Chore: Update gcp-pd-csi-driver to the latest release",
                "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "75",
                        "type": "pullRequest",
                        "title": "STOR-2257: Rebase to upstream v1.17.4 for OCP 4.19",
                        "body": "Issue link: This rebase also includes fixes for Diff to upstream v1.15.2: Changes between v1.15.2 (OCP 4.18) and v1.17.4 (OCP 4.19): - - - - - - - Full changelogs: @openshift/storage"
                    }
                ]
            },
            "STOR-2253": {
                "summary": "Chore: Update aws-ebs-csi-driver to the latest release",
                "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated) 4.19 special: check make sure it's enabled + tested in OCP",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "283",
                        "type": "pullRequest",
                        "title": "STOR-2253: Rebase to upstream v1.39.0 for OCP 4.19",
                        "body": "Issue link: Diff to upstream v1.39.0: Important changes between v1.34.0 (OCP 4.18) and v1.39.0 (OCP 4.19): - Breaking Metrics Changes Node plugin metrics have been renamed to follow Prometheus best practices: Added aws_ebs_csi_ prefix Added _total suffix for counters Changed time units from microseconds to seconds for all counters The controller plugin metrics now use the prefix aws_ebs_csi_ instead of cloudprovider_aws_. The old metric names will still be emitted, but can be disabled via the CLI parameter --deprecated-metrics=false on the controller. This will default to true in a future version of the EBS CSI Driver. The old metric names (cloudprovider_aws_) are deprecated and will be removed in a future version of the EBS CSI Driver. - ACTION REQUIRED Update to the EBS CSI Driver IAM Policy Due to an upcoming change in handling of IAM polices for the CreateVolume API when creating a volume from an EBS snapshot, a change to your EBS CSI Driver policy may be needed. For more information and remediation steps, see This change affects all versions of the EBS CSI Driver and action may be required even on clusters where the driver is not upgraded. - - - Full changelogs: @openshift/storage"
                    }
                ]
            },
            "STOR-2251": {
                "summary": "Chore: update CSI sidecars",
                "description": "Update all CSI sidecars to the latest upstream release from external-attacher external-provisioner external-resizer external-snapshotter node-driver-registrar livenessprobe Corresponding downstream repos have `csi-` prefix, e.g. github.com/openshift/csi-external-attacher operator assets I.e. copy all snapshot CRDs from upstream| to the operator assets + go get -u github.com/kubernetes-csi/external-snapshotter/client/v6 in the operator repo.",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "83",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase external-attacher to v4.8.1 for OCP 4.19",
                        "body": "Diff to upstream 4.8.1: Changes between 4.7.0 (OCP 4.18) and 4.8.1 (OCP 4.19): - - Full changelogs: @openshift/storage"
                    },
                    {
                        "id": "111",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase external-provisioner to upstream v5.2.0 for OCP 4.19",
                        "body": "Diff to upstream 5.2.0: Changes between 5.1.0 (OCP 4.18) and 5.2.0 (OCP 4.19): - - - - Full changelogs: @openshift/storage"
                    },
                    {
                        "id": "169",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase external-resizer to upstream v1.13.2 for OCP 4.19",
                        "body": "Diff to upstream 1.13.2: Changes between 1.12.0 (OCP 4.18) and 1.13.2 (OCP 4.19): - - - - - Full changelogs: @openshift/storage"
                    },
                    {
                        "id": "177",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase external-snapshotter to v8.2.1 for OCP 4.19",
                        "body": "Diff to upstream 8.2.1: Notable changes between 8.2.0 (OCP 4.18) and 8.2.1 (OCP 4.19): - Full changelogs: @openshift/storage"
                    },
                    {
                        "id": "71",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase livenessprobe to v2.15.0 for OCP 4.19",
                        "body": "Diff to upstream 2.15.0: Changes between 2.14.0 (OCP 4.18) and 2.15.0 (OCP 4.19): - Full changelogs: @openshift/storage"
                    },
                    {
                        "id": "78",
                        "type": "pullRequest",
                        "title": "STOR-2251: Rebase node-driver-registrat to v2.13.0 for OCP 4.19",
                        "body": "Diff to upstream 2.13.0: Changes between 2.12.0 (OCP 4.18) and 2.13.0 (OCP 4.19): - Full changelogs: @openshift/storage"
                    }
                ]
            },
            "STOR-2249": {
                "summary": "Chore: Update ibm-vpc-block-csi-driver to the latest release",
                "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "79",
                        "type": "pullRequest",
                        "title": "STOR-2249: Rebase to upstream 5.2.14 for OCP 4.19",
                        "body": "Diff to upstream v5.2.14: Important changes between 5.2.8 (OCP 4.17) and 5.2.14 (OCP 4.19): - Previous releases are not compatible with updated deployment files associated with the current release due to the changes in csi sidecar livenessprobe securityContext changes. driver/blob/master/deploy/kubernetes/driver/kubernetes/manifests/node-server.yaml E.g. If using manifests from old release lets say v5.2.8 and wants to use the latest code, it is recommended to use the latest manifests - Added support for the cross account snapshot restore. - - - Full changelogs: @openshift/storage"
                    }
                ]
            },
            "STOR-2245": {
                "summary": "CI implementation: OCP 4.19 release chores",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "302",
                        "type": "pullRequest",
                        "title": "STOR-2245: add manifest-topology.yaml for topology related feature and test",
                        "body": "When adding the multi-zone/multi-vcenter CI, some cases will check if there are at least 2 nodes in the same zone and need the topology Capability. The other profile that doesn't have the zonal configuration will still need the previous manifest. (The topology key is null)"
                    }
                ]
            },
            "STOR-2260": {
                "summary": "Early chore: update OCP version in OLM metadata",
                "description": "Update OCP release number in OLM metadata manifests of: local-storage-operator aws-efs-csi-driver-operator gcp-filestore-csi-driver-operator secrets-store-csi-driver-operator smb-csi-driver-operator OLM metadata of the operators are typically in /config/manifest directory of each operator. Example of such a bump: We should do it early in the release, so QE can identify new operator builds easily and they are not mixed with the old release.",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "351",
                        "type": "pullRequest",
                        "title": "STOR-2260: Bump OLM metadata to 4.19"
                    }
                ]
            },
            "STOR-2256": {
                "summary": "Chore: Update azure-disk-csi-driver to the latest release",
                "description": "Update the driver to the latest upstream release. Notify QE and docs with any new features and important bugfixes that need testing or documentation. (Using separate cards for each driver because these updates can be more complicated)",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "99",
                        "type": "pullRequest",
                        "title": "STOR-2256: Rebase v1.314 upstream",
                        "body": "Fixes STOR-2256( Also for unblocking hypershift team. Diff with v1.31.4"
                    }
                ]
            },
            "STOR-2252": {
                "summary": "Chore: update libraries in all operators",
                "description": "Update all OCP and kubernetes libraries in storage operators to the appropriate version for OCP release. Please wait for openshift/api, openshift/library-go, and openshift/client-go are updated to the newest Kubernetes release! There may be non-trivial changes in these libraries. This includes (but is not limited to): Kubernetes: client-go controller-runtime OCP: library-go openshift/api openshift/client-go operator-sdk Operators: csi-operator gcp-filestore-csi-driver-operator ibm-vpc-block-csi-driver-operator cluster-storage-operator local-storage-operator (please cross-check with -operator + vsphere-problem-detector in our tracking sheet and tools/bump-all| may be useful. For 4.16, this was enough: {code:java} mkdir 4.16-bump cd 4.16-bump ../library-bump.py --debug --web file with repo list STOR-1574 --run \"$PWD/../bump-all github.com/google/cel-go@v0.17.7\" --commit-message \"Bump all deps for 4.16\" {code} 4.17 perhaps needs an older prometheus: {code:java} ../library-bump.py --debug --web file with repo list STOR-XXX --run \"$PWD/../bump-all github.com/google/cel-go@v0.17.8 github.com/prometheus/common@v0.44.0 github.com/prometheus/client_golang@v1.16.0 github.com/prometheus/client_model@v0.4.0 github.com/prometheus/procfs@v0.10.1\" --commit-message \"Bump all deps for 4.17\" {code} 4.18 special: Add \"spec.unhealthyEvictionPolicy: AlwaysAllow\" to all PodDisruptionBudget objects of all our operators + operands. See WRKLDS-1490 for details There has been change in library-go function called `WithReplicasHook`. See",
                "epic_key": "STOR-2241",
                "GITHUB": [
                    {
                        "id": "361",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 cc @openshift/storage"
                    },
                    {
                        "id": "228",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 Notes: - Updated calls for `NewControllerCommandConfig`, `NewKubeRecorder`, `NewKubeInformersForNamespaces`"
                    },
                    {
                        "id": "554",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 Notes: - Updated calls for functions with new parameter clock"
                    },
                    {
                        "id": "139",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 cc @openshift/storage"
                    },
                    {
                        "id": "141",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps for 4.19",
                        "body": "Bump all deps for 4.19 Notes: - remove missed replaces cc @openshift/storage"
                    },
                    {
                        "id": "137",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 Notes: - Updated calls for functions with new parameter clock cc @openshift/storage"
                    },
                    {
                        "id": "81",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 Notes: - Updated calls for functions with new parameter clock cc @openshift/storage"
                    },
                    {
                        "id": "296",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 cc @openshift/storage"
                    },
                    {
                        "id": "177",
                        "type": "pullRequest",
                        "title": "STOR-2252: Bump all deps",
                        "body": "Bump all deps for 4.19 Notes: - added implementation of new methods - updated function parameters cc @openshift/storage"
                    }
                ]
            },
            "STOR-2136": {
                "summary": "Move snapshot featuregate to GA",
                "epic_key": "STOR-2120",
                "GITHUB": [
                    {
                        "id": "219",
                        "type": "pullRequest",
                        "title": "STOR-2136: Enable volume groupsnapshots APIs v1beta1 version",
                        "body": "This is meant to be used with which relies on using featuregate for enabling group snapshots."
                    },
                    {
                        "id": "5277",
                        "type": "pullRequest",
                        "title": "STOR-2136: Give delete permissions to snapshot-operator role",
                        "body": "for removing webhook deployment To be used with but this PR can be merged first, so as not to break CI and stuff."
                    },
                    {
                        "id": "29383",
                        "type": "pullRequest",
                        "title": "STOR-2136: Remove CSI snapshot webhook tests",
                        "body": "- We don't need these CSI snapshot webhook tests any more, because we are no longer deploying snapshot webhook stuff in OCP. Ref -"
                    },
                    {
                        "id": "29365",
                        "type": "pullRequest",
                        "title": "STOR-2136: Remove CSI snapshot webhook conditions checks",
                        "body": "We don't need these conditions, because we are no longer deploying snapshot webhook stuff in OCP. See -"
                    }
                ]
            }
        },
        "epics": {
            "STOR-2301": {
                "summary": "Document and support CNS volume migration using vCenter UI",
                "description": "Epic Goal We need to document and support CNS volume migration using native vCenter UI, so as customers can migrate volumes between datastores. Why is this important? (mandatory) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. Scenarios (mandatory) As an vCenter/Openshift admin, I want to migrate CNS volumes between datastores for existing vSphere CSI persistent volumes (PVs). This should cover attached and detached volumes. Special cases such as RWX, zonal or encrypted should also be tested to confirm is there is any limitation we should document. Dependencies (internal and external) (mandatory) This feature depends on VMware vCenter Server 7.0 Update 3o or vCenter Server 8.0 Update 2. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - STOR QE - STOR PX - Others - Acceptance Criteria (optional) This is mostly a testing / documentation epic, which will change current wording about unsupported CNS volume migration using vCenter UI. As part of this epic, we also want to remove the CLI tool we developed for from the payload. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
            },
            "STOR-2265": {
                "summary": "Upstream Beta Tracking: VolumeGroupSnapshot (TP)",
                "description": "Epic Goal Support upstream feature \"VolumeGroupSnapshot\"\" in OCP as -Beta- -GA- Beta, i.e. test it and have docs for it. Why is this important? We get this upstream feature through Kubernetes rebase. We should ensure it works well in OCP and we have docs for it. Upstream links Enhancement issue: KEP: Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) External: the feature is currently scheduled for GA in Kubernetes 1.32, i.e. OCP 4.19. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "STOR-2281": {
                "summary": "Remove volume snapshot webhook",
                "description": "Epic Goal Remove the csi-snapshot-validation-webhook image from OCP and all references to it. Why is this important? (mandatory) Upstream has removed the webhook. We have removed its Deployment from hypershift and standalone OCP| but our CI still builds an image with that name with a fake content. We should remove the image build + remove all references to it. Scenarios (mandatory) As OCP engineering team, we don't build & ship & reference useless images. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) `oc adm release info` does not show csi-snapshot-validation-webhook image + it has been removed from CI builds. Drawbacks or Risk (optional) Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\"",
                "GITHUB": [
                    {
                        "id": "231",
                        "type": "pullRequest",
                        "title": "STOR-2281: Remove csi-snapshot-validation-webhook references",
                        "body": "Remove csi-snapshot-validation-webhook references."
                    },
                    {
                        "id": "175",
                        "type": "pullRequest",
                        "title": "STOR-2281: Remove csi-snapshot-validation-webhook",
                        "body": "Remove csi-snapshot-validation-webhook image no longer used."
                    },
                    {
                        "id": "6004",
                        "type": "pullRequest",
                        "title": "STOR-2281: Remove snapshot webhook",
                        "body": "What this PR does / why we need it: Snapshot webhook is no longer used. We will be removing it for future releases. Which issue(s) this PR fixes: Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    },
                    {
                        "id": "5776",
                        "type": "pullRequest",
                        "title": "STOR-2281: Remove csi-snapshot-validation-webhook",
                        "body": "What this PR does / why we need it: Removed csi-snapshot-validation-webhook. Webhook image is no longer used and will not be built on OCP 4.19+. Which issue(s) this PR fixes Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "STOR-2267": {
                "summary": "Upstream Cycle BETA - SELinux context mounts for RWO/RWX PVs",
                "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Tracking upstream beta promotion of the SELinux context mounts for RWX/RWO PVs Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\"",
                "GITHUB": [
                    {
                        "id": "2212",
                        "type": "pullRequest",
                        "title": "STOR-2267: Add SELinuxMount and SELinuxChangePolicy to DevPreview",
                        "body": "They're Kubernetes feature gates, both alpha in Kubernetes 1.32, adding to DevPreviewNoUpgrade, See Depending on testing results, SELinuxChangePolicy might reach TechPreviewNoUpgrade before 4.19 freeze, a separate PR will follow."
                    },
                    {
                        "id": "834",
                        "type": "pullRequest",
                        "title": "STOR-2267: Run SELinux warning controller",
                        "body": "selinux-warning-controller is an optional controller in KCM that emits metrics + events about SELinux usage of persistent volumes in the cluster. Since most Kubernetes distros don't care about SELinux, this controller needs explicit opt-in on KCM cmdline. OCP needs the controller: - to explain to users why their pods may not be running. - to collect metrics about such promebatic pods, to emit alerts and telemetry. The controller is disabled by default in Kubernetes 1.32 (under SELinuxChangePolicy feature gate), which is available under `DevPreviewNoUpgrade` since yesterday( It might reach TechPreviewNoUpgrade in 4.19 if everything goes smooth. Upstream enhancement: ~WIP: manual testing~ Tested manually with DevPreviewNoUpgrade cluster."
                    }
                ]
            },
            "STOR-2241": {
                "summary": "OCP 4.19 release chores",
                "description": "Epic Goal Update all images that we ship with OpenShift to the latest upstream releases and libraries. Exact content of what needs to be updated will be determined as new images are released upstream, which is not known at the beginning of OCP development work. We don't know what new features will be included and should be tested and documented. Especially new CSI drivers releases may bring new, currently unknown features. We expect that the amount of work will be roughly the same as in the previous releases. Of course, QE or docs can reject an update if it's too close to deadline and/or looks too big. Traditionally we did these updates as bugfixes, because we did them after the feature freeze (FF). Why is this important? We want to ship the latest software that contains new features and bugfixes. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents."
            },
            "STOR-2126": {
                "summary": "StoragereadOnlyRootFilesystem should be explicitly to true and if required to false for security reason",
                "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? According to security best practice, it's recommended to set readOnlyRootFilesystem: true for all containers running on kubernetes. Given that openshift-cluster-storage does not set that explicitly, it's requested that this is being evaluated and if possible set to readOnlyRootFilesystem: true or otherwise to readOnlyRootFilesystem: false with a potential explanation why the file-system needs to be write-able. Applies to openshift-cluster-storage, we should also check other storage operator for the same Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Extensive security audits are run on OpenShift Container Platform 4 and are highlighting that many vendor specific container is missing to set readOnlyRootFilesystem: true or else justify why readOnlyRootFilesystem: false is set. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As an OCP admin I want to ensure that best practice are applied unless there is a valid reason not to do so Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Need to be careful readOnlyRootFilesystem: true doesn't break anything Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\"",
                "GITHUB": [
                    {
                        "id": "370",
                        "type": "pullRequest",
                        "title": "STOR-2126: Enable readOnlyFileSystem",
                        "body": "Enable readOnlyFileSystem in the operator for security concerns. Recommended for all containers running in kubernetes."
                    },
                    {
                        "id": "229",
                        "type": "pullRequest",
                        "title": "STOR-2126: Enable readOnlyFileSystem",
                        "body": "Enable readOnlyFileSystem in the operator for security concerns. Recommended for all containers running in kubernetes."
                    },
                    {
                        "id": "564",
                        "type": "pullRequest",
                        "title": "STOR-2126: Enable readOnlyFileSystem",
                        "body": "Enable readOnlyFileSystem in the operator for security concerns. Recommended for all containers running in kubernetes."
                    }
                ]
            },
            "STOR-2078": {
                "summary": "Upstream Beta Tracking: VolumeAttributesClass (TP)",
                "description": "Epic Goal Support upstream feature \"VolumeAttributesClass\" in OCP as Beta, i.e. test it and have docs for it. Why is this important? We get this upstream feature through Kubernetes rebase. We should ensure it works well in OCP and we have docs for it. Upstream links Enhancement issue: KEP: Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "314",
                        "type": "pullRequest",
                        "title": "STOR-2078: Enable VolumeAttributesClass on AWS EBS for resizer + provisioner",
                        "body": "Add RBAC ClusterRoleBinding for resizer + provisioner"
                    },
                    {
                        "id": "574",
                        "type": "pullRequest",
                        "title": "STOR-2078: Enable VolumeAttributesClass for GCP-PD",
                        "body": "Enabling VolumeAttributesClass for GCP-PD driver and adding needed resources."
                    },
                    {
                        "id": "550",
                        "type": "pullRequest",
                        "title": "STOR-2078: VolumeAttributesClasses typo in resources",
                        "body": "VolumeAttributesClasses typo in resources"
                    },
                    {
                        "id": "549",
                        "type": "pullRequest",
                        "title": "STOR-2078: Typo in clusterRole resources for VolumeAttributesClasses",
                        "body": "Noted: Resources are in plural form."
                    },
                    {
                        "id": "545",
                        "type": "pullRequest",
                        "title": "STOR-2078: Add RBAC rule for Volumeattributesclass"
                    },
                    {
                        "id": "542",
                        "type": "pullRequest",
                        "title": "STOR-2078: Add OPERATOR_IMAGE_VERSION var to AWS-EBS operator",
                        "body": "Adding `OPERATOR_IMAGE_VERSION` variable to AWS EBS operator."
                    }
                ]
            },
            "STOR-1823": {
                "summary": "Remove Shared Resource CSI Driver Feature",
                "description": "Epic Goal Remove the Shared Resource CSI Driver as a tech preview feature. Why is this important? (mandatory) Shared Resources was originally introduced as a tech preview feature in OpenShift Container Platform. After extensive review, we have decided to GA this component through the Builds for OpenShift layered product. Expected GA will be alongside OpenShift 4.16. Therefore it is safe to remove in OpenShift 4.17 Scenarios (mandatory) Accessing RHEL content in builds/workloads Sharing other information across namespaces in the cluster (ex: OpenShift pull secret) Dependencies (internal and external) (mandatory) BUILD-793 Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - OpenShift Storage, OpenShift Builds (forum-openshift-builds) Documentation - QE - PX - Others - Acceptance Criteria (optional) Shared Resource CSI driver cannot be installed using OCP feature gates/tech preview feature set. Drawbacks or Risk (optional) Using Shared Resources requires installation of a layered product, not part of OCP core. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\"",
                "GITHUB": [
                    {
                        "id": "2052",
                        "type": "pullRequest",
                        "title": "STOR-1823: Remove CSIDriverSharedResource feature gate",
                        "body": "/hold for /cc @openshift/storage @adambkaplan @sayan-biswas"
                    }
                ]
            },
            "STOR-2141": {
                "summary": "Make maxAllowedBlockVolumesPerNode configurable (TP)",
                "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? vSphere 8 now allows 255 volumes per VM (i.e OCP workers) and we use the default value of 59 which is safe for vsphere 7 but vsphere 8 customers want to be able to leverage the new improved limit. In order to limit potentials failures, reduce the complexity of that epic and deliver it in time, we are going to limit this feature to homogeneous vsphere 8 environments which only contains ESXi 8 hypervisors. Heterogeneous environment which contain a mix of ESXi 7 & 8 will not be allowed to use this feature. This will be explicitly documented. Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Customers who are using vsphere 8 are stuck with the default limit that applies to vsphere 7. They want to increase the value to benefit from vsphere 8 improvements. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As an OCP admin on top of vsphere 8 i want to increase the maximum number of volumes that can be attached to OCP workers. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. We need a cluster that has pvscsiCtrlr256DiskSupportEnabled set to true Confirm with VMware has support for it and link to official doc Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR Documentation - STOR QE - STOR PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. maxAllowedBlockVolumesPerNode config is changed accordingly Should we reject values greater than 255? Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\"",
                "GITHUB": [
                    {
                        "id": "2190",
                        "type": "pullRequest",
                        "title": "STOR-2141: add MaxAllowedBlockVolumesPerNode field to VSphereCSIDriverConfigSpec"
                    },
                    {
                        "id": "287",
                        "type": "pullRequest",
                        "title": "STOR-2141: add support for maxAllowedBlockVolumesPerNode",
                        "body": "Depends on - openshift/api: - openshift/client-go: Manual verification Test value limits for `maxAllowedBlockVolumesPerNode` field: ``` oc patch clustercsidriver/csi.vsphere.vmware.com --type=merge -p '{\"spec\":{\"driverConfig\":{\"vSphere\":{\"maxAllowedBlockVolumesPerNode\":-1}}}}' The ClusterCSIDriver \"csi.vsphere.vmware.com\" is invalid: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode: Invalid value: -1: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode in body should be greater than or equal to 1 $ oc patch clustercsidriver/csi.vsphere.vmware.com --type=merge -p '{\"spec\":{\"driverConfig\":{\"vSphere\":{\"maxAllowedBlockVolumesPerNode\":0}}}}' The ClusterCSIDriver \"csi.vsphere.vmware.com\" is invalid: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode: Invalid value: 0: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode in body should be greater than or equal to 1 $ oc patch clustercsidriver/csi.vsphere.vmware.com --type=merge -p '{\"spec\":{\"driverConfig\":{\"vSphere\":{\"maxAllowedBlockVolumesPerNode\":256}}}}' The ClusterCSIDriver \"csi.vsphere.vmware.com\" is invalid: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode: Invalid value: 256: spec.driverConfig.vSphere.maxAllowedBlockVolumesPerNode in body should be less than or equal to 255 ``` Validate `maxAllowedBlockVolumesPerNode` value propagation to driver deployment as `MAX_VOLUMES_PER_NODE`: ``` $ oc patch clustercsidriver/csi.vsphere.vmware.com --type=merge -p '{\"spec\":{\"driverConfig\":{\"vSphere\":{\"maxAllowedBlockVolumesPerNode\":60}}}}' clustercsidriver.operator.openshift.io/csi.vsphere.vmware.com patched $ oc -n openshift-cluster-csi-drivers get deployment.apps/vmware-vsphere-csi-driver-controller -o jsonpath='{.spec.template.spec.containers0.env}' {\"name\":\"CSI_ENDPOINT\",\"value\":\"unix:///var/lib/csi/sockets/pluginproxy/csi.sock\"},{\"name\":\"X_CSI_MODE\",\"value\":\"controller\"},{\"name\":\"VSPHERE_CSI_CONFIG\",\"value\":\"/etc/kubernetes/vsphere-csi-config/cloud.conf\"},{\"name\":\"INCLUSTER_CLIENT_QPS\",\"value\":\"100\"},{\"name\":\"INCLUSTER_CLIENT_BURST\",\"value\":\"100\"},{\"name\":\"CSI_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}},{\"name\":\"X_CSI_SERIAL_VOL_ACCESS_TIMEOUT\",\"value\":\"3m\"},{\"name\":\"X_CSI_SPEC_DISABLE_LEN_CHECK\",\"value\":\"true\"},{\"name\":\"MAX_VOLUMES_PER_NODE\",\"value\":\"60\"} $ oc -n openshift-cluster-csi-drivers get daemonset.apps/vmware-vsphere-csi-driver-node -o jsonpath='{.spec.template.spec.containers0.env}' {\"name\":\"NODE_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"spec.nodeName\"}}},{\"name\":\"CSI_ENDPOINT\",\"value\":\"unix:///csi/csi.sock\"},{\"name\":\"X_CSI_MODE\",\"value\":\"node\"},{\"name\":\"X_CSI_SPEC_DISABLE_LEN_CHECK\",\"value\":\"true\"},{\"name\":\"CSI_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}},{\"name\":\"MAX_VOLUMES_PER_NODE\",\"value\":\"60\"} ``` Validate propagation to CSINode as allocatable count: ``` $ oc get csinode/ci-ln-k30mn5t-c1627-2tk2k-worker-0-72mfn -o jsonpath='{.spec.drivers0.allocatable.count}' 60 ``` If `maxAllowedBlockVolumesPerNode` is unset (for example after cluster upgrade) we must use default value (never zero): ``` $ oc get clustercsidriver/csi.vsphere.vmware.com -o jsonpath='{.spec.driverConfig}' {\"driverType\":\"\"} $ oc -n openshift-cluster-csi-drivers get deployment.apps/vmware-vsphere-csi-driver-controller -o jsonpath='{.spec.template.spec.containers0.env?(@.name==\"MAX_VOLUMES_PER_NODE\")}' {\"name\":\"MAX_VOLUMES_PER_NODE\",\"value\":\"59\"} ```"
                    }
                ]
            },
            "STOR-2120": {
                "summary": "Support for VolumeGroup Snapshots (TP)",
                "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Add Volume Group Snapshots as Tech Preview. This is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. We will rely on the newly beta promoted feature. This feature is driver dependent. This will need a new external-snapshotter rebase + removal of the feature gate check in csi-snapshot-controller-operator. Freshly installed or upgraded from older release, will have group snapshot v1beta1 API enabled + enabled support for it in the snapshot-controller (+ ship corresponding external-snapshotter sidecar). No opt-in, no opt-out. OCP itself will not ship any CSI driver that supports it. Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? This is also a key requirement for backup and DR solutions specially for OCP virt. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. External snapshotter rebase to the upstream version that include the beta API. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - STOR / ODF Documentation - STOR QE - STOR / ODF PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Since we don't ship any driver with OCP that support the feature we need to have testing with ODF Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. No risk, behind feature gate Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Basic e2e automationTests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Engineering Stories Merged All associated work items with the Epic are closed Epic status should be \"Release Pending\""
            }
        }
    },
    "OpenShift Container Platform (OCP) Strategy": {
        "description": "For the strategic OpenShift (OCP) work across the product (Outcomes, Features, Initiatives)",
        "features": {
            "OCPSTRAT-1680": {
                "summary": "Migrating CNS volumes between datastores via vSphere UI (GA)",
                "description": "Feature Overview (aka. Goal Summary) Allow customers to migrate CNS volumes (i.e vsphere CSI volumes) from one datastore to another. This operator relies on a new VMware CNS API and requires 8.0.2 or 7.0 Update 3o minimum versions In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. Goals (aka. expected user outcomes) Often our customers are looking to migrate volumes between datastores because they are running out of space in current datastore or want to move to more performant datastore. Previously this was almost impossible or required modifying PV specs by hand to accomplish this. It was also very error prone. As a first version, we developed a CLI tool that is shipped as part of the vsphere CSI operator. We keep this tooling internal for now, support can guide customers on a per request basis. This is to manage current urgent customer's requests, a CLI tool is easier and faster to develop it can also easily be used in previous OCP releases. After multiple discussion with VMware we now have confidence that we can rely on their built-in vSphere UI tool to migrate CNS volume from one datastore to another. This includes attached and detached volumes. Vmware confirmed they have confidence in this scenario and they fully support this operation for attached volumes. Requirements (aka. Acceptance Criteria): SInce the feature is external to OCP, it is mostly a matter of testing it works as expected with OCP but customers will be redirected to Vmware documentation as all the steps are done through the vSphere UI. Perform testing for attached and detached volumes + special cases such as RWX, zonal, encrypted. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YesHosted control planes YesConnected / Restricted Network x86Operator compatibility noUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCP on vsphere only| Use Cases (Optional): As a admin - want to migrate all my PVs or optional PVCs belonging to certain namespace to a different datastore within cluster without potentially requiring extended downtime. I want to move volumes to another datastore that has better performances I want to move volumes to another datastore current the current one is getting full I want to move all volumes to another datastore because the current one is being decommissioned. Questions to Answer (Optional): Get full support confirmation from vmware that their CNS volume migration feature Can be supported for OCP - YES is supported with attached volumes - YES Should detect if a volume is not migreable - YES Out of Scope Limited to what VMware supports. At the moment only one volume can be migrated at a time. Background We had a lot of requests to migrate volumes between datastore for multiple reason. Up until now it was not natively supported by VMware. In 8.0.2 they added a CNS API and a vsphere UI feature to perform volume migration. In 4.17 we shipped a devpreview CLI tool (OCPSTRAT-1619) to cover existing urgent requests. This CLI tool will be removed as soon as this feature is available in OCP. This feature also includes the work needed to remove the CLI tool Customer Considerations Need to be explicit on requirements and limitations. Documentation Considerations Documented as part of the vsphere CSI OCP documentation. Specify min vsphere version. Document any limitation found during testing Redirect to vmware documentation. Announce removal of the CLI tool + update KB. Interoperability Considerations OCP on vSphere only"
            },
            "OCPSTRAT-1921": {
                "summary": "Support for VolumeGroup Snapshots (GA)",
                "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as GA, have docs updated, testing as well as removing feature gate to enable it by default. Requirements (aka. Acceptance Criteria): Tests and CI must pass. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want my customers to benefit from the VolumeGroupSnapshot feature included in my CSI driver. As a backup/DR software vendor I want to use the VolumeGroupSnapshot feature. As a customer I want access to use VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs or use a backup/DR solution that leverages VolumeGroupSnapshot Out of Scope CSI drivers development/support for this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
            },
            "OCPSTRAT-1783": {
                "summary": "vSphere - MachineSet - Support of more than one disk",
                "description": "Goal Support for more than one disk in machineset API for vSphere provider Feature description Customers using vSphere should be able to create machines with more than one disk. This is already available for other cloud and on-prem providers. Why do customers need this? To have Proper disk layout that better address their needs. Some examples are using the local storage operator or ODF. Affected packages or components RHCOS, Machine API, Cluster Infrastructure, CAPV."
            },
            "OCPSTRAT-1577": {
                "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
                "description": "Feature Overview Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
            },
            "OCPSTRAT-1823": {
                "summary": "GA 'oc adm upgrade status' command (and optionally status API)",
                "description": "As a customer of self managed OpenShift or an SRE managing a fleet of OpenShift clusters I should be able to determine the progress and state of an OCP upgrade and only be alerted if the cluster is unable to progress. Support a cli-status command and status-API which can be used by cluster-admin to monitor the progress. status command/API should also contain data to alert users about potential issues which can make the updates problematic. Feature Overview (aka. Goal Summary) {color:676767}_Show nodes where pod draining is taking more time._ _Customers have to dig deeper often to find the nodes for further debugging._ _The ask has been to bubble up this on the update progress window._{color} {color:676767}_oc update status ?_ _From the UI we can see the progress of the update. From oc cli we can see this from \"oc get cvo\"_ _But the ask is to show more details in a human-readable format._{color} _Know where the update has stopped. Consider adding at what run level it has stopped._ {code:java} oc get clusterversion NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.12.0 True True 16s Working towards 4.12.4: 9 of 829 done (1% complete) {code} Documentation Considerations {color:676767}_Update docs for UX and CLI changes_ _Reference :"
            },
            "OCPSTRAT-1711": {
                "summary": "Tech Preview OLM v1: Manage operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes",
                "description": "Feature Overview (aka. Goal Summary) OLM v1 effectively manages operators packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Goals (aka. expected user outcomes) Users can rely on OLM v1 to manage operators packaged in registry+v1 bundle format, including those with OwnNamespace and SingleNamespace installModes. Operator authors can rely on OLM v1 to propagate the specified targetNamespaces to the operator deployment manifest during installation as the WATCH_NAMESPACE env vars, ensuring that the operator is scoped to the correct namespace without modifications Background Our Telco customers and ISV partners are eager to leverage OLM v1's ability to declare specific Operator versions for managed clusters using GitOps/ZTP workflows. By defining Operator versions directly in Git repositories, customers can ensure that only compatible versions are deployed to specific configurations. This GitOps process streamlines initial deployment and subsequent updates, ensuring alignment between Operator versions and managed cluster configurations. While the initial GA release of OLM v1 may have limitations, our goal is to support a broader range of operators, including those packaged in registry+v1 bundles with OwnNamespace and SingleNamespace installModes. This will enable us to meet the evolving needs of our Telco customers and protect our existing investments. By preserving compatibility with the current operator landscape, we can facilitate a smoother transition to the OLM v1. This not only secures existing workloads but also opens up new opportunities for growth within the OpenShift business. Requirements (aka. Acceptance Criteria) TargetNamespace propagation: OLM v1 can handle situations where the targetNamespace differs from or equals to the installNamespace when installing a registry+v1 bundle. OLM v1 can propagate the specified targetNamespace to the operator deployment manifest, ensuring correct namespace scoping for registry+v1 bundles. RBAC enforcement: OLM v1 enforces RBAC permissions based on the targetNamespace to prevent unauthorized access to cluster-wide resources Error handling and troubleshooting: OLM v1 provides warning and error logs to help users troubleshoot potential issues related to installing registry+v1 bundles with OwnNamespace and SingleNamespace installModes. Customer Considerations Telco customers. Documentation Considerations A step-by-step guide on configuring and managing registry+v1 bundles with OwnNamespace and SingleNamespace install modes in OLM v1. Interoperability Considerations Existing Red Hat and certified operators packaged in registry+v1 bundles support OwnNamespace and SingleNamespace installModes."
            },
            "OCPSTRAT-1583": {
                "summary": "Tech Preview OLM v1: Create a ServiceAccount with necessary permissions for managing cluster content lifecycle",
                "description": "Feature Overview (aka. Goal Summary) OLM v1 assists users in creating required ServiceAccounts with necessary permissions for managing cluster content lifecycle. Goals (aka. expected user outcomes) - Users can easily preview the required permissions before installing or upgrading an extension/operator. - Users can create a ServiceAccount with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Background By default, OLM v1 requires users to provide a service account for installing, upgrading, and deleting cluster content. This aligns with the least privilege principle, as OLM v1's default service account is limited to granted permissions and cannot easily perform actions on behalf of users with lower privileges. However, this requires cluster administrators or users with sufficient permissions to create a service account capable of creating, modifying, and deleting Kubernetes resources like Deployments, Services, and ConfigMaps, as needed by the extension/operator packages. To simplify this process, OLM v1 aims to assist users in determining and creating service accounts with appropriate permissions to manage cluster content. Requirements (aka. Acceptance Criteria) - Required permissions analysis: OLM v1 analyzes an extension/operator bundle to determine the required permissions for its lifecycle management. -- The required permissions include CRUD (Create, Read, Update, Delete) operations on all Kubernetes objects within the extension/operator bundle, as well as any permissions granted by included RBAC resources (if any) and resource dependencies. - Required permissions preview: OLM v1 provides a preview of required permissions for installing or upgrading operators/extensions to users. - Roles and RoleBindings creation: OLM v1 assists users in generating Role and RoleBinding objects based on the determined permissions, adhering to security best practices and least privilege principles. - ServiceAccount creation: OLM v1 assists users in creating a ServiceAccount and associate it with the generated RoleBindings with appropriate permissions for installing or upgrading extensions/operators. -- Provides an option for customizing the ServiceAccount name. - User Interaction: OLM v1 offers guidance and options for users to review and modify the generated Role/RoleBinding/ServiceAccount before creation. -- Provides an option for specifying custom permissions if needed. -- Handles errors during Role/RoleBinding/ServiceAccount creation with retry. -- Provides error messages for troubleshooting. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Open Questions: - How does this work for the contents packaged in Helm charts? -- Helm does not strictly require users to provide a ServiceAccount to create Kubernetes objects but can rely on the context in the Kubeconfig or the service account token mounted in the pod. Should we follow that pattern as one of the options to streamline the UX? Out of Scope __ your text here Documentation Considerations __ - The steps for previewing the required permissions before installing or upgrading an extension/operator. - The steps for creating a ServiceAccount and those associated Roles/Rolebindings with OLM v1's guidance, ensuring it has the necessary permissions for installing or upgrading extensions/operators. Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1327": {
                "summary": "Tech Preview (Phase 1) Next-gen OLM UX: Unifying workload management in the console",
                "description": "Feature Overview (aka. Goal Summary) : This ticket introduces the initial Tech Preview release of the next-generation OLM (OLM v1) user experience in the console. : This ticket focuses on enabling a unified catalog UX in the console. This will allow customers to manage layered capabilities delivered through operators and partners' workloads, including OpenShift certified Helm charts, using the next-generation OLM (OLM v1) in the OpenShift console. : This is enabled through the novel in-cluster efficient catalog content service designed in OCPSTRAT-1655 and delivered in the 4.18 timeframe. Goals (aka. expected user outcomes) In essence, customers can: discover collections of k8s extension/operator contents released in the FBC format with richer visibility into their release channels, versions, update graphs, and the deprecation information (if any) to make informed decisions about installation and/or update them. install a k8s extension/operator declaratively and potentially automate with GitOps to ensure predictable and reliable deployments. update a k8s extension/operator to a desired target version or keep it updated within a specific version range for security fixes without breaking changes. remove a k8s extension/operator declaratively and entirely including cleaning up its CRDs and other relevant on-cluster resources (with a way to opt out of this coming up in a later release). Requirements (aka. Acceptance Criteria): 1) Pre-installation: Both cluster-admins or non-privileged end-users can explore and discover the layered capabilities or workloads delivered by k8s extensions/operators or plain helm charts from a unified ecosystem catalog UI in the \u2018Administrator Perspective\u2019 in the console. Users can filter the available offerings based on the delivery mechanism/source type (i.e., operator-backed or plain helm charts), providers (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Users can discover all versions in all channels that an offering/package defines in a catalog, select a version from a channel, and see its detailed description, provided APIs, and other metadata before the installation. 2) Installation: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) using a ServiceAccount with sufficient permissions can install a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users can see the recommended installation namespace if provided by the package authors for installation. Users get notified through error messages from the OLM API whenever two conflicting k8s extensions/operators (will be) owning the same API objects, i.e., no conflicting ownership, after triggering the installation. During the installation, users can see the installation progress reported from the \u2018ClusterExtension\u2019 API object. After installed, users (who have access to OLM v1\u2019s user-facing \u2018ClusterExtension\u2019 API) can see can access the metadata of the installed k8s extension/operator to see essential information such as its provided APIs, example YAMLs of its provided APIs, descriptions, infrastructure features, valid subscriptions, etc. 3) Update: Users (who have access to OLM v1\u2019s user facing \u2018ClusterExtension\u2019 API) can see what updates are available for their k8s extension/operators in the form of immediate target versions and the associated update channels. Users can trigger the update of a k8s extension/operator with a desired target version or the latest version within a specific version range (from the associated channel) to get the latest security fixes. Users get notified through error messages whenever a k8s extension/operator is prevented from updating to a newer version that has a backward incompatible CustomResourceDefinition (CRD) that will cause workload or k8s extension/operator breakage. During OpenShift cluster update, users get Informed when installed k8s extensions/operators do not support the next OpenShift version (when annotated by the package author/provider). Customers must update those k8s extensions/operators to a newer/compatible version before OLM unblocks the OpenShift cluster update. During the update, users can see the progress reported from the \u2018ClusterExtension\u2019 API object. 4) Uninstallation/Deletion: Users are made aware of OLM v1 by default cleanly remove an installed k8s extension/operator including deleting CustomResourceDefinitions (CRDs), custom resource objects (CRs) of the CRDs, and other relevant resources to revert the cluster to its original state before the installation. Users can see a list of resources that are relevant to the installed k8s extension/operator they are about to remove and then explicitly confirm the deletion. Questions to Answer (Optional): What impact will the console's \"perspective consolidation\" initiative have on this? Out of Scope __ your text here Background Our customers will experience a streamlined approach to managing layered capabilities and workloads delivered through operators, operators packaged in Helm charts, or even plain Helm charts. The next generation OLM will power this central distribution mechanism within the OpenShift in the future. Customers will be able to explore and discover the layered capabilities or workloads, and then install those offerings and make them available on their OpenShift clusters. Similar to the experience with the current OperatorHub, customers will be able to sort and filter the available offerings based on the delivery mechanism (i.e., operator-backed or plain helm charts), source type (i.e., from Red Hat or ISVs), valid subscriptions, infrastructure features, etc. Once click on a specific offering, they see the details which include the description, usage, and requirements of the offering, the provided services in APIs, and the rest of the relevant metadata for making the decisions. The next-gen OLM aims to unify workload management. This includes operators packaged for current OLM, operators packaged in Helm charts, and even plain Helm charts for workloads. We want to leverage the current support for managing plain Helm charts within OpenShift and the console for leveraging our investment over the years. Documentation Considerations Refer to the \"Documentation Considerations\" section of the OLM v1 GA feature. Relevant documents Next-gen OLM UX: Unifying workload management Operator Framework F2F - PM Session OLM F2F Discussion - Roadmap"
            },
            "OCPSTRAT-1684": {
                "summary": "Add all Dev only UI pages to the Admin Perspective",
                "description": "Feature Overview (aka. Goal Summary) __ your text here Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1973": {
                "summary": "BYOPKI for image verification in OCP - TP in 4.20",
                "description": "Feature Overview (aka. Goal Summary) Tech P : OCP 4.20 BYOPKI for image verification in OCP"
            },
            "OCPSTRAT-683": {
                "summary": "Migrate MAPI to Cluster API for AWS (TP) - Phase 1",
                "description": "Feature Overview (aka. Goal Summary) Implement Migration core for MAPI to CAPI for AWS This feature covers the design and implementation of converting from using the Machine API (MAPI) to Cluster API (CAPI) for AWS This Design investigates possible solutions for AWS Once AWS shim/sync layer is implemented use the architecture for other clouds in phase-2 & phase 3 Acceptance Criteria When customers use CAPI, There must be no negative effect to switching over to using CAPI . Seamless migration of Machine resources. the fields in MAPI/CAPI should reconcile from both CRDs."
            },
            "OCPSTRAT-330": {
                "summary": "Upstream OpenShift AutoScaler TechDebt (Phase 3)",
                "description": "Feature Overview This is a TechDebt and doesn't impact OpenShift Users. As the autoscaler has become a key feature of OpenShift, there is the requirement to continue to expand it's use bringing all the features to all the cloud platforms and contributing to the community upstream. This feature is to track the initiatives associated with the Autoscaler in OpenShift. Goals Scale from zero available on all cloud providers (where available) Required upstream work Work needed as a result of rebase to new kubernetes version Requirements RequirementNotesisMvp? Out of Scope n/a Background, and strategic fit Autoscaling is a key benefit of the Machine API and should be made available on all providers Assumptions Customer Considerations Documentation Considerations Target audience: cluster admins Updated content: update docs to mention any change to where the features are available."
            },
            "OCPSTRAT-680": {
                "summary": "Integrate Cluster API in standalone OCP-Phase 2",
                "description": "Feature Overview (aka. Goal Summary) Phase 2 Goal: Complete the design of the Cluster API (CAPI) architecture and build the core operator logic attach and detach of load balancers for internal and external load balancers for control plane machines on AWS, Azure, GCP and other relevant platforms manage the lifecycle of Cluster API components within OpenShift standalone clusters E2E tests for Phase-1, incorporating the assets from different repositories to simplify asset management. Background, and strategic fit Overarching Goal Move to using the upstream Cluster API (CAPI) in place of the current implementation of the Machine API for standalone Openshift. Phase 1 & 2 covers implementing base functionality for CAPI. Phase 2 also covers migrating MAPI resources to CAPI. Initially CAPI did not meet the requirements for cluster/machine management that OCP had the project has moved on, and CAPI is a better fit now and also has better community involvement. CAPI has much better community interaction than MAPI. Other projects are considering using CAPI and it would be cleaner to have one solution Long term it will allow us to add new features more easily in one place vs. doing this in multiple places. Acceptance Criteria There must be no negative effect to customers/users of the MAPI, this API must continue to be accessible to them though how it is implemented \"under the covers\" and if that implementation leverages CAPI is open"
            },
            "OCPSTRAT-2071": {
                "summary": "GA Allow Custom machine names when using the CPMS feature",
                "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
            },
            "OCPSTRAT-172": {
                "summary": "GA Cert-manager support router to load secrets",
                "description": "Epic Goal Review design and development PRs that require feedback from NE team. Why is this important? Customer requires certificates to be managed by cert-manager on configured/newly added routes. Acceptance Criteria All PRs are reviewed and merged. Dependencies (internal and external) CFE team dependency for addressing review suggestions. Done Checklist DEV - All related PRs are merged."
            },
            "OCPSTRAT-1418": {
                "summary": "Tech Preview Allow Custom machine names when using the CPMS feature",
                "description": "Feature Overview As a cluster admin for standalone OpenShift, I want to customize the prefix of the machine names created by CPMS due to company policies related to nomenclature. Implement the Control Plane Machine Set (CPMS) feature in OpenShift to support machine names where user can set custom names prefixes. Note the prefix will always be suffixed by \"5-chars-index\" as this is part of the CPMS internal design. Acceptance Criteria A new field called machineNamePrefix has been added to CPMS CR. This field would allow the customer to specify a custom prefix for the machine names. The machine names would then be generated using the format: machineNamePrefix{-}5-chars{-}index Where: machineNamePrefix is the custom prefix provided by the customer 5-chars is a random 5 character string (this is required and cannot be changed) index represents the index of the machine (0, 1, 2, etc.) Ensure that if the machineNamePrefix is changed, the operator reconciles and succeeds in rolling out the changes."
            },
            "OCPSTRAT-134": {
                "summary": "Gateway API using Istio for Cluster Ingress - GA",
                "description": "Goal: Graduate to GA (full support) Gateway API with Istio to unify the management of cluster ingress with a common, open, expressive, and extensible API. Description: Gateway API is the evolution of upstream Kubernetes Ingress APIs. The upstream project is part of Kubernetes, working under SIG-NETWORK. OpenShift is contributing to the development, building a leadership position, and preparing OpenShift to support Gateway API, with Istio as our supported implementation. The plug-able nature of the implementation of Gateway API enables support for additional and optional 3rd-party Ingress technologies."
            },
            "OCPSTRAT-525": {
                "summary": "Enable HAProxy Dynamic Configuration Manager for OpenShift - Tech Preview",
                "description": "We need to do a lot of R&D and fix some known issues (e.g., see linked BZs). R&D targetted at 4.16 and productisation of this feature in 4.17"
            },
            "OCPSTRAT-1845": {
                "summary": "GADisconnected Cluster Update and Boot without local image registry - phase 2",
                "description": "Feature Overview Note: This feature will be a TechPreview in 4.16 since the newly introduced API must graduate to v1. Overarching Goal Customers should be able to update and boot a cluster without a container registry in disconnected environments. This feature is for Baremetal disconnected cluster. Background For a single node cluster effectively cut off from all other networking, update the cluster despite the lack of access to image registries, local or remote. For multi-node clusters that could have a complete power outage, recover smoothly from that kind of disruption, despite the lack of access to image registries, local or remote. Allow cluster node(s) to boot without any access to a registry in case all the required images are pinned"
            },
            "OCPSTRAT-1945": {
                "summary": "Updated boot images: Phase 4 (GCP, AWS to opt-out)",
                "description": "Feature Overview OCP 4 clusters still maintain pinned boot images. We have numerous clusters installed that have boot media pinned to first boot images as early as 4.1. In the future these boot images may not be certified by the OEM and may fail to boot on updated datacenter or cloud hardware platforms. These \"pinned\" boot images should be updateable so that customers can avoid this problem and better still scale out nodes with boot media that matches the running cluster version. In phase 1 provided tech preview for GCP. In phase 2, GCP support goes to GA and AWS goes to TP. In phase 3, AWS support goes to GA . In phase 4, AWS and GCP goes to opt-out. Requirements"
            },
            "OCPSTRAT-1474": {
                "summary": "Exploitation of hardware based root volume LUKS encryption (IBM Z)",
                "description": "Feature Overview (aka. Goal Summary) As LUKS encryption is required for certain customer environments e.g. being PCI compliant and the current implementation with Network Based LUKS encryption are a) complex and b) not reliable and secure we need to support our Customers with an way to have the Root Device encrypted on a secure way with IBM HW based HSM to secure the LUKS Key. This is a kind of TPM approach to store the luks key but fence it from the user. Hardware based LUKS encryption requires injection of the read of secure keys in clevis during boot time. Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both YHosted control planes YConnected / Restricted Network IBM ZOperator compatibility n/aUI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1938": {
                "summary": "On Cluster Layering: Parity",
                "description": "Feature Overview (aka. Goal Summary) The original release of on cluster layering (OCL informally) came with known limitations and parity gaps. We need to close those gaps so everyone can have \"Image mode on OpenShift\". Goals (aka. expected user outcomes) OCL should work for as many OCP deployment patterns and footprints as possible. Ideally everywhere you can run OpenShift. Requirements (aka. Acceptance Criteria): Priority platform issues: Disconnected Single Node Two Node Hosted Control Planes Multi-arch (homogeneous arch cluster) Multi-arch (heterogeneous arch cluster) Priority feature issues: Node disruption policy Extensions"
            },
            "OCPSTRAT-1389": {
                "summary": "On Cluster Layering: Phase 3 (GA)",
                "description": "Feature Overview This is Image mode on OpenShift. It uses the rpm-ostree native containers interface and not bootc but that is an implementation detail. In the initial delivery of CoreOS Layering, it is required that administrators provide their own build environment to customize RHCOS images. That could be a traditional RHEL environment or potentially an enterprising administrator with some knowledge of OCP Builds could set theirs up on-cluster. The primary virtue of an on-cluster build path is to continue using the cluster to manage the cluster. No external dependency, batteries-included. On-cluster, automated RHCOS Layering builds are important for multiple reasons: One-click/one-command upgrades of OCP are very popular. Many customers may want to make one or just a few customizations but also want to keep that simplified upgrade experience. Customers who only need to customize RHCOS temporarily (hotfix, driver test package, etc) will find off-cluster builds to be too much friction for one driver. One of OCP's virtues is that the platform and OS are developed, tested, and versioned together. Off-cluster building breaks that connection and leaves it up to the user to keep the OS up-to-date with the platform containers. We must make it easy for customers to add what they need and keep the OS image matched to the platform containers. Goals & Requirements The goal of this feature is primarily to bring the 4.14 progress (OCPSTRAT-35) to a Tech Preview or GA level of support. Customers should be able to specify a Containerfile with their customizations and \"forget it\" as long as the automated builds succeed. If they fail, the admin should be alerted and pointed to the logs from the failed build. The admin should then be able to correct the build and resume the upgrade. Intersect with the Custom Boot Images such that a required custom software component can be present on every boot of every node throughout the installation process including the bootstrap node sequence (example: out-of-box storage driver needed for root disk). Users can return a pool to an unmodified image easily. RHEL entitlements should be wired in or at least simple to set up (once). Parity with current features - including the current drain/reboot suppression list, CoreOS Extensions, and config drift monitoring."
            },
            "OCPSTRAT-943": {
                "summary": "Dev Preview AutoNode (Native Karpenter) with HCP",
                "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP -OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift) as APIs for Managed Services (ROSA) to consume. It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 (out of scope for Dev Preview) _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade (TBD for Dev Preview but potentially out of scope for Dev Preview, i.e. may slip to Tech Preview) Expose Karpenter metrics to Cluster Admin (out of scope for Dev Preview, Targeting Tech Preview) Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1657": {
                "summary": "Add a Mechanism to Label all Pods for a Hosted Cluster in the Control Plane Namespace",
                "description": "Background As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. Goal Implement Mechanism for Labeling Hosted Cluster Control Plane Pods Use-cases - Adherance to Microsoft 1p Resource Provider Requirements Components - Any pods that hypershift deploys or run in the hosted cluster namespace."
            },
            "OCPSTRAT-1916": {
                "summary": "Azure - Remove not required permissions from the Nodes",
                "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria) _The Installer only creates the minimum permissions required to deploy OpenShift on Azure_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background Customer Considerations _A KCS will be created for customers running previous OpenShift releases who want to remove this resource_ Documentation Considerations"
            },
            "OCPSTRAT-561": {
                "summary": "Support Private Google Access to GCP endpoints",
                "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
            },
            "OCPSTRAT-569": {
                "summary": "AWS - Allocate Load Balancers (API & Ingress) to Specific Subnets",
                "description": "Add ability to choose subnet while creating ingresscontroller of type LoadBalancerService. Checking ingresscontroller CRD could see that there is no such way to set subnet of load balancer. Why is this important? Currently, when deploying an IngressController instance , all the FrontendIPs will be in the same subnet. However, the LoadBalancer Service implementation allows specifying the target subnet through service annotation. Therefore the need to introduce an additional field to the ingresscontroller CRD, that allows to specify the target subnet. The value of this field is then used to annotate the created LoadBalancer Service from the beginning on, so the ingress controller immediately gets its FrontendIP into the right subnet. Scenarios If the cluster is spread across multiple subnets then its good to have a way to set subnet while creating ingresscontroller of type LoadBalancerService."
            },
            "OCPSTRAT-1005": {
                "summary": "Remove Terraform from the Azure Stack Hub IPI installer",
                "description": "Feature Overview (aka. Goal Summary) As a result of Hashicorp's license change to BSL, Red Hat OpenShift needs to remove the use of Hashicorp's Terraform from the installer - specifically for IPI deployments which currently use Terraform for setting up the infrastructure. To avoid an increased support overhead once the license changes at the end of the year, we want to provision Azure Stack Hub infrastructure without the use of Terraform. Requirements (aka. Acceptance Criteria): _The Azure Stack Hub IPI Installer no longer contains or uses Terraform._ _The new provider should aim to provide the same results and have parity with the existing Azure Stack Hub Terraform provider. Specifically,_ we should aim for feature parity against the install config and the cluster it creates to minimize impact on existing customers' UX. Use Cases (Optional): __ Questions to Answer (Optional): __ Out of Scope __ Background __ Customer Considerations __ Documentation Considerations __ Interoperability Considerations __"
            },
            "OCPSTRAT-1360": {
                "summary": "Remove ARO build-flag in openshift-installer (ARO fork removal - Phase II - Part 1)",
                "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1097": {
                "summary": "Add support to enable boot diagnostics option at installation time in Azure (ARO fork removal - Phase II - Part 2)",
                "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ Documentation Considerations Interoperability Considerations"
            },
            "OCPSTRAT-1665": {
                "summary": "CAPI-based Installer technical debt",
                "description": "Feature Overview (aka. Goal Summary) _Review, refine and harden the CAPI-based Installer implementation introduced in 4.16_ Goals (aka. expected user outcomes) _From the implementation of the CAPI-based Installer started with OpenShift 4.16 there is some technical debt that needs to be reviewed and addressed to refine and harden this new installation architecture._ Requirements (aka. Acceptance Criteria): _Review existing implementation, refine as required and harden as possible to remove all the existing technical debt_ __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Documentation Considerations _There should not be any user-facing documentation required for this work_"
            },
            "OCPSTRAT-1361": {
                "summary": "BGP for UDN GA On-prem",
                "description": "Feature Overview (aka. Goal Summary) OVN Kubernetes BGP support as a routing protocol for User Defined Network (Segmentation) pod and VM addressability. Goals (aka. expected user outcomes) OVN-Kubernetes BGP support enables the capability of dynamically exposing cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. OVN-Kubernetes currently has no native routing protocol integration, and relies on a Geneve overlay for east/west traffic, as well as third party operators to handle external network integration into the cluster. This enhancement adds support for BGP as a supported routing protocol with OVN-Kubernetes. The extent of this support will allow OVN-Kubernetes to integrate into different BGP user environments, enabling it to dynamically expose cluster scoped network entities into a provider\u2019s network, as well as program BGP learned routes from the provider\u2019s network into OVN. In a follow-on release, this enhancement will provide support for EVPN, which is a common data center networking fabric that relies on BGP via OCPSTRAT-1744 Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Use Cases (Optional): Integration with 3rdparty load balancers that send packets directly to OpenShift nodes with the destination IP address of a targeted pod, without needing custom operators to detect which node a pod is scheduled to and then add routes into the load balancer to send the packet to the right node. Questions to Answer (Optional): Out of Scope Support of any other routing protocol Running separate BGP instances per VRF network Support for any other type of L3VPN with BGP, including MPLS Providing any type of API or operator to automatically connect two Kubernetes clusters via L3VPN Replacing the support that MetalLB provides today for advertising service IPs Asymmetric Integrated Routing and Bridging (IRB) with EVPN Background BGP Importing Routes from the Provider Network Today in OpenShift there is no API for a user to be able to configure routes into OVN. In order for a user to change how cluster traffic is routed egress into the cluster, the user leverages local gateway mode, which forces egress traffic to hop through the Linux host's networking stack, where a user can configure routes inside of the host via NM State. This manual configuration would need to be performed and maintained across nodes and VRFs within each node. Additionally, if a user chooses to not manage routes within the host and use local gateway mode, then by default traffic is always sent to the default gateway. The only other way to affect egress routing is by using the Multiple External Gateways (MEG) feature. With this feature the user may choose to have multiple different egress gateways per namespace to send traffic to. As an alternative, configuring BGP peers and which route-targets to import would eliminate the need to manually configure routes in the host, and would allow dynamic routing updates based on changes in the provider\u2019s network. Exporting Routes into the Provider Network There exists a need for provider networks to learn routes directly to services and pods today in Kubernetes. Metal LB is already one solution whereby load balancer IPs are advertised by BGP to provider networks, and this feature development does not intend to duplicate or replace the function of Metal LB. Metal LB should be able to interoperate with OVN-Kubernetes, and be responsible for advertising services to a provider\u2019s network. However, there is an alternative need to advertise pod IPs on the provider network. One use case is integration with 3rd party load balancers, where they terminate a load balancer and then send packets directly to OCP nodes with the destination IP address being the pod IP itself. Today these load balancers rely on custom operators to detect which node a pod is scheduled to and then add routes into its load balancer to send the packet to the right node. By integrating BGP and advertising the pod subnets/addresses directly on the provider network, load balancers and other entities on the network would be able to reach the pod IPs directly. EVPN Extending OVN-Kubernetes VRFs into the Provider Network This is the most powerful motivation for bringing support of EVPN into OVN-Kubernetes. A previous development effort enabled the ability to create a network per namespace (VRF) in OVN-Kubernetes, allowing users to create multiple isolated networks for namespaces of pods. However, the VRFs terminate at node egress, and routes are leaked from the default VRF so that traffic is able to route out of the OCP node. With EVPN, we can now extend the VRFs into the provider network using a VPN. This unlocks the ability to have L3VPNs that extend across the provider networks. Utilizing the EVPN Fabric as the Overlay for OVN-Kubernetes In addition to extending VRFs to the outside world for ingress and egress, we can also leverage EVPN to handle extending VRFs into the fabric for east/west traffic. This is useful in EVPN DC deployments where EVPN is already being used in the TOR network, and there is no need to use a Geneve overlay. In this use case, both layer 2 (MAC-VRFs) and layer 3 (IP-VRFs) can be advertised directly to the EVPN fabric. One advantage of doing this is that with Layer 2 networks, broadcast, unknown-unicast and multicast (BUM) traffic is suppressed across the EVPN fabric. Therefore the flooding domain in L2 networks for this type of traffic is limited to the node. Multi-homing, Link Redundancy, Fast Convergence Extending the EVPN fabric to OCP nodes brings other added benefits that are not present in OCP natively today. In this design there are at least 2 physical NICs and links leaving the OCP node to the EVPN leaves. This provides link redundancy, and when coupled with BFD and mass withdrawal, it can also provide fast failover. Additionally, the links can be used by the EVPN fabric to utilize ECMP routing. Customer Considerations For customers using MetalLB, it will continue to function correctly regardless of this development. Documentation Considerations Interoperability Considerations Multiple External Gateways (MEG) Egress IP Services Egress Service Egress Firewall Egress QoS"
            },
            "OCPSTRAT-1682": {
                "summary": "OCP Console - Upgrade to PatternFly 6 (PF6)",
                "description": "Feature Overview (aka. Goal Summary) __ Upgrade the OCP console to Pattern Fly 6. Goals (aka. expected user outcomes) __ The core OCP Console should be upgraded to PF 6 and the Dynamic Plugin Framework should add support for PF6 and deprecate PF4. Requirements (aka. Acceptance Criteria): __ Console, Dynamic Plugin Framework, Dynamic Plugin Template, and Examples all should be upgraded to PF6 and all PF4 code should be removed. __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Background __ As a company we have all agreed to getting our products to look and feel the same. The current level is PF6. Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1844": {
                "summary": "TechDebt - OCP Console - Dependency Cleanup",
                "description": "Feature Overview (aka. Goal Summary) We need to maintain our dependencies across all the libraries we use in order to stay in compliance. Goals (aka. expected user outcomes) __ your text here Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1390": {
                "summary": "HCP KubeVirt VM Enhanced Topology Spread",
                "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope __ your text here Background __ your text here Customer Considerations __ your text here Documentation Considerations __ your text here Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1869": {
                "summary": "Phase 1: Cosign tag-based discovery oc-mirror v2: Discover and mirror SigStore-style attachments",
                "description": "Feature Overview (aka. Goal Summary) oc-mirror v2 can mirror images and their associated signatures and public keys, providing flexibility for both current \"SigStore/Cosign tag-based\" and the future \"OCI 1.1 referrer-based\" discovery modes. Goals (aka. expected user outcomes) oc-mirror v2 can discover and mirror image signatures alongside the images it mirrors, adhering to the Cosign tag convention. {color:00875a}(Out of scope for Phase 1, see reason in the comment Public Key Mirroring (or made available offline): Mirror public keys required for signature verification, including: Red Hat's public key for verifying Red Hat content. Rekor's public key for verifying signature transaction records for Red Hat content. To support enclave use cases, mirror public keys and store them locally to enable offline verification. This may require special permissions to access the /etc directory for storage. Out of Scope in Phase 1 __ OCI 1.1 referrer-based Discovery: The tool should be able to discover signatures referenced in the OCI 1.1 image manifest. SigStore-style Attachments Mirroring: SigStore-style attachments, such as SBOMs in formats like text/spdx or application/vnd.cyclonedx, should be optionally discoverable and mirrorable. Users can choose to enable this feature and specify the desired OCI media types. Flexible Signature Discovery - OCI 1.1 First, Cosign Second: oc-mirror v2 will prioritize discovering image signatures using the OCI 1.1 referrer-based approach, falling back to the SigStore/Cosign tag-based approach if necessary. Documentation Considerations __ your text here"
            },
            "OCPSTRAT-1426": {
                "summary": "GA OC mirror v2",
                "description": "Feature description Oc-mirror v2 is focuses on major enhancements that include making oc-mirror faster and more robust and introduces caching as well as address more complex air-gapped scenarios. OC mirror v2 is a rewritten version with three goals: Manage complex air-gapped scenarios, providing support for the enclaves feature Faster and more robust: introduces caching, it doesn\u2019t rebuild catalogs from scratch Improves code maintainability, making it more reliable and easier to add features, and fixes, and including a feature plugin interface"
            },
            "OCPSTRAT-1699": {
                "summary": "Configure containers to set readOnlyRootFilesystem to true starting in OCP 4.19",
                "description": "Red Hat Product Security recommends that pods be deployed with readOnlyRootFilesystem set to true in the SecurityContext, but does not require it because a successful attack can only be carried out with a combination of weaknesses and OpenShift runs with a variety of mitigating controls. However, customers are increasingly asking questions about why pods from Red Hat, and deployed as part of OpenShift, do not follow common hardening recommendations. Note that setting readOnlyRootFilesystem to true ensures that the container's root filesystem is mounted as read-only. This setting has nothing to do with host access. For more information, see Setting the readOnlyRootFilesystem flag to true reduces the attack surface of your containers, preventing an attacker from manipulating the contents of your container and its root file system. If your container needs to write temporary files, you can specify the ability to mount an emptyDir in the Security Context for your pod as described here. The following containers have been identified by customer scans as needing remediation. If your pod will not function with readOnlyRootFilesystem set to true, please document why so that we can document the reason for the exception. Service Mesh operator with sidecar-injector (this needs some additional investigation as we no longer ship the sidecar-injector with Service Mesh) S2I and Build operators: webhook tekton-pipelines-controller tekton-chains-controller openshift-pipelines-operator-cluster-operations tekton-operator-webhook openshift-pipelines-operator-lifecycle-event-listener Pac-webhook (part of Pipelines) Cluster ingress operator: serve-healthcheck-canary Node tuning operator: Tuned Machine Config Operator: Machine-config-daemon ACM Operator: Klusterlet-manifestwork-agent. This was fixed in ACM 2.10."
            },
            "OCPSTRAT-1946": {
                "summary": "Tech Preview AutoNode (Native Karpenter) with ROSA-HCP",
                "description": "Feature Overview (aka. Goal Summary) As a cluster administrator, I want to use Karpenter on an OpenShift cluster running in AWS to scale nodes instead of Cluster Autoscalar(CAS). I want to automatically manage heterogeneous compute resources in my OpenShift cluster without the additional manual task of managing node pools. Additional features I want are: Reducing cloud costs through instance selection and scaling/descaling Support GPUs, spot instances, mixed compute types and other compute types. Automatic node lifecycle management and upgrades This feature covers the work done to integrate upstream Karpenter 1.x with ROSA HCP. This eliminates the need for manual node pool management while ensuring cost-effective compute selection for workloads. Red Hat manages the node lifecycle and upgrades. The goal is roll this out with ROSA-HCP (AWS) since it has more mature Karpenter ecosystem, followed by ARO-HCP (Azure) implementation (refer to OCPSTRAT-1498). This feature will be delivered in 3 Phases: Dev Preview: Autonode with HCP (OCPSTRAT-943) - targeting OCP 4.19 Preview (Tech Preview): Autonode for ROSA-HCP (OCPSTRAT-1946) - TBD (2025) GA: Autonode for ROSA-HCP - OCPSTRAT-2336 The Dev Preview release will expose AutoNode capabilities on Hosted Control Planes for AWS (note this is not meant to be productized on self-managed OpenShift). It includes the following capabilities: _Service Consumer_ opts-in to AutoNode on Day 1 and Day 2 _Service Provider_ lifecycles Karpenter management side _Cluster Admin_ gains access to Karpenter CRDs and default nodeClass _Cluster Admin_ creates a NodePool and scale out workloads _Service Consumer_ signals cluster control plane upgrade Expose Karpenter metrics to Cluster Admin Goals (aka. expected user outcomes) Run Karpenter in management cluster and disable CAS Automate node provisioning in workload cluster automate lifecycle management in workload cluster Reduce cost in heterogenous compute workloads Additional features karpenter Requirements (aka. Acceptance Criteria): Run Karpenter in management cluster and disable CAS OCM API Enable/Disable Cluster autoscaler Enable/disable AutoNode feature New ARN role configuration for Karpenter Optional: New managed policy or integration with existing nodepool role permissions Expose NodeClass/Nodepool resources to users. secure node provisioning and management, machine approval system for Karpenter instances HCP Karpenter cleanup/deletion support ROSA CAPI fields to enable/disable/configure Karpenter Write end-to-end tests for karpenter running on ROSA HCP __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both N/AHosted control planes MNOConnected / Restricted Network x86_x64, ARM (aarch64)Operator compatibility NoUI need (e.g. OpenShift Console, dynamic plugin, OCM) OCM, rosa-cli, ACM, cost management for monitoring and reporting purposes Documentation Considerations __ Migration guides from using CAS to Karpenter Performance testing to compare CAS vs Karpenter on ROSA HCP API documentation for NodePool and EC2NodeClass configuration Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1874": {
                "summary": "Dev Preview Agent-Installer Installation UI for OpenShift Virtualization",
                "description": "Summary The installation process for the OpenShift Virtualization Engine (OVE) has been identified as a critical area for improvement to address customer concerns regarding its complexity compared to competitors like VMware, Nutanix, and Proxmox. Customers often struggle with disconnected environments, operator configuration, and managing external dependencies, making the initial deployment challenging and time-consuming. To resolve these issues, the goal is to deliver a streamlined, opinionated installation workflow that leverages existing tools like the Agent-Based Installer, the Assisted Installer, and the OpenShift Appliance (all sharing the same underlying technology) while pre-configuring essential operators and minimizing dependencies, especially the need for an image registry before installation. By focusing on enterprise customers, particularly VMware administrators working in isolated networks, this effort aims to provide a user-friendly, UI-based installation experience that simplifies cluster setup and ensures quick time-to-value. Objectives and Goals Primary Objectives Simplify the OpenShift Virtualization installation process to reduce complexity for enterprise customers coming from VMware vSphere. Enable installation in disconnected environments with minimal prerequisites. Eliminate the dependency on a pre-existing image registry in disconnected installations. Provide a user-friendly, UI-driven installation experience for users used to VMware vSphere. Goals Deliver an installation experience leveraging existing tools like the Agent-Based Installer, Assisted Installer, and OpenShift Appliance, i.e. the Assisted Service. Pre-configure essential operators for OVE and minimize external day 1 dependencies (see OCPSTRAT-1811 \"Agent Installer interface to install Operators\") Ensure successful installation in disconnected environments with standalone OpenShift, with minimal requirements and no pre-existing registry Personas Primary Audience VMware administrators transitioning to OpenShift Virtualization in isolated/disconnected environments. Pain Points Lack of UI-driven workflows; writing YAML files is a barrier for the target user (virtualization platforms admins) Complex setup requirements (e.g., image registries in disconnected environments). Difficulty in configuring network settings interactively. Lack of understanding when to use a specific installation method Hard time finding the relevant installation method (docs or at console.redhat.com) Technical Requirements Image Registry Simplification Eliminate the dependency on an existing external image registry for disconnected environments. Support a workflow similar to the OpenShift Appliance model, where users can deploy a cluster without external dependencies. Agent-Based Installer Enhancements Extend the existing UI to capture all essential data points (e.g., cluster details, network settings, storage configuration) without requiring YAML files. Install without a pre-existing registry in disconnected environment Install required operators for virtualization OpenShift Virtualization Reference Implementation Guide v1.0.2) doing a POC which was promising: User Interface (no configuration files) The type of users coming from VMware vSphere expect a UI. They aren't used to writing YAML files and this has been identified as a blocker for some of them. We must provide a simple UI to stand up a cluster. Proposed Workflow PRD and notes from regular meetings|"
            },
            "OCPSTRAT-1093": {
                "summary": "Support for VolumeGroup Snapshots (TP)",
                "description": "Feature Overview (aka. Goal Summary) Volume Group Snapshots is a key new Kubernetes storage feature that allows multiple PVs to be grouped together and snapshotted at the same time. This enables customers to takes consistent snapshots of applications that span across multiple PVs. This is also a key requirement for backup and DR solutions. This feature tracks the Tech Preview implementation behind feature gate. Goals (aka. expected user outcomes) Productise the volume group snapshots feature as tech preview have docs, testing as well as a feature gate to enable it in order for customers and partners to test it in advance. Requirements (aka. Acceptance Criteria): The feature should be graduated beta upstream to become TP in OCP. Tests and CI must pass and a feature gate should allow customers and partners to easily enable it. We should identify all OCP shipped CSI drivers that support this feature and configure them accordingly. Use Cases (Optional): As a storage vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my driver support. As a backup vendor I want to have early access to the VolumeGroupSnapshot feature to test and validate my backup solution. As a customer I want early access to test the VolumeGroupSnapshot feature in order to take consistent snapshots of my workloads that are relying on multiple PVs. Out of Scope CSI drivers development/support of this feature. Background __ This allows backup vendors to implemented advanced feature by taking snapshots of multiple volumes at the same time a common use case in virtualisation. Customer Considerations Documentation Considerations Interoperability Considerations"
            },
            "OCPSTRAT-1785": {
                "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
                "description": "Feature Overview Requirements Users can specify multiple NICs for the OpenShift VMs that will be created for the OpenShift cluster nodes with different subnets."
            },
            "OCPSTRAT-1892": {
                "summary": "vSphere - Delete PV and PVCs when destroying a cluster",
                "description": "Goal Remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? When an OpenShift cluster on vSphere with CSI volumes is destroyed the volumes are not deleted, leaving behind multiple objects within vSphere. This leads to storage usage by orphan volumes that must be manually deleted. Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
            },
            "OCPSTRAT-1834": {
                "summary": "Tech Preview OCP Update Precheck command to improve update experience",
                "description": "Feature Overview (aka. Goal Summary) As a cluster-admin I can use a single command to see all upgrade checklist before I trigger an update. Create a Update precheck command that is part of core openshift that helps customers identify potential issues before triggering an OpenShift cluster upgrade, without blocking the upgrade process. This tool aims to reduce upgrade failures and support tickets by surfacing common issues beforehand. Goals (aka. expected user outcomes) Enable users (especially those with limited OpenShift expertise) to identify potential upgrade issues before starting the upgrade Reduce the number of failed upgrades and support tickets Provide clear, actionable information about cluster state relevant to upgrades Help customers make informed decisions about when to initiate upgrades Requirements (aka. Acceptance Criteria): Check Pod Disruption Budgets (PDBs): Identify existing PDBs that might impact the upgrade Display information about PDBs in a way that's understandable to users with limited Kubernetes experience workaround - Check DVO PDB checks Image Registry Access Verification: Validate access to required image repositories Pre-check ability to pull images needed for the upgrade Verify connectivity to public registries or repository of choice workaround : Image pinning GA Node Health Verification: Check for unavailable nodes Identify nodes in maintenance mode Detect unscheduled nodes Verify overall node health status Core Platform Component Health: Verify health of control plane workloads Check core platform operators' health Alert Analysis: List any active critical alerts Display relevant warning alerts Focus on alerts that could impact upgrade success Version-Specific Checks: Include checks specific to the target upgrade version Verify requirements for new features or changes between versions Check networking-related requirements (e.g., SDN to OVN migrations) Output Requirements: Provide clear, understandable output for users without deep OpenShift knowledge Don't block upgrades even if issues are found Present information in an easily digestible format ============== {}New additions in 2025{} MCP status Check the maxUnavailable Compare maxUnavailable to the request level or current load level (if above request level) and determine if this is the correct setting Check to see if the MCPs are paused Make a note if etcd is backed up Other operators Note which operators are set to manual vs automatic update Check to determine the next update of all OLM based operators __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both standaloneHosted control planes AllConnected / Restricted Network AllOperator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Use Cases (Optional): __ your text here Questions to Answer (Optional): __ your text here Out of Scope Blocking upgrade execution Checking entire cluster state Verifying non-platform workloads Automated issue resolution Comprehensive cluster health checking Extensive operator compatibility verification beyond core platform ACM integration Although Operations will use ACM for day 2 operations. Customer Engineering will use cli for patching, updating, precheck etc. Background __ your text here Customer Considerations Target users may have limited Kubernetes/OpenShift expertise Many users coming from VMware background Customers often don't have TAM or premium support Users may not be familiar with platform-specific concepts Need to accommodate users who prefer not to read extensive documentation Documentation Considerations Interoperability Considerations __ your text here",
                "epic_key": "OTA-1432"
            },
            "OCPSTRAT-1585": {
                "summary": "Cluster-version operator version-pod failure accessability",
                "description": "Feature Overview (aka. Goal Summary) Goals (aka. expected user outcomes) Simplify debugging when a cluster fails to update to a new target release image, when that release image is unsigned or otherwise fails to pull. Requirements (aka. Acceptance Criteria): __ enter _general_ Feature acceptance here - Kubelet/CRIO to verify RH images & release payload sigstore signatures - ART will add sigstore signatures to core OCP images __ These acceptance criteria are for all deployment flavors of OpenShift. List applicable specific needs (N/A = not applicable)Self-managed, managed, or both yesHosted control planes Connected / Restricted Network Operator compatibility none, Other (please specify) Documentation Considerations Add documentation for sigstore verification and gpg verification Interoperability Considerations For folks mirroring release images (e.g. disconnected/restricted-network): oc-mirror need to support sigstore mirroring (OCPSTRAT-1417). Customers using BYO image registries need to support hosting sigstore signatures."
            },
            "OCPSTRAT-2240": {
                "summary": "Adding topology-awareness to Cinder CSI Driver",
                "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs| Since OpenStack does not provide a mechanism to map compute nodes to block storage AZs, the Cinder CSI driver treats the compute AZ as a block storage AZ, assuming that the operator has used the same naming convention across their deployment (that is, if there are three compute AZs, {{{}az-0{}}}, {{{}az-1{}}}, and {{{}az-2{}}}, then there will always be at least three block storage AZs with the same name and same semantic meaning (e.g. azN implies a particular rack, room, or data center for both the compute and block storage services). This is a reasonable position and is one the Nova project endorses, however, it isn't always true. Where a deployment is not doing and has divergent compute and block storage AZs, the Cinder CSI driver can end up requesting volumes with block storage AZs that don't exist. The way we have worked around this to date is to selectively enable or disable the topology feature flag provided to the external provisioner side car container, as deployed and managed by the Cinder CSI Driver Operator. This feature flag is being removed in a future release (when?), which means we can't rely on this long-term. We should therefore port the logic for determining whether or not to enable the topology feature from the Cinder CSI Driver Operator to the Cinder CSI Driver itself. Once this is done, we should remove the logic from the Operator since it should no longer be needed and will eventually not be supported. This epic tracks the above work. Why is this important? If we don't do this, we would lose the ability to disable the topology feature in environment where this is not supported (due to mismatched compute and block storage AZ sets). This will affect a number of customers."
            },
            "OCPSTRAT-1873": {
                "summary": "OLMv1: Downstream Feature Gate Promotion Mechanics",
                "description": "Feature Overview (aka. Goal Summary) __ Support iterative development by enabling OLMv1 to work both with a _TechPreviewNoUpgrade_ feature set and a _GeneralAvailability_ feature set. Goals (aka. expected user outcomes) __ Users will be able to opt into testing out new features via _TechPreviewNoUpgrade_ Users will be able to use OLMv1 GA features without the risk of _TechPreview_ features Requirements (aka. Acceptance Criteria): __ All necessary infrastructure in place to enable the use of feature gates CI jobs to ensure that nothing from the _TechPreviewNoUpdate_ feature set breaks OLMv1 __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) | Out of Scope __ OLMv1 will soon need a mechanism for supporting alpha/beta fields in CRDs, but it has been decided to explore this as a separate effort. Background __ Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP which might span multiple releases to reach completeness and stability (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will need to work both with TechPreviewNoUpgrade and without it but will have a different set of features. Customer Considerations __ your text here Documentation Considerations __ Will need documentation on how to enable the _TechPreviewNoUpgrade_ feature set Interoperability Considerations __ your text here"
            },
            "OCPSTRAT-1341": {
                "summary": "Remove Cgroup v1 from OCP in 4.19",
                "description": "Feature Overview (aka. Goal Summary) Cgroup V1 was deprecated in OCP 4.16 . RHEL will be removing support for cgroup v1 in RHEL 10 so we will remove it in OCP 4.19 Goal Upgrade Scenario For clusters running cgroup v1 on OpenShift 4.18 or earlier, upgrading to OpenShift 4.19 will be blocked. To proceed with the upgrade, clusters on OpenShift 4.18 must first switch from cgroup v1 to cgroup v2. Once this transition is complete, the cluster upgrade to OpenShift 4.19 can be performed."
            },
            "OCPSTRAT-1654": {
                "summary": "GA User Name Space in OpenShift 4.20",
                "description": "Feature Overview (aka. Goal Summary) GA User Name Space in OpenShift 4.20 continue work from"
            },
            "OCPSTRAT-2073": {
                "summary": "GA for sigstore API(clusterimagepolicy, imagepolicy)",
                "description": "GA for sigstore API(clusterimagepolicy, imagepolicy)"
            },
            "OCPSTRAT-1359": {
                "summary": "BYOPKI for image verification in OCP - Dev P in 4.19",
                "description": "Feature Overview (aka. Goal Summary) BYOPKI for image verification in OCP"
            },
            "OCPSTRAT-1805": {
                "summary": "Azure - Add support for Dxv6 machine series",
                "description": "Feature Overview (aka. Goal Summary) Dlsv6 Dsv6 Goals (aka. expected user outcomes) Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) Dldsv6 Ddsv6| Documentation Considerations Interoperability Considerations"
            },
            "OCPSTRAT-1854": {
                "summary": "Support EndPort in MultiNetworkPolicy",
                "description": "Feature Overview (aka. Goal Summary) This Feature adds support for EndPort in MultiNetworkPolicy for customers migrating VM instances to OpenShift Virtualization, and with a requirement to specify a port-range without having to individually specify each port separately. Without this Feature, customers will have issues migrating specific VMs to OpenShift Virtualization. It is currently supported with NetworkPolicy, but not yet with MultiNetworkPolicy. Goals (aka. expected user outcomes) A port range can be specified in MultiNetworkPolicy, instead of having to specify each port individually. Requirements (aka. Acceptance Criteria): __ List applicable specific needs (N/A = not applicable)Self-managed, managed, or both Hosted control planes Connected / Restricted Network Operator compatibility UI need (e.g. OpenShift Console, dynamic plugin, OCM) NetworkPolicy product docs| Questions to Answer (Optional): Out of Scope Background Customer Considerations Documentation Considerations Interoperability Considerations"
            }
        }
    },
    "OpenShift Specialist Platform Team": {
        "stories": {
            "SPLAT-2079": {
                "summary": "vSphere CPMS GA+1 Cleanup",
                "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere CPMS support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for vSphere CPMS. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the vSphere CPMS feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD",
                "GITHUB": [
                    {
                        "id": "2236",
                        "type": "pullRequest",
                        "title": "SPLAT-2079: Removed VSphereControlPlaneMachineSet feature gate",
                        "body": "SPLAT-2079( Changes - Removed VSphereControlPlaneMachineSet feature gate - Fixed CRD to have config available as default Dependencies -"
                    },
                    {
                        "id": "354",
                        "type": "pullRequest",
                        "title": "SPLAT-2079: Removed VSphereControlPlaneMachineset feature gate",
                        "body": "SPLAT-2079( Changes - Removed VSphereControlPlaneMachineset feature gate"
                    }
                ]
            },
            "SPLAT-2078": {
                "summary": "vSphere Static IP GA+1 Cleanup",
                "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere Static IP support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for Static IP. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the static ip feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD",
                "GITHUB": [
                    {
                        "id": "9560",
                        "type": "pullRequest",
                        "title": "SPLAT-2078: Removed VSphereStaticIPs feature gate",
                        "body": "SPLAT-2078( Changes - Removed references to VSphereStaticIPs feature gate Notes After a feature gate has been GA in an OCP release, we are supposed to clean up feature gate usage in the next release. VSphere static IP support GA'd in 4.16, so the intention is to remove the feature gate as part of 4.19."
                    },
                    {
                        "id": "2232",
                        "type": "pullRequest",
                        "title": "SPLAT-2078: Removed VSphereStaticIPs feature gate",
                        "body": "SPLAT-2078( Changes - Removed VSphereStaticIPs feature gate Dependencies - -"
                    },
                    {
                        "id": "1346",
                        "type": "pullRequest",
                        "title": "SPLAT-2078: Removed VSphereStaticIPs feature gate",
                        "body": "SPLAT-2078( Changes - Removed VSphereStaticIPs feature gate Notes After a feature gate has been GA in an OCP release, we are supposed to clean up feature gate usage in the next release. VSphere static IP support GA'd in 4.16, so the intention is to remove the feature gate as part of 4.19."
                    }
                ]
            },
            "SPLAT-2072": {
                "summary": "vSphere Multi vCenter GA+1 Cleanup",
                "description": "USER STORY: As a developer, I need to remove all feature gates around vSphere muliti vCenter support one release after GA so that feature gate logic is removed and all functions are no longer needing feature gate protections. DESCRIPTION: This story will clean up all of the feature gate logic for multi vCenter. Currently there are several projects that check to see if the feature gate is enabled in order for the logic to be performed. As part of being GA, the code is enabled by default. You can still force disable it in install-config if you wish, which is why we left it in GA+0, but GA+1 we are to remove it assuming all major bugs are fixed. ACCEPTANCE CRITERIA: All components referencing the multi vCenter feature gate have been updated to no longer use it and point to a version of API where the feature gate no longer exists. ENGINEERING DETAILS: TBD",
                "GITHUB": [
                    {
                        "id": "9553",
                        "type": "pullRequest",
                        "title": "SPLAT-2072: Removed VSphereMultiVCenters feature gate",
                        "body": "SPLAT-2072( Changes - Removed usage of feature gate VSphereMultiVCenter for multiple vCenter configurations since its now GA. - Removed legacy ini configuration from installer now that multi vCenter is GA. Notes After a feature gate has been GA in an OCP release, we are supposed to clean up feature gate usage in the next release. Multi vCenter support GA'd in 4.18, so the intention is to remove the feature gate as part of 4.19."
                    },
                    {
                        "id": "300",
                        "type": "pullRequest",
                        "title": "SPLAT-2072: Removed VSphereMultiVCenters feature gate",
                        "body": "SPLAT-2072( Changes - Removed usage of VSphereMultiVCenters feature gate - Migrated multivcenterstorageclasscontroller into the default controller Notes After a feature gate has been GA in an OCP release, we are supposed to clean up feature gate usage in the next release. VSphere multi vCenter support GA'd in 4.18, so the intention is to remove the feature gate as part of 4.19."
                    }
                ]
            },
            "SPLAT-1809": {
                "summary": "Enhance Installer to support vSphere multi disk",
                "description": "User Story: As an OpenShift Engineer I need to enhance the OpenShift installer to support creating a cluster with additional disks added to control plane and compute nodes so that I can use the new data disks for various OS needs. Description: This task is to enhance the installer to allow configuring data disks in the install-config.yaml. This will also require setting the necessary fields in machineset and machine definitions. The important one being for CAPV to do the initial creation of disks for the configured masters. Acceptance Criteria: - install-config.yaml supports configuring data disks in all machinepools. - CAPV has been updated with new multi disk support. - CAPV machines are created that result in control plane nodes with data disks. - MachineSet definitions for compute nodes are created correctly with data disk values from compute pool. - CPMS definition for masters has the data disks configured correctly. Notes: We need to be sure that after installing a cluster, the cluster remains stable and has all correct configurations.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "9035",
                        "type": "pullRequest",
                        "title": "SPLAT-1809: Added vSphere multi disk support",
                        "body": "SPLAT-1809( Changes - Enhance install-config.yaml to allow for creating of additional vSphere disks - Leverage post-provision hook to add disks to masters after they have been created Prerequisites -"
                    }
                ]
            },
            "SPLAT-1801": {
                "summary": "vsphere problem detector needs additional checks for group zonal",
                "description": "As an openshift engineer make changes to vpd for host vm zonal so that a support engineer or customer is notified of potential missing groups or rules. Acceptance Criteria - Since there is a new requirement for: host groups, vm groups and host vm group rules make sure these exist properly",
                "epic_key": "SPLAT-1728",
                "GITHUB": [
                    {
                        "id": "175",
                        "type": "pullRequest",
                        "title": "SPLAT-1801: Add support for host groups",
                        "body": "Summary - Add checks for host groups - Eliminate test mask in preference for SimulatorActions"
                    }
                ]
            },
            "SPLAT-1800": {
                "summary": "Enable host vm group zonal for mao",
                "description": "As an openshift engineer enable host vm group zonal in mao so that compute nodes properly are deployed Acceptance Criteria: - Modify workspace to include vmgroup - properly configure vsphere cluster to add vm into vmgroup",
                "epic_key": "SPLAT-1728",
                "GITHUB": [
                    {
                        "id": "1323",
                        "type": "pullRequest",
                        "title": "SPLAT-1800: vSphere host zonal missing operator feature",
                        "body": "Follow up to 1285. This PR adds `FeatureGateVSphereHostVMGroupZonal` to the map of features so the command line of the controller pods is set correctly."
                    },
                    {
                        "id": "1285",
                        "type": "pullRequest",
                        "title": "SPLAT-1800: Add support for vSphere host and vm group based zonal",
                        "body": "Changes Misc required change: - Added RealClock to NewKubeRecorder and NewLoggingEventRecorder Changes: - Added FeatureGateVSphereHostVMGroupZonal - Added modifyVMGroup which adds or removes a virtual machine from the vCenter cluster's vm-host group associated with the failure domain - Added validation for the length of a vm-group name - Added vm-host group tests Additional PRs - - - - - - - -"
                    }
                ]
            },
            "SPLAT-1799": {
                "summary": "Enable host vm group zonal for cpms",
                "description": "As an openshift engineer enable host vm group zonal in CPMS so that control plane nodes properly are redeployed Acceptance Criteria: - Control plane nodes properly roll out when requried - Control plane nodes do not roll out when not needed",
                "epic_key": "SPLAT-1728",
                "GITHUB": [
                    {
                        "id": "325",
                        "type": "pullRequest",
                        "title": "SPLAT-1799: Add support for vSphere host and vm group based zonal",
                        "body": "Changes - Modified events.NewLoggingEventRecorder adding clock.RealClock{} param - Add checks for failureDomain hostgroup vmgroup fields - Add checks for machine workspace vmgroup fields Additional PRs - - - - - - - -"
                    }
                ]
            },
            "SPLAT-1742": {
                "summary": "Modify the installer to support host and vm group based zonal",
                "description": "USER STORY: As someone that installs openshift on vsphere, I want to install zonal via host and vm groups so that I can use a stretched physical cluster or use a cluster as a region and hosts as zones . DESCRIPTION: Required: Nice to have: ACCEPTANCE CRITERIA: - start validating tag naming - validate tags exist - validate host group exists - update platform spec - unit tests - create capv deployment and failure domain manifests - per failure domain, create vm group and vm host rule ENGINEERING DETAILS: Configuration steps: - Create tag and tag categories - Attach zonal tags to ESXi hosts - capv will complain extensively if this is not done - Host groups MUST be created and populated prior to installation (maybe, could we get the hosts that are attached or vice versa hosts in host group are attached?) one per zone - vm groups wil be created by the installer one per zone - vm / host rule will be created by the installer one per zone",
                "epic_key": "SPLAT-1728",
                "GITHUB": [
                    {
                        "id": "8873",
                        "type": "pullRequest",
                        "title": "SPLAT-1742: vSphere - enable host group based zonal",
                        "body": "Changes - Migrated to - Updated validations to include vm-host group of type Host and tagging. Also associated tests. - Create capv FailureDomain and DeploymentZone - Delete vm-host group of type VirtualMachine and vm-host rule created by installer - Create an associated vm-host group of type virtual machine and a affinity vm-host rule. - Update platform spec topology to include `hostGroup` which needs to be created day 0, prior to installation. Additional PRs - - - - - - - -"
                    }
                ]
            },
            "SPLAT-1722": {
                "summary": "remove alibaba from cluster-ingress-operator",
                "description": "Acceptance Criteria - Since api and library-go are the last projects for removal, remove only alibaba specific code and vendoring",
                "epic_key": "SPLAT-1454",
                "GITHUB": [
                    {
                        "id": "1111",
                        "type": "pullRequest",
                        "title": "SPLAT-1722: Remove alibaba",
                        "body": "This PR removes support for the Alibaba cloud which is being removed from the product. Additional PRs - - - - - -"
                    }
                ]
            },
            "SPLAT-2060": {
                "summary": "Create e2e tests for vSphere Data Disks",
                "description": "USER STORY: As a developer, I need to create e2e tests for the new vSphere Data Disk feature so that we have proper code coverage and meet the required metrics to allow the feature to become GA some point in the future Required: Need to create e2e tests that meet the metrics defined in the openshift/api project Nice to have: Move these e2e tests into the projects that should own them using the new external tests feature being added to origin. ACCEPTANCE CRITERIA: At least 5 tests 95% of past tests have passed ENGINEERING DETAILS: Lets use this opportunity to start migrating e2e tests to the project where the new features are being added which will remove the dependency of having to stage merging code in one repo and then updating e2e in origin.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "1338",
                        "type": "pullRequest",
                        "title": "SPLAT-2060: Create e2e tests for vSphere Data Disk feature",
                        "body": "SPLAT-2060( Changes - Created new external test extension to origin - Added e2e tests for vSphere data disks Blocks - Notes After this merges, we will need to update origin to register this extension so it runs in future e2e tests. This enhancment is leveraging the following origin framework enhancement:"
                    },
                    {
                        "id": "29579",
                        "type": "pullRequest",
                        "title": "SPLAT-2060: Registered Machine API tests extension",
                        "body": "SPLAT-2060( Changes - Registered Machine API tests extension"
                    }
                ]
            },
            "SPLAT-2051": {
                "summary": "create origin e2e tests",
                "description": "{}USER STORY:{} As an OpenShift engineer, I want to create 5 e2e tests to validate multi-nic capabilities so that regressions are uncovered more quickly. {}DESCRIPTION:{} Create tests which specifically address 5 test cases/configurations to validate. {}Required:{} {}Nice to have:{} ... {}ACCEPTANCE CRITERIA:{} 5 test cases related to the multi-network feature gate Tests are passing {}ENGINEERING DETAILS:{} !-- Any additional information that might be useful for engineers: related repositories or pull requests, related email threads, GitHub issues or other online discussions, how to set up any required accounts and/or environments if applicable, and so on. --",
                "epic_key": "SPLAT-1944",
                "GITHUB": [
                    {
                        "id": "1347",
                        "type": "pullRequest",
                        "title": "SPLAT-2051: implement e2e tests for vSphere multi network"
                    }
                ]
            },
            "SPLAT-2000": {
                "summary": "Add thinProvisioned to the new multi disk api",
                "description": "USER STORY: As an OpenShift administrator, I want to be able to configure thin provisioned for my new data disks so that adjust the behavior that may be different than my default storage policy. DESCRIPTION: Currently, we have the machine api changes forcing the thin provisioned flag to true. We need to add a flag to allow admin to configure this. The default behavior will be to not set the flag and use default storage policy. ACCEPTANCE CRITERIA: API has new flag Machine API has been modified to to use the new flag if set, else do not set the thinProvisioned attribute during clone.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "9439",
                        "type": "pullRequest",
                        "title": "SPLAT-2000: Added vSphere provisioning mode support for data disks",
                        "body": "SPLAT-2000( Changes - Updated installer to contain new provisioning mode field in install-config for vSphere data disks - Updated installer to use new CAPV and openshift/API for data disk provisioning mode - Updated installer to generate CAPV and MAPI machines with provisioning fields populated Dependencies -"
                    },
                    {
                        "id": "2154",
                        "type": "pullRequest",
                        "title": "SPLAT-2000: Add configuration for vSphere multi disk thinProvisioned",
                        "body": "SPLAT-2000( Changes - Added thinProvisioned to VSphereDisk Blocks - - Notes - Working with upstream CAPV to add similar behavior to prevent gap - CAPV changes:"
                    },
                    {
                        "id": "1328",
                        "type": "pullRequest",
                        "title": "SPLAT-2000: Added vSphere provisioning mode to data disks",
                        "body": "SPLAT-2000( Changes - Added ability to set provisioning mode to data disks Dependencies -"
                    }
                ]
            },
            "SPLAT-1995": {
                "summary": "Destroy and confirm there are no lingering CNS volumes",
                "description": "As a openshift engineer I want the installer to make sure there are no CNS volumes so we are not leaking volumes that could be taking disk space or alerting in vCenter. acceptance criteria - check if there are cns volumes, check if there are still pv from previous delete. If there are still CNS volumes and no PV(s) try to delete, if unsuccessful just return list - are you sure warning",
                "epic_key": "SPLAT-1993",
                "GITHUB": [
                    {
                        "id": "9425",
                        "type": "pullRequest",
                        "title": "SPLAT-1995: vsphere on destroy remove cns volumes",
                        "body": "This pr does the following: - Add vSphere CNS client - Returns a slice of CNS volumes associated with the infraid via `GetCnsVolumes()` - Destroys all CNS volumes via `DeleteCnsVolumes()` based on slice provide by `GetCnsVolumes()` - Updates the generated mock for the new methods"
                    }
                ]
            },
            "SPLAT-1817": {
                "summary": "Update CPMS operator to support vSphere multi disk",
                "description": "User Story: As an OpenShift Engineer I need to ensure the the CPMS Operator now works with detecting any changes needed when data disks are added to the CPMS definition. Description: This task is to verify if any changes are needed in the CPMS Operator to handle the change data disk definitions in the CPMS. Acceptance Criteria: - CPMS does not roll out changes when initial install is performed. - Adding a disk to CPMS results in control plane roll out. - Removing a disk from CPMS results in control plane roll out. - No errors logged as a result of data disks being present in the CPMS definition. Notes: Ideally we just need to make sure the operator is updated to pull in the new CRD object definitions that contain the new data disk field.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "335",
                        "type": "pullRequest",
                        "title": "SPLAT-1817: Bump openshift/api to get vSphere multi disk support",
                        "body": "SPLAT-1817( Changes - Bumped openshift/api version Prerequisites - -"
                    }
                ]
            },
            "SPLAT-1811": {
                "summary": "Add vSphere multi disk support to machine api operator",
                "description": "User Story: As an OpenShift Engineer I need to ensure the the MAPI Operator. Description: This task is to verify if any changes are needed in the MAPI Operator to handle the change data disk definitions in the CPMS. Acceptance Criteria: - Adding a disk to MachineSet does not result in new machines being rolled out. - Removing a disk from MachineSet does not result in new machines being rolled out. - After making changes to a MachineSet related to data disks, when MachineSet is scaled down and then up, new machines contain the new data disk configurations. - All attempts to modify existing data disk definitions in an existing Machine definition are blocked by the webhook. Notes: The behaviors for the data disk field should be the same as all other provider spec level fields. We want to make sure that the new fields are no different than the others. This field is not hot swap capable for running machines. A new VM must be created for this feature to work.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "1290",
                        "type": "pullRequest",
                        "title": "SPLAT-1811: Add vSphere multi disk support",
                        "body": "SPLAT-1811( Changes - Add logic for creating additional disks to new machines Prerequisites - -"
                    }
                ]
            },
            "SPLAT-1808": {
                "summary": "Create feature gate and initial CRD changes for multi disk",
                "description": "User Story: As an OpenShift Engineer I need to create a new feature gate and CRD changes for vSphere multi disk so that we can gate the new function until all bugs are ironed out. Description: This task is to create the new feature gate to be used by all logical changes around multi disk support for vSphere. We also need to update the types for vsphere machine spec to include new array field that contains data disk definitions. Acceptance Criteria: - New feature gate exists for components to use. - Initial changes to the CRD for data disks are present for components to start using.",
                "epic_key": "SPLAT-1880",
                "GITHUB": [
                    {
                        "id": "2028",
                        "type": "pullRequest",
                        "title": "SPLAT-1808: Add vSphere multi disk support",
                        "body": "SPLAT-1808( Changes - Create new feature gate for vSphere multi disk - Create initial CRD changes for adding additional disks to vSphere machines Blocks - - - - Notes - This is being done as part of vSphere work for this( feature - MAO Changes:"
                    }
                ]
            },
            "SPLAT-1743": {
                "summary": "Change API to support host vm group based zonal",
                "description": "As an openshift engineer update the infrastructure and the machine api objects so it can support host vm group zonal Acceptance criteria - Add the appropriate fields for host vm group zonal - Add the appropriate documentation for the above fields - Add a new feature gate for host vm zonal",
                "epic_key": "SPLAT-1728",
                "GITHUB": [
                    {
                        "id": "1999",
                        "type": "pullRequest",
                        "title": "SPLAT-1743: vSphere - add host and vm based zonal",
                        "body": "Changes - New feature gate `VSphereHostVMGroupZonal` - Add `RegionType` and `ZoneType` fields to `VSpherePlatformFailureDomainSpec` to specify region and zone failure domain types. - Add `VSphereFailureDomainAffinity` type that contains three fields required for vm-host zonal: `VMGroup`, `HostGroup` and `VMHostRule`. - Add `VSphereFailureDomainAffinity` to `VSpherePlatformTopology` - Create additional unit tests to cover the introduction of vm-host zonal provisioning types. - Add `VMGroup` in a machine(s) workspace. Additional PRs - - - - - - -"
                    },
                    {
                        "id": "2074",
                        "type": "pullRequest",
                        "title": "SPLAT-1743: Add VMGroup to VSphere Provider workspace",
                        "body": "Changes For the machine api operator to support vm-host based zonal a single new field of `vmGroup` needs to be added that will indicate the vCenter cluster group to add a newly created virtual machine into. Additional PRs - - - - - - -"
                    }
                ]
            }
        },
        "epics": {
            "SPLAT-1880": {
                "summary": "vsphere Multi Disk Support",
                "description": "User Story: As an OpenShift administrator, I need to be able to configure my OpenShift cluster to have additional disks on each vSphere VM so that I can use the new data disks for various OS needs. Description: This goal of this epic is to be able to allow the cluster administrator to install and configure after install new machines with additional disks attached to each virtual machine for various OS needs. Required: Installer allows configuring additional disks for control plane and compute virtual machines Control Plane Machine Sets (CPMS) allows configuring control plane virtual machines with additional disks Machine API (MAPI) allows for configuring Machines and MachineSets with additional disks Cluster API (CAPI) allows for configuring Machines and MachineSets with additional disks Nice to Have: Acceptance Criteria: Notes:"
            },
            "SPLAT-1728": {
                "summary": "Tech Preview OpenShift Zones support for vSphere Host Groups",
                "description": "Epic Goal Support mapping OpenShift zones to vSphere host groups, in addition to vSphere clusters. When defining zones for vSphere administrators can map regions to vSphere datacenters and zones to vSphere clusters. There are use cases where vSphere clusters have only one cluster construct with all their ESXi hosts but the administrators want to divide the ESXi hosts in host groups. A common example is vSphere stretched clusters, where there is only one logical vSphere cluster but the ESXi nodes are distributed across to physical sites, and grouped by site in vSphere host groups. In order for OpenShift to be able to distribute its nodes on vSphere matching the physical grouping of hosts, OpenShift zones have to be able to map to vSphere host groups too. Requirements{} Users can define OpenShift zones mapping them to host groups at installation time (day 1) Users can use host groups as OpenShift zones post-installation (day 2)"
            },
            "SPLAT-1454": {
                "summary": "Remove IPI/UPI support of Alibaba cloud from OpenShift",
                "description": "OCP/Telco Definition of Done Epic Goal We want to remove official support for UPI and IPI support for Alibaba Cloud provider. Going forward, we are recommending installations on Alibaba Cloud with either external platform installation method. Why is this important? __ Scenarios Impacted areas based on CI: alibaba-cloud-csi-driver/openshift-alibaba-cloud-csi-driver-release-4.16.yaml alibaba-disk-csi-driver-operator/openshift-alibaba-disk-csi-driver-operator-release-4.16.yaml cloud-provider-alibaba-cloud/openshift-cloud-provider-alibaba-cloud-release-4.16.yaml cluster-api-provider-alibaba/openshift-cluster-api-provider-alibaba-release-4.16.yaml cluster-cloud-controller-manager-operator/openshift-cluster-cloud-controller-manager-operator-release-4.16.yaml machine-config-operator/openshift-machine-config-operator-release-4.16.yaml Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI jobs are removed Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "SPLAT-1944": {
                "summary": "GA vSphere multi-NIC VM creation support in the IPI installer",
                "description": "OCP/Telco Definition of Done Epic Goal In 4.18, support for multiple NICs was released as tech preview. The goal of this epic is to promote the feature to GA. Primarily, this involves proving the stability of the feature through supporting CI jobs and Sippy. Once proven, the feature gate and associated logic across impacted components are removed. Why is this important? Provides production/typical support for this feature. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "SPLAT-1993": {
                "summary": "vSphere - Delete PV and PVCs on installer destroy",
                "description": "Epic Goal The goal of this epic is upon destroy to remove all persistent volumes and claims. Also check if there are any CNS volumes that could be removed but the pv/pvc deletion should check for that. Why is this important? Multiple customers have requested this feature and we need this feature for CI. PV(s) are not cleaned up and leave behind CNS orphaned volumes that cannot be removed."
            }
        }
    },
    "Observability UI": {
        "stories": {
            "OU-659": {
                "summary": "Migrate the monitoring plugin to use patternfly 6",
                "description": "Background OCP 4.19 introduces PF6, this changes dramatically the UI. As the monitoring plugin is included by default with OCP we need to upgrade it so is consistent with the console UI Outcomes The plugin works without errors in the 4.19 console running Patternfly v6 Custom classes are removed when possible to make future upgrades easier",
                "epic_key": "OU-639",
                "GITHUB": [
                    {
                        "id": "380",
                        "type": "pullRequest",
                        "title": "OU-659: PF-6 Migration"
                    }
                ]
            }
        },
        "epics": {
            "OU-639": {
                "summary": "Migrate UIPlugins to PF6",
                "description": "Description \"In order to keep plugins working with future versions of the OpenShift web console and match the new PF6 design, we as the Observability UI Team need to upgrade them to PF6.\" Goals & Outcomes +Engineering/Data Analytics Requirements:+ All UI plugins use PF6 dependency Documentation Patternfly support for plugins in the console:"
            }
        }
    },
    "OpenShift Over the Air": {
        "stories": {
            "OTA-1427": {
                "summary": "USC: Maintain status insights for Nodes",
                "description": "Implement a new Informer controller in the Update Status Controller to watch Node resources in the cluster and maintain an update status insight for each. The informer will need to interact with additional resources such as MachineConfigPools and MachineConfigs, e.g. to discover the OCP version tied to config that is being reconciled on the Node, but should not attempt to maintain the MachineConfigPool status insights. Generally the node status insight should carry enough data for any client to be able to render a line that the oc adm upgrade status currently shows: {code} NAME ASSESSMENT PHASE VERSION EST MESSAGE build0-gstfj-ci-prowjobs-worker-b-9lztv Degraded Draining 4.16.0-ec.2 ? failed to drain node: node after 1 hour. Please see machine-config-controller logs for more informatio build0-gstfj-ci-prowjobs-worker-d-ddnxd Unavailable Pending ? ? Machine Config Daemon is processing the node build0-gstfj-ci-tests-worker-b-d9vz2 Unavailable Pending ? ? Not ready build0-gstfj-ci-tests-worker-c-jq5rk Unavailable Updated 4.16.0-ec.3 - Node is marked unschedulable {code} The basic expectations for Node status insights are described in the design docs| but the current source of truth for the data structure is the NodeStatusInsight structure from . Definition of Done - During the upgrade, the status api contains a Node status insight for each Node in the cluster - Do not bother with the status insight lifecycle (when a Node is removed from the cluster, the status insight should technically disappear, but do not address that in this card, suitable lifecycle mechanism for this does not exist yet and OTA-1418 will address it) - Overall the functionality should match what oc adm upgrade status client-based checks - The NodeStatusInsight should have correctly populated: name, resource, poolResource, scopeType, version, estToComplete and message fields, following the existing logic from oc adm upgrade status - Health insights are out of scope - Status insights for MCPs are out of scope - The Updating condition has a similar meaning and interpretation like in the other insights. -- When its status is False, it will contain a reason which needs to be interpreted. Three known reasons are Pending, Updated and Paused: --- Pending: Node will eventually be updated but has not started yet --- Updated: Node already underwent the update. --- Paused: Node is running an outdated version but something is pausing the process (like parent MCP .spec.paused field) -- When Updating=True, there are also three known reasons: Draining, Updating and Rebooting. --- Draining: MCO drains the node so it can be updated and rebooted --- Updating: MCO applies the new config and prepares the node to be rebooted into the new OS version --- Rebooting: MCO is rebooting the node, after which it (hopefully) becomes ready again - The Degraded and Unavailable condition logic should match the existing assessment logic from oc adm upgrade status",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1164",
                        "type": "pullRequest",
                        "title": "OTA-1427: Reconcile all nodes via a special event",
                        "body": "This PR addressed the remained comments from In the 1st commit it improves the error handling on `reconcileAllNodes()` (See 1 for details). Prior to this commit, the event on MC/MCP will be re-queued if `reconcileAllNodes()` hits an error. However, `syncMachineConfig()` (or `syncMachineConfigPool` respectively) is stateful, i.e., the result replies on the content of the caches that might be changed from the original event. With the commit, a special event stays between an MC/MCP event and `reconcileAllNodes()`. An error from the latter will re-queue the special event which basically means triggering another run of `reconcileAllNodes()`. The 2nd commit is to improve the code either for better readability or for better APIs from the lib. Sync.Map with powerful functions for atomic ops on a map is an example for the latter case. In the 3rd commit, it moves sync.Once into nodeInformerController controller level. As a result each nodeInformerController instance will execute the function once which makes more sense than doing it once grobally because what we do there is initialization of the caches that are associated with the instance. For the core code, it is not a big deal since we have only ONE instance of the controller. However, it matters for unit tests where there are many controllers. 1."
                    },
                    {
                        "id": "1144",
                        "type": "pullRequest",
                        "title": "OTA-1427: Recompute node insights when MC/MCP changes",
                        "body": "This is to address the following comments from the review 1: - Decide what to do with the code we copied from MCO ( - x Avoid iterating all MCP resource and re-computing selectors on every Node-triggered sync ( - x Recompute Node insights when MCP selectors change (changed selectors can change MCP membership, potentially making Node insights stale) ( - x Avoid iterating all MC resources on every Node-triggered sync ( - x Decide how to gracefully handle Nodes that are not matched by any MCP selector ( - Polishing condition reasons, messages and the \"summary\" insight message ( 1."
                    },
                    {
                        "id": "1145",
                        "type": "pullRequest",
                        "title": "OTA-1427: Polish reason and message in Node's status insight",
                        "body": "This is to address the following comments from the review 1: - Decide what to do with the code we copied from MCO ( - Avoid iterating all MCP resource and re-computing selectors on every Node-triggered sync ( - Recompute Node insights when MCP selectors change (changed selectors can change MCP membership, potentially making Node insights stale) ( - Avoid iterating all MC resources on every Node-triggered sync ( - Decide how to gracefully handle Nodes that are not matched by any MCP selector ( - x Polishing condition reasons, messages and the \"summary\" insight message ( 1."
                    },
                    {
                        "id": "1136",
                        "type": "pullRequest",
                        "title": "OTA-1427: USC: Maintain status insights for Nodes"
                    },
                    {
                        "id": "1142",
                        "type": "pullRequest",
                        "title": "OTA-1427: Add hack/test-usc-integration.sh to start testing USC in CI",
                        "body": "Required by /cc @petr-muller /auto-cc"
                    }
                ]
            },
            "OTA-1418": {
                "summary": "USC: Implement proper lifecycle for health insights",
                "description": "Health insights have a lifecycle that is not suitable for the async producer/consumer architecture USC has right now, where update informers send individual insights to the controller that maintains the API instance. Health insights are expected to disappear and appear following the trigger condition, and this needs to be respected through controller restart, API unavailability etc. Basically this means that the informer should ideally work as a standard, full-reconciliation controller over a set of its insights. We should also have an easy method to test health insight lifecycle: easy on/off switch for an artificial health insight to be reported or not, to avoid relying on true problematic conditions for testing the controller operation. Something like an existence of a label or an annotation on a resource to trigger a health insight directly. Definition of Done - When a resource watched by USC (so ClusterVersion ATM) has a certain annotation, USC should produce an artificial health insight - The properties of the health insight do not matter but must clearly indicate the health insight is artificial and intended for testing - All these scenarios must work: -- When insight is not in API, USC is running and the annotation is added to CV - insight is added to API -- When insight is not in API, USC is running, annotation is not on CV, insight is manually added to API - insight is removed from API -- When insight is in the API, USC is running, annotation is removed from CV - insight is removed from API -- When insight is in the API, USC is running, insight is manually removed from API - insight is added from API -- When insight is not in API, USC is stopped, annotation is added to CV, USC is started - insight is added to API -- When insight is in the API, USC is stopped, annotation is removed from CV, USC is started - insight is removed from API - In all cases where there is an existing insight in the API and the annotation was never observed removed from the CV, any \"refresh\" by USC must respect the original properties of the insight (start time, uid, etc) - Identical sync mechanism should be used for Status Insights",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1162",
                        "type": "pullRequest",
                        "title": "OTA-1418: Decribe informer-usc communication",
                        "body": "Going from USC-scrapes-all-insights periodically to unidirectional message passing makes insight lifecycle a bit harder (if an informer is not saying anyting about an insight Status API knows about, is it just unchanged, or did the informer lose track of it?). List of known insight UIDs is lightweight-enough payload to be included in every message sent to USC and the USC can (eventually) drop the insights that the informer stops reporting as known, and can drop the insights the informer requests to be dropped. Unsure whether this mechanism survives until the final version but it is reasonably robust for the current state."
                    },
                    {
                        "id": "1150",
                        "type": "pullRequest",
                        "title": "OTA-1418: USC: Drop unknown insights after grace period",
                        "body": "When we first see an insight in the API that is no longer known by the informer that originally reported it, mark it for expiration instead of dropping it right away. If it becomes known again, unmark the expiration. If it does not become known until expired, drop it from the API."
                    },
                    {
                        "id": "1141",
                        "type": "pullRequest",
                        "title": "OTA-1418: Allow forcing health insights on CV",
                        "body": "OTA-1418( Allow forcing health insights on CV When a `ClusterVersion` is annotated with `usc.openshift.io/force-health-insight`, make USC emit a health insight, suitable for testing health insight lifecycle. Note that such health insight never goes away for now; this will be addressed by followup PRs. Health insights UIDs are a hash of the insight message and the resources involved. This will become problematic if we ever want to do aggregation, but for now it suits the purpose. Builds on 1138 /cc @hongkailiu /auto-cc"
                    }
                ]
            },
            "OTA-1405": {
                "summary": "Introduce the configuration API into HyperShift",
                "description": "The CVO is configurable using a manifest file. We now can proceed to HyperShift. Introduce the HyperShift API changes described in the enhancement. Definition of Done: API changes are merged in HyperShift.",
                "epic_key": "OTA-923",
                "GITHUB": [
                    {
                        "id": "5569",
                        "type": "pullRequest",
                        "title": "OTA-1405: Add the ClusterVersionOperator API",
                        "body": "Add the ClusterVersionOperator API: Add the feature-gated ClusterVersionOperator API for HyperShift, which is described in more detail in the CVO Log Level API enhancement( This PR only introduces the API changes. The API will be implemented as part of the OTA-1406( issue. This is to break down the problem into smaller PRs. In case it is more desired to couple the API changes together with the implementation, please let me know. Which issue(s) this PR fixes: Fixes OTA-1405( Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - x This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "OTA-1393": {
                "summary": "status: recognize the process of migration to multi-arch",
                "description": "After OTA-960 is fixed, ClusterVersion/version and oc adm upgrade can be used to monitor the process of migrating a cluster to multi-arch. {noformat} $ oc adm upgrade info: An upgrade is in progress. Working towards 4.18.0-ec.3: 761 of 890 done (85% complete), waiting on machine-config Upgradeable=False Reason: PoolUpdating Message: Cluster operator machine-config should not be upgraded between minor versions: One or more machine config pools are updating, please see `oc get mcp` for further details Upstream: Channel: candidate-4.18 (available channels: candidate-4.18) No updates available. You may still upgrade to a specific release image with --to-image or wait for new updates to be available. {noformat} But oc adm upgrade status reports COMPLETION 100% while the migration/upgrade is still ongoing. {noformat} $ OC_ENABLE_CMD_UPGRADE_STATUS=true oc adm upgrade status Unable to fetch alerts, ignoring alerts in 'Update Health': failed to get alerts from Thanos: no token is currently in use for this session = Control Plane = Assessment: Completed Target Version: 4.18.0-ec.3 (from 4.18.0-ec.3) Completion: 100% (33 operators updated, 0 updating, 0 waiting) Duration: 15m Operator Status: 33 Healthy Control Plane Nodes NAME ASSESSMENT PHASE VERSION EST MESSAGE ip-10-0-95-224.us-east-2.compute.internal Unavailable Updated 4.18.0-ec.3 - Node is unavailable ip-10-0-33-81.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - ip-10-0-45-170.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - = Worker Upgrade = WORKER POOL ASSESSMENT COMPLETION STATUS worker Completed 100% 3 Total, 2 Available, 0 Progressing, 0 Outdated, 0 Draining, 0 Excluded, 0 Degraded Worker Pool Nodes: worker NAME ASSESSMENT PHASE VERSION EST MESSAGE ip-10-0-72-40.us-east-2.compute.internal Unavailable Updated 4.18.0-ec.3 - Node is unavailable ip-10-0-17-117.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - ip-10-0-22-179.us-east-2.compute.internal Completed Updated 4.18.0-ec.3 - = Update Health = SINCE LEVEL IMPACT MESSAGE - Warning Update Speed Node ip-10-0-95-224.us-east-2.compute.internal is unavailable - Warning Update Speed Node ip-10-0-72-40.us-east-2.compute.internal is unavailable Run with --details=health for additional description and links to related online documentation $ oc get clusterversion version NAME VERSION AVAILABLE PROGRESSING SINCE STATUS version 4.18.0-ec.3 True True 14m Working towards 4.18.0-ec.3: 761 of 890 done (85% complete), waiting on machine-config $ oc get co machine-config NAME VERSION AVAILABLE PROGRESSING DEGRADED SINCE MESSAGE machine-config 4.18.0-ec.3 True True False 63m Working towards 4.18.0-ec.3 {noformat} The reason is that PROGRESSING=True is not detected for co/machine-config as the status command checks only and it needs to check ClusterOperator.Status.Versionsname==\"operator-image\" as well. For grooming: It will be challenging for the status command to check the operator image's pull spec because it does not know the expected value. CVO knows it because CVO holds the manifests (containing the expected value) from the multi-arch payload. One \"hacky\" workaround is that the status command gets the pull spec from the MCO deployment: {noformat} oc get deployment -n openshift-machine-config-operator machine-config-operator -o json .image' quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:787a505ca594b0a727549353c503dec9233a9d3c2dcd6b64e3de5f998892a1d5 {noformat} Note this co/machine-config - deployment/machine-config-operator trick may not be feasible if we want to extend it to all cluster operators. But it should work as a hacky workaround to check only MCO. We may claim that the status command is not designed for monitoring the multi-arch migration and suggest to use oc adm upgrade instead. In that case, we can close this card as Obsolete/Won'tDo. ^manifests.ziphas the mockData/manifests for the status cmd that are taken during the migration. oc1920| started the work for the status command to recognize the migration and we need to extend the work to cover (the comments from Petr's review): - \"Target Version: 4.18.0-ec.3 (from 4.18.0-ec.3)\": confusing. We should tell \"multi-arch\" migration somehow. Or even better: from the current arch to multi-arch, for example \"Target Version: 4.18.0-ec.3 multi (from x86_64)\" if we could get the origin arch from CV or somewhere else. -- We have spec.desiredUpdate.architecture since forever, and can use that being Multi as a partial hint. MULTIARCH-4559 is adding tech-preview status properties around architecture in 4.18, but tech-preview, so may not be worth bothering with in oc code. Two history entries with the same version string but different digests is probably a reliable-enough heuristic, coupled with the spec-side hint. - \"Duration: 6m55s (Est. Time Remaining: 1h4m)\": We will see if we could find a simple way to hand this special case. I do not understand \"the 97% completion will be reached so fast.\" as I am not familiar with the algorithm. But it seems acceptable to Petr that we show N/A for the migration. -- I think I get \"the 97% completion will be reached so fast.\" now as only MCO has the operator-image pull spec. Other COs claim the completeness immaturely. With that said, \"N/A\" sounds like the most possible way for now. - Node status like \"All control plane nodes successfully updated to 4.18.0-ec.3\" for control planes and \"ip-10-0-17-117.us-east-2.compute.internal Completed\". It is technically hard to detect the transaction during migration as MCO annotates only the version. This may become a separate card if it is too big to finish with the current one. - \"targetImagePullSpec := getMCOImagePullSpec(mcoDeployment)\" should be computed just once. Now it is in the each iteration of the for loop. We should also comment about why we do it with this hacky way.",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1933",
                        "type": "pullRequest",
                        "title": "OTA-1393: status: recognize multi-arch in node phase (2)",
                        "body": "It turns out that MCO annotates( a node and updates it and only then moves on the next node. This means for example, the worker pool with 3 nodes has always `COMPLETION=67% (2/3)` during migration. I thought about use `mcp.status.updatedMachineCount/mcp.status.machineCount` for the migration case. With the mcp in our example( `COMPLETION=33% (1/3)` instead of `COMPLETION=67% (2/3)`. It would work ONLY for the case without the customized pools. To fix it properly, we might have to resort to the changes on MCO. `N/A` does not sound ideal but an admin might not be interested in the progress of existing worker nodes restarts as it is irrelevant to the goal of migrating a cluster to multi-arch: create a new pool of nodes with a secondary arch. As soon as the control planes upgrade completes, the cluster is ready for that. --- With the idea of we fixed the node phase, assessment, and the completion. --- We found a limitation that the status command claims \"not updating\" while the data plane update is still running (after the control plane update is complete). See the discussion( /cc @petr-muller @DavidHurta"
                    },
                    {
                        "id": "1928",
                        "type": "pullRequest",
                        "title": "OTA-1393: status: recognize multi-arch in node phase",
                        "body": "Address I thought we would need something from the MCO side to recognize the transactions between node phases (as the operator image pull spec is a new thing). While thinking about it again, MCO must have known if it needs to update the nodes without the pull spec and found the annotations. I checked the machine configs for multi-arch migration (see the following bash output): `master` (`worker` respectively) has two with `4.18.0-ec.3` and each of them should be for before-and-after the update. The `status` command does not need to use these annotations as the version annotation( is enough until the multi-arch migration is under consideration because the version stays the same. ``` $ cat pkg/cli/admin/upgrade/status/examples/to-multi-arch-mc.yaml |select(.metadata.annotations\"machineconfiguration.openshift.io/release-image-version\"==\"4.18.0-ec.3\")' \"rendered-master-1f718b5bc3667e49797e9a47d9f48725\", \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f135a14056f31c9c21782ac194f527e358d59a67e20ccf75f08706cc68c8c980\", \"2024-11-20T23:24:40Z\" \"rendered-master-909adc791f502a35c20ba70158c4c582\", \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c3a4fec8e58e41a24e4bc15de9c1261b917448246d62b37dc83e4871837e3fbb\", \"2024-11-20T22:36:32Z\" \"rendered-worker-869799cd72c887a5da8248efd58bb2a9\", \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c3a4fec8e58e41a24e4bc15de9c1261b917448246d62b37dc83e4871837e3fbb\", \"2024-11-20T22:36:32Z\" \"rendered-worker-a079988a31206987bed827f0242d7b3f\", \"quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f135a14056f31c9c21782ac194f527e358d59a67e20ccf75f08706cc68c8c980\", \"2024-11-20T23:24:40Z\" ``` /cc @petr-muller @DavidHurta"
                    },
                    {
                        "id": "1926",
                        "type": "pullRequest",
                        "title": "OTA-1393: status: recognize multi-arch in target version",
                        "body": "Address and /cc @petr-muller @DavidHurta"
                    },
                    {
                        "id": "1924",
                        "type": "pullRequest",
                        "title": "OTA-1393: status: compute mcoImagePullSpec only once",
                        "body": "To address /cc @petr-muller @DavidHurta"
                    }
                ]
            },
            "OTA-1269": {
                "summary": "Scaffold the status API controller",
                "description": "On the call to discuss oc adm upgrade status roadmap to server side-implementation (notes) we agreed on basic architectural direction and we can starting moving in that direction: - status API will be backed by a new controller - new controller will be a separate binary but delivered in the CVO image (=release payload) to avoid needing new ClusterOperator - new operator will maintain a singleton resource of a new UpgradeStatus CRD - this is the interface to the consumers Let's start building this controller; we can implement the controller perform the functionality currently present in the client, and just expose it through an API. I am not sure how to deal with the fact that we won't have the API merged until it merges into o/api, which is not soon. Maybe we can implement the controller over a temporary fork of o/api and rely on manually inserting the CRD into the cluster when we test the functionality? Not sure. We need to avoid committing to implementation details and investing effort into things that may change though. Definition of Done - (/) CVO repository has a new controller (a new cluster-version-operator cobra subcommand sounds like a good option; an alternative would a completely new binary included in CVO image) - (/) The payload contains manifests (SA, RBAC, Deployment) to deploy the new controller when DevPreviewNoUpgrade feature set is enabled (but not TechPreview) - (/) The controller uses properly scoped minimal necessary RBAC through a dedicated SA - (/) The controller will react on ClusterVersion changes in the cluster through an informer - (/) The controller will maintain a single ClusterVersion status insight as specified by the Update Health API Draft together with the necessary generated code (like deepcopy). These local types will need to be replaced with the types eventually merged into o/api and vendored to o/cluster-version-operator Testing notes This card only brings a _skeleton_ of the desired functionality to the DevPreviewNoUpgrade feature set. Its purpose is mainly to enable further development by putting the necessary bits in place so that we can start developing more functionality. There's not much point in automating testing of any of the functionality in this card, but it should be useful to start getting familiar with how the new controller is deployed and what are its concepts. For seeing the new controller in action: 1. Launch a cluster that includes both the code and manifests. As of Nov 11, 1107 is not yet merged so you need to use launch 4.18,openshift/cluster-version-operator1107 aws,no-spot 2. Enable the DevPreviewNoUpgrade feature set. CVO will restart and will deploy all functionality gated by this feature set, including the USC. It can take a bit of time, ~10-15m should be enough though. 3. Eventually, you should be able to see the new openshift-update-status-controller Namespace created in the cluster 4. You should be able to see a update-status-controller Deployment in that namespace 5. That Deployment should have one replica running and being ready. It should not crashloop or anything like that. You can inspect its logs for obvious failures and such. At this point, its log should, near its end, say something like \"the ConfigMap does not exist so doing nothing\" 6. Create the ConfigMap that mimics the future API (make sure to create it in the openshift-update-status-controller namespace): oc create configmap -n openshift-update-status-controller status-api-cm-prototype 7. The controller should immediately-ish insert a usc-cv-version key into the ConfigMap. Its content is a YAML-serialized ClusterVersion status insight (see design doc| As of OTA-1269 the content is not _that_ important, but the (1) reference to the CV (2) versions field should be correct. 8. The status insight should have a condition of Updating type. It should be False at this time (the cluster is not updating). 9. Start upgrading the cluster (it's a cluster bot cluster with ephemeral 4.18 version so you'll need to use --to-image=pullspec and probably force it 10. While updating, you should be able to observe the controller activity in the log (it logs some diffs), but also the content of the status insight in the ConfigMap changing. The versions field should change appropriately (and startedAt too), and the Updating condition should become True. 11. Eventually the update should finish and the Updating condition should flip to False again. Some of these will turn into automated testcases, but it does not make sense to implement that automation while we're using the ConfigMap instead of the API.",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1107",
                        "type": "pullRequest",
                        "title": "OTA-1269: USC: Add `DevPreviewNoUpgrade`-gated manifests",
                        "body": "- x Builds on: - x Blocked by: (fails hypershift without that change) --- - Namespace - ServiceAccount - Role that allows read, watch & update ConfigMaps (+binding) - ClusterRole that allows read & watch ConfigVersions (+rolebinding) - Deployment Squashed review commits: USC: Do not tolerate taints The original Deployment was copied from CVO which contained a bunch of strong tolerations. It does not seem USC needs to be this robust against node conditions and can be easily evicted if necessary. USC: Use `openshift-user-critical` priority class The original USC deployment was copied from CVO which is obviously very important component and should be protected from OOM kills and preemption but USC does not seem to need it. If it is fine for your operator/operand to be preempted by user-workload and OOMKilled use openshift-user-critical priority class"
                    }
                ]
            },
            "OTA-1029": {
                "summary": "Enhancement proposal to allow changing the log level of CVO using API configuration",
                "description": "We need to send an enhancement proposal that would contain the design changes we suggest in openshift/api/config/v1/types_cluster_version.go Merged in the API repository.",
                "epic_key": "OTA-923",
                "GITHUB": [
                    {
                        "id": "2044",
                        "type": "pullRequest",
                        "title": "OTA-1029: Add ClusterVersionOperator API",
                        "body": "This pull request will introduce the `ClusterVersionOperator` API in the `DevPreviewNoUpgrade` feature set as described in the enhancement."
                    }
                ]
            },
            "OTA-1488": {
                "summary": "Remove USC manifests from DevPreview payload content and related tests",
                "description": "For the effort to deliver the UpdateStatus API in 4.19 we made DevPreview OCP deploy the Update Status Controller skeleton via payload manifests: new namespace, service account, rbac and deployment. We are not going to ship any meaningful UpdateStatus API implementation in 4.19, so we should remove all these manifests so that no useless ballast is deployed on 4.19 clusters after GA, even in DevPreview. We also have CI jobs / tests that exercise USC deployment in DevPreview; these need to be removed / disabled as well.",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1176",
                        "type": "pullRequest",
                        "title": "OTA-1488: Remove USC code",
                        "body": "For now we are not moving forward with the USC effort. It not necessary to build and include the code in the CVO binary, as well to carry the necessary docs. If we need ideas from USC we can find the code in Git history."
                    },
                    {
                        "id": "1175",
                        "type": "pullRequest",
                        "title": "OTA-1488: Remove DevPreview USC manifests",
                        "body": "We are shipping no USC functionality in 4.19, not even in DevPreview, so we should not include these manifests in the payload."
                    }
                ]
            },
            "OTA-1426": {
                "summary": "Extend tech-preview 'oc adm upgrade recommend' to render relevant alerts",
                "description": "oc adm upgrade recommend should retrieve alerts from the cluster (similar to how oc adm upgrade status already does), and inject them as conditional update risks (similar to how OTA-902 / cvo1907 injected Upgradeable issues). The set of alerts to include is: All critical alerts, because that's explicitly selected in OCPSTRAT-1834. This includes {{{}ClusterOperatorDown{}}}, which overlaps with the existing ClusterVersion Failing condition which is part of PDB coverage. PDB coverage: warning PodDisruptionBudgetAtLimit Nodes: warning KubeNodeNotReady and KubeNodeUnreachable| Definition of done / test-plan: Find a cluster with both update recommendations and some of the mentioned alerts firing. Run {{{}OC_ENABLE_CMD_UPGRADE_RECOMMEND=true OC_ENABLE_CMD_UPGRADE_RECOMMEND_PRECHECK=true oc adm upgrade recommend{}}}. Confirm that the command calls out the expected alerts for all update targets.",
                "epic_key": "OTA-1422",
                "GITHUB": [
                    {
                        "id": "1970",
                        "type": "pullRequest",
                        "title": "OTA-1426: pkg/cli/admin/upgrade: Check and display interesting alerts",
                        "body": "Retrieving alerts and rendering the interesting ones, as described in OTA-1426( See the individual commit messages for me trying to walk through building this in steps, and the new `pkg/cli/admin/upgrade/recommend/examples/4.16.27-degraded-monitoring.output` for the most direct example of the new logic."
                    }
                ]
            },
            "OTA-1411": {
                "summary": "USC: Maintain status insights for ClusterOperator resources",
                "description": "Extend the Control Plane Informer in the Update Status Controller so it watches ClusterOperator resources in the cluster and maintains an update status insight for each. The actual API structure for an update status insights needs to be taken from whatever state is at the moment. The story does not include the actual API form nor how it is exposed by the cluster (depending on the state of API approval, the USC may still expose the API as a ConfigMap or an actual custom resource), it includes just the logic to watch ClusterOperator resources and producing a matching set of cluster operator status insights. The basic expectations for cluster operator status insights are described in the design docs like we have in the prototype) ClusterOperator in the cluster - Outside of control plane update, the cluster operator status insights are not present - Updating condition: -- If a CO is updating right now (Progressing=True && does not have a target operator version), then Updating=True and a suitable reason -- Otherwise, if it has target version, Updating=False and Reason=Updated -- Otherwise, if it does not have target version, Updating=False and Reason=Pending - Healthy condition -- Corresponds to the existing checks in the client prototype, taking into account the thresholds (Healthy=True if the \"bad\" condition is not holding long enough yet, but we may invent a special Reason for this case) - This card does not involve creating Health Insights when there are unhealthy operators - This card does not involve updating a ClusterVersion status insight from the CO-related data (such as completeness percentage or duration estimate)",
                "epic_key": "OTA-1260",
                "GITHUB": [
                    {
                        "id": "1135",
                        "type": "pullRequest",
                        "title": "OTA-1411: USC: Maintain status insights for ClusterOperator resources"
                    }
                ]
            },
            "OTA-1403": {
                "summary": "Create e2e test/s to ensure that the CVO is correctly reconciling the new configuration API in standalone",
                "description": "Feature gates must demonstrate completeness and reliability. As per ??Tests must contain either OCPFeatureGate:FeatureGateName or the standard upstream FeatureGate:FeatureGateName.?? ??There must be at least five tests for each FeatureGate.?? ??Every test must be run on every TechPreview platform we have jobs for. (Ask for an exception if your feature doesn't support a variant.)?? ??Every test must run at least 14 times on every platform/variant.?? ??Every test must pass at least 95% of the time on every platform/variant.?? ??If your FeatureGate lacks automated testing, there is an exception process that allows QE to sign off on the promotion by commenting on the PR.?? The introduced functionality is not that complex. The only newly introduced ability is to modify the CVO log level using the API. However, we should still introduce an e2e test or tests to demonstrate that the CVO correctly reconciles the new configuration API. The tests may be: Check whether the CVO notices a new configuration in a reasonable time. Check whether the CVO increments the observedGeneration correctly. Check whether the CVO changes its log level correctly. TODO: Think of more cases. Definition of Done: e2e test/s exists to ensure that the CVO is correctly reconciling the new configuration API",
                "epic_key": "OTA-923",
                "GITHUB": [
                    {
                        "id": "1172",
                        "type": "pullRequest",
                        "title": "OTA-1403: Add an always passing test case",
                        "body": "Will rebase after gets in."
                    },
                    {
                        "id": "1168",
                        "type": "pullRequest",
                        "title": "OTA-1403: Create minimal openshift tests extension",
                        "body": "Create a minimal Cluster Version Operator OpenShift tests extension. For more information regarding the extension, see openshift-eng/openshift-tests-extension( repository or the respective enhancement( This pull request references --- Following PRs will: - Include new targets in the Makefile. - `update` to run `_output/$OS/$ARCH/openshift-tests-extension update` this generates a file that contains metadata about existing tests, the `update` generates an error if some invalid changes were made - `verify` to verify no changes are present after the `update` invocation - Create a new job to run the `verify` on presubmits - Create the first test in the extension - Update the Dockerfile to include the extension binary in a payload image - Register the extension in origin - Add remaining tests"
                    },
                    {
                        "id": "1169",
                        "type": "pullRequest",
                        "title": "OTA-1403: Move the CVO CLI Application to a Separate Folder",
                        "body": "So that we can have multiple CLI applications in the `cmd` folder. For example, `cmd/cluster-version-operator/`, `cmd/cvo-tests/`, `cmd/cluster-version-operator-v2/`. Being done to tidy the folder for"
                    }
                ]
            },
            "OTA-1307": {
                "summary": "ClusterVersion status should include version-Pod error details",
                "description": "Currently the CVO launches a Job and waits for it to complete| to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so we probably want to drop it and create and monitor the Pod ourselves. Definition of done: failure modes like unretrievable image digests (e.g. quay.io/openshift-release-dev/ocp-release@sha256:0000000000000000000000000000000000000000000000000000000000000000) or images with missing or unacceptable Sigstore signatures with OTA-1304's ClusterImagePolicy) have failure-mode details in ClusterVersion's RetrievePayload message, instead of the current Job was active longer than specified deadline. Not clear to me what we want to do with reason, which is currently DeadlineExceeded. Keep that? Split out some subsets like SignatureValidationFailed and whatever we get for image-pull-failures? Other?",
                "epic_key": "OTA-1321",
                "GITHUB": [
                    {
                        "id": "1105",
                        "type": "pullRequest",
                        "title": "OTA-1307: pkg/cvo/updatepayload: Drop the Job controller for release-manifests downloads",
                        "body": "Previously, the CVO launched a Job and waited for it to complete to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so we're dropping it and monitoring the Pod ourselves."
                    }
                ]
            },
            "OTA-861": {
                "summary": "CVO allows new unforced updates even when it is currently midway through a partial update. It should require a force to retarget mid-update",
                "description": "Moving the bug to this Jira card Currently we had a customer that triggered the upgrade from 4.1.27 to 4.3, having intermediate versions on 4.2 in partial state. We have asked for details of the CVO from the customer to understand better the procedure taken, but we might need to implement a way to either stop the upgrade in case customer makes a mistake or block the upgrade if the customer changes the channel on the console to a version which the upgrade does not support, like in this case. As per OTA - Inhibit minor version upgrades when an upgrade is in progress We should inhibit minor version upgrades via Upgradeable=False whenever an existing upgrade is in progress. This prevents retargetting of upgrades before we've reached a safe point. Imagine: Be running 4.6.z. Request an update to 4.7.z'. CVO begins updating to 4.7.z'. CVO requests recommended updates from 4.7.z', and hears about 4.8.z\". User accepts recommended update to 4.8.z\" before the 4.7.z' OLM operator had come out to check its children's max versions against 4.8 and set Upgradeable=False. Cluster core hits 4.8.z\" and some OLM operators fail on compat violations. This should not inhibit further z-stream upgrades, but we should be sure that we catch the case of 4.6.z to 4.7.z to 4.7.z+n to 4.8.z whenever 4.7.z was not marked as Complete. Update: Eventually, the output of this card: y-then-y: blocked (originally required in this card description). z-then-y: blocked (not originally required in this card description but during the implementation we think it is good to have). y-then-z or z-then-z: accepted because we need to always allow z-upgrade to include fixes.",
                "GITHUB": [
                    {
                        "id": "1093",
                        "type": "pullRequest",
                        "title": "OTA-861: Generate an accepted risk for Y-then-Z upgrade",
                        "body": "This one will be rebased after is merged."
                    }
                ]
            },
            "OTA-209": {
                "summary": "Enable the CVO in standalone OpenShift to change its log level based on the new API",
                "description": "The ClusterVersionOperator API has been introduced in the DevPreviewNoUpgrade feature set. Enable the CVO in standalone OpenShift to change its log level based on the new API. Definition of Done: New DevPreviewNoUpgrade manifests are introduced in the OCP payload The CVO is correctly reconciling the new CR and all of its new fields The CVO is changing its log level based on the log level in the new API",
                "epic_key": "OTA-923",
                "GITHUB": [
                    {
                        "id": "2162",
                        "type": "pullRequest",
                        "title": "OTA-209: operator/v1alpha1: Add the `ClusterVersionOperator` to scheme",
                        "body": "operator/v1alpha1: Add the `ClusterVersionOperator` to scheme I forgot to register the new type in the scheme when adding the `ClusterVersionOperator` API in This pull request references"
                    },
                    {
                        "id": "1163",
                        "type": "pullRequest",
                        "title": "OTA-209: Add CVOConfiguration controller",
                        "body": "The controller has an empty reconciliation logic as of now. The logic will be implemented in a follow-up PR. The goal of this PR is to introduce a new CVO controller, which can optionally (depending on the state of the `CVOConfiguration` feature gate) create a new informer."
                    },
                    {
                        "id": "1161",
                        "type": "pullRequest",
                        "title": "OTA-209: Add `ClusterVersionOperator` manifests",
                        "body": "will be broken down into smaller PRs. This the initial PR. For more information regarding the resources, see This pull request references"
                    },
                    {
                        "id": "1147",
                        "type": "pullRequest",
                        "title": "OTA-209: Bump `github.com/openshift/client-go`",
                        "body": "Bump the `github.com/openshift/client-go` module. The module was being bumped in the pull request; however, I have decided to separate the two efforts as the commits and issues were growing."
                    },
                    {
                        "id": "1148",
                        "type": "pullRequest",
                        "title": "OTA-209: Resolve lint issues",
                        "body": "These issues are not currently detected by the `ci/prow/lint` job as the job uses the version `v1.54.2` of the `golangci-lint` tool 1( It is needed to update the tool to support go `1.23`, which will be introduced in PR. However, updating the tool right now in would cause the `ci/prow/lint` presubmit to fail on PRs( Fix the lint issues in this PR. After that, we will update the `golangci-lint` tool. No disruptions to existing PRs will be introduced."
                    },
                    {
                        "id": "1130",
                        "type": "pullRequest",
                        "title": "OTA-209: Bump `github.com/openshift/api`",
                        "body": "Steps: ``` $ go get github.com/openshift/api $ go mod tidy $ go mod vendor $ go mod verify ``` So that we can use the new `ClusterVersionOperator` API."
                    }
                ]
            }
        },
        "epics": {
            "OTA-1260": {
                "summary": "Status API for oc adm upgrade status command",
                "description": "Epic Goal Add a new command `oc adm upgrade status` command which is backed by an API. Please find the mock output of the command output attached in this card. Why is this important? (mandatory) From the UI we can see the progress of the update. Using OC CLI we can see some of the information using \"oc get clusterversion\" but the output is not readable and it is a lot of extra information to process. Customer as asking us to show more details in a human-readable format as well provide an API which they can use for automation. Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
            },
            "OTA-923": {
                "summary": "Reduce cluster version operator logging",
                "description": "Epic Goal What is our purpose in implementing this? What new capability will be available to customers? Why is this important? (mandatory) What are the benefits to the customer or Red Hat? Does it improve security, performance, supportability, etc? Why is work a priority? Scenarios (mandatory) Provide details for user scenarios including actions to be performed, platform specifications, and user personas. Dependencies (internal and external) (mandatory) What items must be delivered by other teams/groups to enable delivery of this epic. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - Documentation - QE - PX - Others - Acceptance Criteria (optional) Provide some (testable) examples of how we will know if we have achieved the epic goal. Drawbacks or Risk (optional) Reasons we should consider NOT doing this such as: limited audience for the feature, feature will be superseded by other work that is planned, resulting feature will introduce substantial administrative complexity or user confusion, etc. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
            },
            "OTA-1422": {
                "summary": "Extend tech-preview 'oc adm upgrade recommend' to render relevant alerts",
                "description": "Epic Goal OCPSTRAT-1834 is requesting an oc precheck command that helps customers identify potential issues before triggering an OpenShift cluster upgrade. For 4.18, we'd built a tech-preview oc adm upgrade recommend (OTA-1270, product docs?\" space, and this Epic is about extending that subcommand with alerts to deliver the coverage requested by OCPSTRAT-1834. Why is this important? We currently document some manual checks for customer admins to run before launching an update. For example, RFE-5104 is up asking to automate whatever we're hoping customer are supposed to look for vs. critical alerts. But updating the production OpenShift Update Service is complicated, and it's easier to play around in a tech-preview oc subcommand, while we get a better idea of what information is helpful, and which presentation approaches are most accessible. 4.18's OTA-902 / cvo1907 and this Epic proposes to continue in that direction by retrieving update-relevant alerts and folding those in as additional client-side Conditional Update risks. Scenarios As a cluster administrator interested in launching an OCP update, I want to run an oc command that talks to me about my next-hop options, including any information related to known regressions with those target releases, and also including any information about things I should consider addressing in my current cluster state. Dependencies The initial implementation can be delivered unilaterally by the OTA updates team. The implementation may surface ambiguous or hard-to-actuate alert messages, and those messages will need to be improved by the component team responsible for maintaining that alert. Contributing Teams (and contacts) Development - OTA Documentation - no docs required QE - OTA PX - OTA Others - Acceptance Criteria OCPSTRAT-1834 customer is happy :) Drawbacks or Risk Client-side Conditional Update risks are helpful for cluster administrators who use that particular client. But admins who use older oc or who are using the in-cluster web-console and similar will not see risks known only to newer oc. If we can clearly tie a particular cluster state to update risk, declaring that risk via the OpenShift Update Service would put the information in front of all cluster administrators, regardless of which update interface they use. However, trialing update risks client-side in tech-preview oc and then possibly promoting them to risks served by the OpenShift Update Service in the future might help us identify cluster state that's only weakly coupled to update success but still interesting enough to display. Or help us find more accessible ways of displaying that context before putting the message in front of large chunks of the fleet. Done - Checklist CI Testing - Tests are merged and completing successfully Documentation - No docs. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
            },
            "OTA-1432": {
                "summary": "oc precheck command for oc cli",
                "description": "Epic Goal oc precheck is a command that the customer can run pre-update in 4.16 and above clusters to check for a limited number of conditions that might cause issues in upgrading the cluster. this command will create a report that the customer can read before they start the upgrade. the command output will NOT make any decisions regarding to proceed with the upgrade or not. That decision should be taken by the customer after reading the report. Why is this important? (mandatory) The customer requires a command that they can run on the cluster that prints a report of cluster conditions that might affect the upgrade process. Scenarios (mandatory) customer uses oc precheck command to check for cluster conditions that can hinder the upgrade process like strict pdb, etc Dependencies (internal and external) (mandatory) this is entirely an OTA team undertaking. Contributing Teams(and contacts) (mandatory) Our expectation is that teams would modify the list below to fit the epic. Some epics may not need all the default groups but what is included here should accurately reflect who will be involved in delivering the epic. Development - OTA Documentation - OTA docs team QE - OTA PX - Others - Acceptance Criteria (optional) OTA team ships a `oc precheck` command to check for cluster conditions. Drawbacks or Risk (optional) We can check for frequent conditions that cause issues with upgrades, but we cannot guarantee that the cluster upgrade will go smoothly even after the customer reads the report of this command as every cluster is different and there are lot of unknowns. Done - Checklist (mandatory) The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
            },
            "OTA-1321": {
                "summary": "ClusterVersion status should include version-Pod error details",
                "description": "Epic Goal Currently the CVO launches a Job and waits for it to complete to get manifests for an incoming release payload. But the Job controller doesn't bubble up details about why the pod has trouble (e.g. Init:SignatureValidationFailed), so to get those details, we need direct access to the Pod. The Job controller doesn't seem like it's adding much value here, so the goal of this Epic is to drop it and create and monitor the Pod ourselves, so we can deliver better reporting of version-Pod state. Why is this important? When the version Pod fails to run, the cluster admin will likely need to take some action (clearing the update request, fixing a mirror registry, etc.). The more clearly we share the issues that the Pod is having with the cluster admin, the easier it will be for them to figure out their next steps. Scenarios oc adm upgrade and other ClusterVersion status UIs will be able to display Init:SignatureValidationFailed and other version-Pod failure modes directly. We don't expect to be able to give ClusterVersion consumers more detailed next-step advice, but hopefully the easier access to failure-mode context makes it easier for them to figure out next-steps on their own. Dependencies This change is purely and updates-team/OTA CVO pull request. No other dependencies. Contributing Teams Development - OTA Documentation - OTA QE - OTA Acceptance Criteria Definition of done: failure modes like unretrievable image digests (e.g. quay.io/openshift-release-dev/ocp-release@sha256:0000000000000000000000000000000000000000000000000000000000000000) or images with missing or unacceptable Sigstore signatures with OTA-1304's ClusterImagePolicy) have failure-mode details in ClusterVersion's RetrievePayload message, instead of the current Job was active longer than specified deadline. Drawbacks or Risk Limited audience, and failures like Init:SignatureValidationFailed are generic, while CVO version-Pod handling is pretty narrow. This may be redundant work if we end up getting nice generic init-Pod-issue handling like RFE-5627. But even if the work ends up being redundant, thinning the CVO stack by removing the Job controller is kind of nice. Done - Checklist The following points apply to all epics and are what the OpenShift team believes are the minimum set of criteria that epics should meet for us to consider them potentially shippable. We request that epic owners modify this list to reflect the work to be completed in order to produce something that is potentially shippable. CI Testing - Tests are merged and completing successfully Documentation - Content development is complete. QE - Test scenarios are written and executed successfully. Technical Enablement - Slides are complete (if requested by PLM) Other"
            }
        }
    },
    "OpenStack as Infra": {
        "stories": {
            "OSASINFRA-3731": {
                "summary": "Consume CA bundle from CCO-provisioned credential secret in csi-operator, cluster-storage-operator",
                "description": "Since 4.18, the csi-operator contains the operators and manifests for both the Cinder and the Manila CSI Drivers. We should modify this component to consume the CA cert from the credentials secrets created by cloud-credential-operator (CCO) in response to CredentialsRequest CRs. We may wish to leave the legacy syncing code that copies the CA bundle from the openshift-cloud-controller-manager / cloud-conf config map until 4.20, to allow time for CCO to do its job and sync everything across.",
                "GITHUB": [
                    {
                        "id": "557",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3731: openstack: Consume CA cert from CCO secret",
                        "body": "In openshift/cloud-credential-operator/pull/780, we have added the ability for `cloud-credential-operator` to consume a CA cert from the root credentials secret and to include in the credentials secrets it provisions. In openshift/installer/pull/9194, we have modified the Installer to start setting this field where necessary. Adapt the assets for both the openstack-cinder and openstack-manila CSI drivers to start consuming this field, where present. We maintain fallbacks for the previous locations of the cert for now, but these can be removed in the next release. This needs wait for the CCO change to be approved before we merge this. /hold"
                    }
                ]
            },
            "OSASINFRA-3730": {
                "summary": "Add CA bundle to root credential secret created by installer",
                "description": "The Installer creates the initial version of the root credential secret at kube-system / openstack-credentials, which cloud-credential-operator (CCO) will consume. Once we have support in CCO for consuming a CA cert from this root credential, we should modify the Installer to start populating the CA cert field. We should also stop adding the CA cert to the openshift-cloud-controller-manager / cloud-conf config map since the Cloud Config Operator (and CSI Drivers) will be able to start consuming the CA cert from the secret instead. This may need to be done separately depending on the order that patches land in.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "9194",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3730: Add support for storing OpenStack CA bundles",
                        "body": "If a CA bundle is required to talk to your OpenStack then obviously all services that talk to the cloud need to have both credentials and said bundle. Currently, these users can get their credentials via cloud credential operator, but they need to source their CA bundle from elsewhere (typically by extracting it from the cloud controller manager's configuration). This makes configuration of services more complicated than necessary. Continue the resolution of the issue by storing the CA bundle, if any, in the root secret on OpenStack. When coupled with the changes introduced in openshift/cloud-credential-operator780, this allows us to dole out the bundle to anyone who asks for it via a `CredentialsRequest`. ~While we're here, we also tweak the configuration for the cloud provider to (a) start generating the configuration file in the new format expected by `cluster-cloud-controller-manager-operator` and (b) stop generating an old secret that only the old, now-removed in-tree OpenStack cloud provider needed and used.~ EDIT: I have deferred the above changes to a different PR."
                    }
                ]
            },
            "OSASINFRA-3755": {
                "summary": "Consume CA bundle from CCO-provisioned credential secret in cloud-network-config-controller",
                "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify cloud-network-config-controller to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "167",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3755: Prefer CA cert from credentials secret",
                        "body": "In openshift/cloud-credential-operator/pull/780, we have added the ability for `cloud-credential-operator` to consume a CA cert from the root credentials secret and to include in the credentials secrets it provisions. In openshift/installer/pull/9194, we have modified the Installer to start setting this field where necessary. Adapt `cloud-network-config-operator` to allow it to start consuming the CA cert from this place. We maintain fallbacks for the previous locations of the cert for now, but these can be removed in the next release."
                    }
                ]
            },
            "OSASINFRA-3747": {
                "summary": "Consume CA bundle from CCO-provisioned credential secret in cluster-image-registry-operator",
                "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify cluster-image-registry-operator to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map when Swift configuration is enabled.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "1190",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3747: Prefer CA cert from credentials secret",
                        "body": "In openshift/cloud-credential-operator/pull/780, we have added the ability for `cloud-credential-operator` to consume a CA cert from the root credentials secret and to include in the credentials secrets it provisions. In openshift/installer/pull/9194, we have modified the Installer to start setting this field where necessary. Adapt `cluster-image-registry-operator` to allow it to start consuming the CA cert from this place. We maintain fallbacks for the previous locations of the cert for now, but these can be removed in the next release. Dependencies: - x"
                    }
                ]
            },
            "OSASINFRA-3746": {
                "summary": "Consume CA bundle from CCO-provisioned credential secret in machine-api-provider-openstack",
                "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify MAPO to consume the CA cert from this location rather than from the openshift-config / cloud-provider-config config map. This has the added bonus of aligning MAPO with CAPO's behavior.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "131",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3746: Consume CA cert from CCO secret",
                        "body": "In openshift/cloud-credential-operator/pull/780, we have added the ability for `cloud-credential-operator` to consume a CA cert from the root credentials secret and to include in the credentials secrets it provisions. In openshift/installer/pull/9194, we have modified the Installer to start setting this field where necessary. Adapt MAPO to allow it to start consuming the CA cert from this place. We maintain fallbacks for the previous locations of the cert for now, but these can be removed in the next release. This needs wait for the CCO change to be approved before we merge this. /hold"
                    }
                ]
            },
            "OSASINFRA-3732": {
                "summary": "Publish CA cert to well-known location in hypershift",
                "description": "In OSASINFRA-3657 we have modified cloud-credential-operator to consume a CA cert from the root credentials secret and include it in the generated credentials secrets under a new key. We should now modify Hypershift to use the same secret key, cacert, rather than the one it currently uses, cabundle.pem.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "5702",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3732: openstack: Sync CA cert to new key",
                        "body": "!-- - Please ensure code changes are split into a series of logically independent commits. - Every commit should have a subject/title (What) and a description/body (Why). - Every PR must have a description. - As an example you can use git commit -m\"What\" -m\"Why\" to achieve the requirements above. GitHub automatically recognises the commit description (-m\"Why\") in single commit PRs and adds it as the PR description. - Use the imperative mood( in the subject line for every commit. E.g `Mark infraID as required` instead of `This patch marks infraID as required` (This follows Git\u2019s own built-in conventions). See as an example. - See for more details. Delete this text before submitting the PR. -- What this PR does / why we need it: cloud-credential-operator now supports syncing CA certs from the root credential secret to the generated credentials secrets. If necessary, CCO expects the CA cert to be provided in the `cacert` key and will place it in the same location in the generated secrets. Start doing the same in control-plane-operator, which allows us to significantly simplify the assets used in cluster-storage-operator and csi-operator. Note that we are intentionally not changing how CA certs are managed for cluster-cloud-controller-manager-operator. There's a good reason for this, and a note is left inline to that effect. /hold Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Per $subject. Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "OSASINFRA-3657": {
                "summary": "Sync CA bundle from root credential to generated credentials",
                "description": "The cloud-credential-operator (CCO) consumes a root credential secret from kube-system / openstack-credentials and rolls it out to secrets in different namespaces, in response to CredentialsRequests CRs. We should modify CCO to start looking for a cacert field in this root credential secret and copying it to the generated secrets. This key name is chosen since it aligns with the expected secret key name used by components like CAPO and ORC.",
                "epic_key": "OSASINFRA-3729",
                "GITHUB": [
                    {
                        "id": "780",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3657: Add support for storing OpenStack CA bundles",
                        "body": "If a CA bundle is required to talk to your OpenStack then obviously all services that talk to the cloud need to have both credentials and said bundle. Currently, these users can get their credentials via cloud credential operator, but they need to source their CA bundle from elsewhere (typically by extracting it from the cloud controller manager's configuration). This makes configuration of services more complicated than necessary. Begin the resolution of the issue by allowing users (i.e. the Installer) to store the CA bundle in their root secret and dole this out to anyone who asks for it via a `CredentialsRequest`. Follow-up changes will be needed in places like the Installer and csi-operator to start setting/consuming this."
                    }
                ]
            },
            "OSASINFRA-3652": {
                "summary": "Remove configuration of '--feature-gates=Topology' opt",
                "description": "We currently set this for Cinder in the csi-operator. We should remove it once we have a new mechanism implemented for enabling/disable the topology feature.",
                "epic_key": "OSASINFRA-3650",
                "GITHUB": [
                    {
                        "id": "345",
                        "type": "pullRequest",
                        "title": "OSASINFRA-3652: openstack-cinder: Use new --with-topology flag",
                        "body": "Rather than relying on the soon-to-be-removed feature gate. /hold ~This requires EDIT: This is now available on the `release-4.18` and `release-4.19` branches"
                    }
                ]
            }
        },
        "epics": {
            "OSASINFRA-3729": {
                "summary": "Sync CA bundle to credentials",
                "description": "Goal Add support for syncing CA bundle to the credentials generated by Cloud Credential Operator. Why is this important? It it generally necessary to provide a CA file to OpenStack clients in order to communicate with a cloud that uses self-signed certificates. The cloud-credential-operator syncs clouds.yaml files to various namespaces so that services running in those namespaces are able to communicate with the cloud, but it does not sync the CA file. Instead, this must be managed using another mechanism. This has led to some odd situations, such as the Cinder CSI driver operator inspecting cloud-provider configuration to pull out this file. We should start syncing not only the clouds.yaml file but also the CA file to anyone that requests it via a CredentialsRequest. Once we've done this, we can modify other components such as the Installer, CSI Driver Operator, Hypershift, and CCM Operator to pull the CA file from the same secrets that they pull the clouds.yaml from, rather than the litany of places they currently use. Scenarios As a deployer, I should be able to update all cloud credential-related information - including certificates - in one central place and see these rolled out to all components that require them. Acceptance Criteria The cloud-credential-operator is capable of consuming a CA cert from kube-system / openstack-credentials and rolling this out to the secrets in other namespaces The installer includes the CA cert in the root kube-system / openstack-credentials secret The UPI playbooks are modified to includes the CA cert in the root kube-system / openstack-credentials secret No regressions. Since we use self-signed certificates in many of our CI systems, we should see regressions early. Release notes and credential rotation documentation is updated to document this change Dependencies (internal and external) None. Previous Work (Optional): None. Open questions: None."
            },
            "OSASINFRA-3650": {
                "summary": "Add topology-awareness to Cinder CSI Driver",
                "description": "Goal The Cinder CSI driver reports the VOLUME_ACCESSIBILITY_CONSTRAINTS plugin capability, meaning it supports Topology-Aware Volume Provisioning, as described in the k8s CSI docs to cover much of this and prevent regressions. We will need to manually test the negative case, where there is a mismatch between the set of Cinder AZs and set of Nova AZs, but this should be trivial to do. Open questions: None."
            }
        }
    },
    "Operator Runtime": {
        "stories": {
            "OPRUN-3821": {
                "summary": "Modify cluster-olm-operator to watch for the new Own/SingleNamespace InstallMode feature gate",
                "description": "Modify cluster-olm-operator to watch for the new Own/SingleNamespace InstallMode feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
                "epic_key": "OPRUN-3748",
                "GITHUB": [
                    {
                        "id": "110",
                        "type": "pullRequest",
                        "title": "OPRUN-3821: Add FeatureFlag for OLMv1 Single/OwnNamespace",
                        "body": "Preconditions: - Changes: - adds support for feature flag for Single/OwnNamespace installation handling"
                    }
                ]
            },
            "OPRUN-3783": {
                "summary": "Create origin tests to test preflight permission Techpreview",
                "description": "Add origin tests in the openshift/| repo using the featureGate to ensure tests run/don't run when the FeatureGate(s) are enabled/disabled. See dev guide for more info:",
                "epic_key": "OPRUN-3613",
                "GITHUB": [
                    {
                        "id": "29730",
                        "type": "pullRequest",
                        "title": "OPRUN-3783: OLMv1: Add support for preflight permissions checks (fixed)",
                        "body": "Adds several data files, of which -0 is the basic one that allows for installation, and -1 through -6 have minor removals to cause a failure. They are each combined with -base to attempt to install an operator. This reapplies cf207152efc99fb5474650649bddd43cea89c71b with fixes due to upstream-to-downstream merge."
                    }
                ]
            },
            "OPRUN-3780": {
                "summary": "Add TechPreviewNoUpgrade feature gate for permissions preflight to openshift/api",
                "description": "Add a new TechPreviewNoUpgrade feature gate for the permissions preflight feature as a PR to openshift/api (propose NewOLMPermissionsPreflight). See dev guide for more information:",
                "epic_key": "OPRUN-3613",
                "GITHUB": [
                    {
                        "id": "2242",
                        "type": "pullRequest",
                        "title": "OPRUN-3780: Add feature flag for NewOLMPreflightPermissionCheck"
                    }
                ]
            },
            "OPRUN-3692": {
                "summary": "Add openshift/origin tests for catalogd web api feature gate",
                "description": "Add origin tests in the openshift/| repo using the featureGate to ensure tests run/don't run when the FeatureGates are enabled/disabled on. See dev guide for more info: AC: - openshift/origin tests that exercise the feature gate",
                "epic_key": "OPRUN-3597",
                "GITHUB": [
                    {
                        "id": "29580",
                        "type": "pullRequest",
                        "title": "OPRUN-3692: Olmv1-catalogd tests for API endpoints",
                        "body": "Introduces tests for the new `api/v1/metas` endpoint when NewOLMCatalogdAPIV1Metas feature gate in enabled."
                    }
                ]
            },
            "OPRUN-3818": {
                "summary": "marketplace-operator: Update controller-runtime and consequently k8s api to latest (v0.20.3 and v0.32.3)",
                "description": "We should keep the marketplace operator updated.",
                "GITHUB": [
                    {
                        "id": "611",
                        "type": "pullRequest",
                        "title": "OPRUN-3818: Bump sigs.k8s.io/controller-runtime from 0.20.2 to 0.20.3",
                        "body": "!-- Upgrade controller-runtime and, consequently, k8s apis Description of the change: Motivation for the change: Reviewer Checklist - Implementation matches the proposed design, or proposal is updated to match implementation - Sufficient unit test coverage - Sufficient end-to-end test coverage - Docs updated or added to `/docs` - Commit messages sensible and descriptive !-- Note: If this PR is fixing an issue make sure to add a note saying: Closes ISSUE_NUMBER --"
                    }
                ]
            },
            "OPRUN-3782": {
                "summary": "Modify cluster-olm-operator to watch for new permissions preflight feature gate",
                "description": "Modify cluster-olm-operator to watch for the new permissions preflight feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
                "epic_key": "OPRUN-3613",
                "GITHUB": [
                    {
                        "id": "113",
                        "type": "pullRequest",
                        "title": "OPRUN-3782: Watch for permissions preflight feature gate",
                        "body": "Now with the proper name flag. Update to version of openshift/api with support."
                    },
                    {
                        "id": "111",
                        "type": "pullRequest",
                        "title": "OPRUN-3782: Watch for permissions preflight feature gate",
                        "body": "Update to version of openshift/api with support."
                    },
                    {
                        "id": "29714",
                        "type": "pullRequest",
                        "title": "OPRUN-3782: OLMv1: Add support for preflight permissions checks",
                        "body": "Adds several data files, of which -0 is the basic one that allows for installation, and -1 through -6 have minor removals to cause a failure. They are each combined with -base to attempt to install an operator."
                    }
                ]
            },
            "OPRUN-3766": {
                "summary": "Add TechPreviewNoUpgrade feature gate for Own/SingleNamespace InstallMode to openshift/api",
                "description": "Add a new TechPreviewNoUpgrade feature gate for the Own/SingleNamespace InstallMode feature. See dev guide for more information: AC: - openshift/api PR for adding a new feature gate with name NewOLMOwnSingleNamespace",
                "epic_key": "OPRUN-3748",
                "GITHUB": [
                    {
                        "id": "2264",
                        "type": "pullRequest",
                        "title": "OPRUN-3766: Add FeatureFlag for OLMv1 Single/OwnNamespace",
                        "body": "Adds the 'NewOLMOwnSingleNamespace' cluster capability. Clones 2258"
                    }
                ]
            },
            "OPRUN-3722": {
                "summary": "UPSTREAM Consolidate catalogd and operator-controller Kustomize configs 1341",
                "description": "Consolidate the catalogd and operator-controller kustomize configuration dirs. e.g. ``` $ tree -d config config \u251c\u2500\u2500 catalogd \u2514\u2500\u2500 operator-controller ``` Goal: We should be able to render separate overlays for each project. However, de-duplication of kustomize configuration files could be done.",
                "epic_key": "OPRUN-3703",
                "GITHUB": [
                    {
                        "id": "273",
                        "type": "pullRequest",
                        "title": "OPRUN-3722: Synchronize From Upstream Repositories",
                        "body": "The downstream repository has been updated through the following upstream commit: Commit Message - - operator-framework/operator-controller@7040ee2( Short( The `vendor/` directory has been updated and the following commits were carried: Commit Message - - openshift/operator-framework-operator-controller@d265155( &lt;carry&gt;: Add OpenShift specific files( Greene openshift/operator-framework-operator-controller@f25a7ee( Automation Release Team( Radchuk openshift/operator-framework-operator-controller@948290f( Tofel( Radchuk openshift/operator-framework-operator-controller@8c61a88( Automation Release Team( Radchuk openshift/operator-framework-operator-controller@528bff5( Rinis( Automation Release Team2025-02-19 00:03:50 openshift/operator-framework-operator-controller@581711d( Short( Automation Release Team2025-02-19 00:03:52UPSTREAM: &lt;carry&gt;: manifests: add hostPath mount for /etc/containers2025-02-19 00:03:53 openshift/operator-framework-operator-controller@f72b0b7( Lanford( Radchuk openshift/operator-framework-operator-controller@f032fc6( Short( Short openshift/operator-framework-operator-controller@94af975( &lt;carry&gt;: Add e2e registry Dockerfile( &lt;carry&gt;: add nodeSelector and tolerations to operator-controller deployment via kustomize patch2025-02-19 00:03:59 openshift/operator-framework-operator-controller@c73f10b( Radchuk( Automation Release Team2025-02-19 00:04:02UPSTREAM: &lt;carry&gt;: Add support for proxy trustedCAs2025-02-19 00:04:03UPSTREAM: &lt;carry&gt;: Fix make verify for mac os envs2025-02-19 00:04:05UPSTREAM: &lt;carry&gt;: Upgrade OCP images from 4.18 to 4.192025-02-19 00:04:07UPSTREAM: &lt;carry&gt;: resolve issue with pre-mature mounting of trusted CA configmap2025-02-19 00:04:11UPSTREAM: &lt;carry&gt;: fixup catalogd.Dockerfile paths2025-02-19 00:04:14UPSTREAM: &lt;carry&gt;: use projected volume for CAs to avoid subPath limitations2025-02-19 00:04:17UPSTREAM: &lt;carry&gt;: use projected volume for CAs to avoid subPath limitations2025-02-19 00:04:20UPSTREAM: &lt;carry&gt;: Skip another upstream test2025-02-19 00:04:24UPSTREAM: &lt;carry&gt;: Enable OCP metrics collection by default| This pull request is expected to merge without any human intervention. If tests are failing here, changes must land upstream to fix any issues so that future downstreaming efforts succeed. /cc @openshift/openshift-team-operator-framework /label tide/merge-method-merge /label kind/sync"
                    }
                ]
            },
            "OPRUN-3690": {
                "summary": "Modify cluster-olm-operator to watch for new catalogd web api feature gate",
                "description": "Modify cluster-olm-operator to watch for the new catalogd web api feature gate and implement reconciliation logic (openshift/ See dev guide for more info:",
                "epic_key": "OPRUN-3597",
                "GITHUB": [
                    {
                        "id": "104",
                        "type": "pullRequest",
                        "title": "OPRUN-3690: Watch for new upstream feature gate APIV1MetasHandler",
                        "body": "refer: Metas Endpoint RFC( PR that introduced metas endpoint in catalogd(operator-framework/operator-controller-1643) APIV1MetasHandler naming( Guide this PR followed for mapping upstream APIV1MetasHandler feature gate to downstream FeatureGateNewOLMCatalogdMetas} feature gate( Also bumps openshift/api to e8e096a21cb3dbeb93f1dc5138bf42ec766fc407"
                    }
                ]
            },
            "OPRUN-3663": {
                "summary": "Modify cluster-olm-operator to watch feature gates",
                "description": "Modify cluster-olm-operator to allow it to respect feature gates AC: cluster-olm-operator watches for OCP's FeatureGate and reconcile OLMv1 components accordingly See RFC| for additional background and details",
                "epic_key": "OPRUN-3646",
                "GITHUB": [
                    {
                        "id": "102",
                        "type": "pullRequest",
                        "title": "OPRUN-3663: Watch and reconcile feature gates changes",
                        "body": "Adds logic to watch and react to downstream feature gate changes by setting container env vars in `operator-controller-controller-manager` deployment manifests. Incorporates the recommended `github.com/openshift/library-go` approach (see to avoid duplicating this logic locally. ~As mentioned internally, there are still some things that need clarification - I'm raising them as comments here.~ @oceanc80 @joelanford"
                    }
                ]
            }
        },
        "epics": {
            "OPRUN-3748": {
                "summary": "Own/SingleNamespace InstallMode Support",
                "description": ""
            },
            "OPRUN-3613": {
                "summary": "UPSTREAM Permission validation pre-flight check 988",
                "description": "From the WIP brieferror Permission Verification do the doc| Step 1 permission verification as well as escalate/bind checking Permissions and Validation Checks SelfSubjectRulesReview runner Testing unit test suites for each of the above two (2) new e2e for this work: happy path and common failure path"
            },
            "OPRUN-3597": {
                "summary": "UPSTREAM catalogd web interface performance improvements 451 TP",
                "description": "The RFC written for identified a desire to formalize the catalogd web API, and divided work into a set of v1.0-blocking changes to enable versioned web interfaces (phase 1( and non-blocking changes to express and extend a formalized API specification (phase 2). This epic is to track the design and implementation work associated with phase 2. During phase 1 RFC review we identified that we needed more work to capture the extensibility design but didn't want to slow progress on the v1.0 blocking changes so the first step should be an RFC to capture the design goals for phase 2, and then any implementation trackers we feel are necessary. Work here will be behind a feature gate. catalogd web api performance improvements RFC 1569 serve catalog content based on supplied parameters 1606 Downstreaming this feature We need to follow this guide to downstream this feature:"
            },
            "OPRUN-3703": {
                "summary": "EPIC - (Post-Monorepo Integration) - Optimize and Streamline Controller Operator and Catalogd",
                "description": "Note: This epic only tracks the phase one of the work listed in the RFC Consolidate catalogd and operator-controller Kustomize configs 1341 DOWNSTREAM: Changes in the kustomize configs might need downstream effort"
            },
            "OPRUN-3646": {
                "summary": "OLMv1 Downstream feature gate promotion mechanics",
                "description": "OCP/Telco Definition of Done Epic Goal To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time Why is this important? Before OCP 4.18 the entirety of OLMv1 was under the TechPreviewNoUpgrade feature set which allowed us to make breaking API changes without having to provide an upgrade path or breaking customers. Starting from OCP 4.18 OLMv1 is part of the default OCP payload and default feature set which means that we need to maintain API compatibility. At the same time OLMv1 is still in active development and we are looking to introduce more features and deeper integration with OCP (such as OCP web console integration). To enable iterative development we might want to put some of the new features under TechPreviewNoUpgrade feature set while maintaining a set of stable features at the same time. Effectively this means that OLMv1 will work both with TechPreviewNoUpgrade and without it but will have a different set of features. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "Origin Community Distribution of Kubernetes": {
        "stories": {
            "OKD-235": {
                "summary": "Move to using pure centos os bootimages in the installer",
                "description": "",
                "GITHUB": [
                    {
                        "id": "9520",
                        "type": "pullRequest",
                        "title": "OKD-235: Add stream metadata for scos to be used by OKD",
                        "body": "OKD will now use centos pure os bootimages This change was generated using: ``` plume cosa2stream \\ --distro rhcos --no-signatures --name c9s \\ --url \\ x86_64=9.0.20250222-0 \\ aarch64=9.0.20250222-0 \\ ppc64le=9.0.20250222-0 \\ s390x=9.0.20250222-0 ```"
                    }
                ]
            }
        }
    },
    "OpenShift Dev Console": {
        "stories": {
            "ODC-7778": {
                "summary": "Include quick create in Admin perspective guided tour",
                "description": "Description As a user, I want to be introduced to Quick create in the Guided tour. Acceptance Criteria Should include a step-in guided tour for quick create. Additional Details: Title: Quickly create Descriptions: Create resources in just a few steps via Git, YAML, or container images.",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14850",
                        "type": "pullRequest",
                        "title": "ODC-7778: Include Quick create in Admin perspective guided tour",
                        "body": "img width=\"597\" alt=\"image\" src=\" /"
                    }
                ]
            },
            "ODC-7776": {
                "summary": "Add getting started alert",
                "description": "Description As a user, I want to see getting started alert when I log in to the console for the first time. Acceptance Criteria Should add getting started alert on the Software catalog, Helm repositories and Helm releases pages. Additional Details: !image-2025-03-04-16-31-15-184.png!",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14830",
                        "type": "pullRequest",
                        "title": "ODC-7776: Add getting started alert",
                        "body": "Story: Demo: -----Developer login---- -----Admin login------"
                    }
                ]
            },
            "ODC-7775": {
                "summary": "Update getting started resources section actions",
                "description": "Description As a user, I want to see actions regarding the merge perspective in getting started resources section Acceptance Criteria Update getting started resources action on the cluster overview page Update getting started resources action on the project overview page in admin perspective content for cluster overview page and project overview page will be same Additional Details: Overview page",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14829",
                        "type": "pullRequest",
                        "title": "ODC-7775: Update content in Getting started resources on cluster and project overview page",
                        "body": "Adds start admin perspective guided tour action and Enable the developer perspective quick start link in cluster and project overview getting started resources section. Step to test - Create a sample quick start with name `enable-developer-perspective` and displayName `Enable the developer perspective`"
                    }
                ]
            },
            "ODC-7773": {
                "summary": "Add e2e tests for Favorites feature",
                "description": "Description As a user, Acceptance Criteria Add e2e tests for Favorites feature Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14802",
                        "type": "pullRequest",
                        "title": "ODC-7773: Add e2e tests for Favorites feature",
                        "body": "Dependent on PR - Story: ODC-7773( img width=\"764\" alt=\"Screenshot 2025-02-27 at 4 13 39 PM\" src=\" /"
                    }
                ]
            },
            "ODC-7767": {
                "summary": "Expose remaining Topology components and utils to openshift-console/dynamic-plugin-sdk",
                "description": "Description As a user, I want to use Topology components in the dynamic plugin Acceptance Criteria Should expose the below Topology components and utils to dynamic-plugin-sdk getModifyApplicationAction baseDataModelGetter getWorkloadResources contextMenuActions CreateConnector createConnectorCallback (e",
                "epic_key": "ODC-7716"
            },
            "ODC-7766": {
                "summary": "Add Admin navigation menu tests for developer console items",
                "description": "Description As a user, Acceptance Criteria Have some tests testing the developer console items on the admin side Update existing tests to reflect the changes in the UI like changing Developer catalog to Software catalog Additional Details:",
                "epic_key": "ODC-7763",
                "GITHUB": [
                    {
                        "id": "14717",
                        "type": "pullRequest",
                        "title": "ODC-7766: Test updates pertaining to perspective merge",
                        "body": "Description: Software catalog tests in Admin view and update developer catalog to software catalog wherever it's being used in tests Story: - ODC-7766( Checks for approving Epic scenarios Automation PR: - Execute the @to-do tagged gherkin scripts manually - Convert the @to-do gherkin scripts to cypress automation scripts - Once scripts are automated, replace tag @to-do with @epic-number - Execute the scripts in Remote cluster Execution Commands: Have a cluster and disable developer perspective in customization of console export NO_HEADLESS=true && export CHROME_VERSION=$(/usr/bin/google-chrome-stable --version) BRIDGE_KUBEADMIN_PASSWORD=YH3jN-PRFT2-Q429c-5KQDr BRIDGE_BASE_ADDRESS= export BRIDGE_KUBEADMIN_PASSWORD export BRIDGE_BASE_ADDRESS oc login -u kubeadmin -p $BRIDGE_KUBEADMIN_PASSWORD oc apply -f ./frontend/packages/console-shared/src/test-data/htpasswd-secret.yaml oc patch oauths cluster --patch \"$(cat ./frontend/packages/console-shared/src/test-data/patch-htpasswd.yaml)\" --type=merge !-- Under frontend folder run -- ./integration-tests/test-cypress.sh -p dev-console Execute file software-catalog-details.feature Screenshots: Browser conformance: - x Chrome - Firefox - Safari - Edge"
                    }
                ]
            },
            "ODC-7727": {
                "summary": "Favoriting page in the Console admin perspective",
                "description": "Description As a user, I want to favorite pages in the Console admin perspective Acceptance Criteria Should favorite the pages in the console Favorite pages can be accessible from the left navigation Additional Details: Design",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14996",
                        "type": "pullRequest",
                        "title": "ODC-7727: Change favourites button defaultName",
                        "body": "The favourites button previously always used the URL for the default name. This causes some strange names such as the `openshift` being the default name for routes like `/helm-repositories/ns/openshift`. This PR changes the behaviour so that if the `PageHeading` title is passed as a string, the defaultName will be that title. Now, the default favourites name for `/helm-repositories/ns/openshift` is `Helm Repositories`. You can now also press Enter to submit the favourites modal. Before pressing enter would do nothing /assign @rhamilto /label px-approved /label docs-approved /label qe-approved"
                    },
                    {
                        "id": "14839",
                        "type": "pullRequest",
                        "title": "ODC-7727: Add hover effect to favourites icon",
                        "body": "follow on ticket to add a hover effect to the add to favourites button, by using the CSS from the ODC topology quick search button before !before( after !after( /assign @lokanandaprabhu adding labels for already approved ticket /label px-approved /label qe-approved /label docs-approved"
                    },
                    {
                        "id": "14765",
                        "type": "pullRequest",
                        "title": "ODC-7727: Favoriting page in the Console admin perspective",
                        "body": "Story: Demo: ----- Maximum count reached---"
                    }
                ]
            },
            "ODC-7726": {
                "summary": "Expose Topology components and utils to openshift-console/dynamic-plugin-sdk",
                "description": "Description As a user, I want to use Topology components in the dynamic plugin Acceptance Criteria Should expose the Topology components and utils to dynamic-plugin-sdk Additional Details: Utils and component needs to be exposed",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14705",
                        "type": "pullRequest",
                        "title": "ODC-7726,ODC-7767: Expose Topology components and utils to openshift-console/dynamic-plugin-sdk",
                        "body": "Story: Description: Exposing the below topology components to dynamic plugins, ``` CpuCellComponent MemoryCellComponent TopologyListViewNode useOverviewMetrics withEditReviewAccess getPodMetricStats getTopologyResourceObject getResource getTopologyEdgeItems getTopologyGroupItems getTopologyNodeItem mergeGroup getModifyApplicationAction baseDataModelGetter getWorkloadResources contextMenuActions CreateConnector createConnectorCallback ```"
                    }
                ]
            },
            "ODC-7723": {
                "summary": "Add quick start for how to enable developer perspective",
                "description": "Description As a user, I want to know how I can enable the developer perspective in the Web console Acceptance Criteria Add a quick start to let the user know about the steps to enable the Developer perspective in the web console Additional Details: Steps to enable dev perspective through UI search for console (console.operator.openshift.io/cluster| on search page open cluster details page click on action menu and select Customize option. It will open Cluster configuration page under General tab there is Perspectives option to enable and disabled UXD:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "968",
                        "type": "pullRequest",
                        "title": "ODC-7723: Add quick start to enable developer perspective",
                        "body": "!Screenshot 2025-03-07 at 3 09 35 PM( img width=\"450\" alt=\"image\" src=\" / img width=\"449\" alt=\"image\" src=\" / img width=\"447\" alt=\"image\" src=\" / img width=\"455\" alt=\"image\" src=\" / img width=\"454\" alt=\"image\" src=\" /"
                    }
                ]
            },
            "ODC-7720": {
                "summary": "Add dev perspective nav options to admin perspective",
                "description": "Description As a user, I want access to all the pages present in the developer perspective from the admin perspective. Acceptance Criteria Add Topology, Helm, Serverless function, and Developer catalog nav items to Admin perspective as per the UX design. Additional Details: UX design",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14911",
                        "type": "pullRequest",
                        "title": "ODC-7720: Further Developer Catalog - Software Catalog renaming",
                        "body": "follow up fix for to decrease the mentions of \"developer catalog\" in the console codebase, which has since been renamed to \"software catalog\". follow up PR for approved ticket, adding labels /label px-approved /label docs-approved /label qe-approved /label acknowledge-critical-fixes-only"
                    },
                    {
                        "id": "14588",
                        "type": "pullRequest",
                        "title": "ODC-7720: Add dev perspective nav options to admin perspective",
                        "body": "Story: Descriptions: - Add `Developer Catalog` nav option under the `Home` section in the Admin perspective - Add `Helm` nav option below `Operators` option in the Admin perspective - Add `Functions` nav option under the `Serverless`section in the Admin perspective img width=\"466\" alt=\"image\" src=\" /"
                    }
                ]
            },
            "ODC-7710": {
                "summary": "Remove RHOAS plugin from the console",
                "description": "Description As a developer, I do not want to maintain the code for a project already dead. Acceptance Criteria Remove RHOAS plugin Remove RHOAS-catalog-source Check if there is dependencies in other package and fix it Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14577",
                        "type": "pullRequest",
                        "title": "ODC-7710: Remove `@console/rhoas-plugin`",
                        "body": "Fixes: !-- For e.g Fixes: -- Analysis / Root cause: !-- Briefly describe analysis of US/Task or root cause of Defect -- `@console/rhoas-plugin` is unmaintained Solution Description: !-- Describe your code changes in detail and explain the solution -- Remove it Screen shots / Gifs for design review: !-- If change affects UI in any way, tag @openshift/team-devconsole-ux and add screenshots/gifs -- Before: !image (1)( After: !image( Unit test coverage report: !-- Attach test coverage report -- n/a Test setup: !-- If any setup required to test this PR, mention the details -- n/a Browser conformance: !-- To mark tested browsers, use x -- - Chrome - Firefox - Safari"
                    }
                ]
            },
            "ODC-6775": {
                "summary": "Upgrade jest from 21 to 29",
                "description": "Description of problem: For ODC-6264, and ODC-6265 as well as updating TypeScript from 3 to 4 as part of CONSOLE-2501 we want to update Jest from version 21 to 29.",
                "epic_key": "CONSOLE-4562",
                "GITHUB": [
                    {
                        "id": "14932",
                        "type": "pullRequest",
                        "title": "ODC-6775, CONSOLE-4393: @types/node pre-merge prep",
                        "body": "Pulling out the `jest.requireActual` change into a separate PR to make the changeset of smaller. This PR also includes the `classNames` import rewrite from in case that is needed for a future Jest upgrade. code review: /assign @jhadvig no docs/qe/px approval needed since this doesn't really affect anything: /label px-approved /label docs-approved /label qe-approved"
                    }
                ]
            },
            "ODC-7781": {
                "summary": "Updating serverless ci tests to run in admin view",
                "description": "Description Currently, serverless tests are being run from the developer perspective. As a part of merging perspective, tests should prioritise administration perspective only Acceptance Criteria Serverless workload should be created from an admin perspective All tests should only be running in admin perspective Additional Details:",
                "epic_key": "ODC-7763",
                "GITHUB": [
                    {
                        "id": "14952",
                        "type": "pullRequest",
                        "title": "ODC-7781: Running knative e2e tests from admin view",
                        "body": "Story:ODC-7781( Description: Running knative e2e tests from admin view Command to execute: export NO_HEADLESS=true && export CHROME_VERSION=$(/usr/bin/google-chrome-stable --version) BRIDGE_KUBEADMIN_PASSWORD=YH3jN-PRFT2-Q429c-5KQDr BRIDGE_BASE_ADDRESS= export BRIDGE_KUBEADMIN_PASSWORD export BRIDGE_BASE_ADDRESS oc login -u kubeadmin -p $BRIDGE_KUBEADMIN_PASSWORD oc apply -f ./frontend/packages/console-shared/src/test-data/htpasswd-secret.yaml oc patch oauths cluster --patch \"$(cat ./frontend/packages/console-shared/src/test-data/patch-htpasswd.yaml)\" --type=merge In frontend folder run ./integration-tests/test-cypress.sh -p knative Run knative-ci file Browser Chrome 125"
                    }
                ]
            },
            "ODC-7780": {
                "summary": "Update all quick starts for ODC",
                "description": "Description As a user, I want to use quick starts by following the given steps. Acceptance Criteria Update the steps in the quick starts as the developer perspective is disabled by default. Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "977",
                        "type": "pullRequest",
                        "title": "ODC-7780: update quick starts to work in the converged perspective",
                        "body": "Update the following quick starts to make it work in the converged perspective 1. Add health checks to your sample application 2. Add Helm Chart Repositories to extend the Developer Catalog for your project 3. Get started with JBoss EAP 7 using a Helm Chart 4. Manage available content in the Helm Chart Catalog 5. Monitor your sample application 6. Get started with Node 7. Get started with Quarkus using a Helm Chart 8. Get started with Quarkus using s2i 9. Install Red Hat Developer Hub (RHDH) using Helm Chart 10. Install Red Hat Developer Hub (RHDH) using the Operator 11. Get started with Spring"
                    }
                ]
            },
            "ODC-7770": {
                "summary": "Remove perspective switcher if only one perspective is present",
                "description": "Description As a user, Acceptance Criteria criteria Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14762",
                        "type": "pullRequest",
                        "title": "ODC-7770: Remove perspective switcher if only one perspective is present",
                        "body": "before: !Screenshot From 2025-02-12 16-39-29( after: !Screenshot From 2025-02-12 16-47-02( /assign @vikram-raj @sanketpathak /label px-approved docs-approved acknowledge-critical-fixes-only"
                    }
                ]
            },
            "ODC-7769": {
                "summary": "Add Getting started section on the Project overview page",
                "description": "Description As a user, Acceptance Criteria criteria Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14792",
                        "type": "pullRequest",
                        "title": "ODC-7769: Add Getting started section in project Overview page",
                        "body": "Add getting started section on the project overview page img width=\"1489\" alt=\"image\" src=\" /"
                    }
                ]
            },
            "ODC-7725": {
                "summary": "Hide perspective preference option on user preferences page incase on one perspective",
                "description": "Description As a user, I do not want a perspective preferences option if only one perspective is enabled. Acceptance Criteria Should not show perspective preference option if only one perspective is enabled. Additional Details:",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14644",
                        "type": "pullRequest",
                        "title": "ODC-7725: Show perspective preferences option if more than one perspective are available",
                        "body": "Story:"
                    }
                ]
            },
            "ODC-7724": {
                "summary": "Add guided tour in the Admin perspective",
                "description": "Description As a user, I want to know about the nav option added to the Admin perspective from the Developer perspective. Acceptance Criteria Add guided tour to the Admin perspective to let the user know about the unified perspective Additional Details: updated design",
                "epic_key": "ODC-7716",
                "GITHUB": [
                    {
                        "id": "14776",
                        "type": "pullRequest",
                        "title": "ODC-7724: Add guided tour in Admin perspective",
                        "body": "Descriptions: Add a guided tour in the Admin perspective to inform the user about the unified perspective."
                    }
                ]
            },
            "ODC-7698": {
                "summary": "Replace react-copy-to-clipboard with PatternFly ClipboardCopyButton component",
                "description": "Location: pipelines-plugin/src/components/repository/form-fields/CopyPipelineRunButton.tsx PF component: AC: Replace react-copy-to-clipboard with PatternFly ClipboardCopyButton component Remove react-copy-to-clipboard as a dependency",
                "epic_key": "CONSOLE-4132",
                "GITHUB": [
                    {
                        "id": "14413",
                        "type": "pullRequest",
                        "title": "ODC-7698: Replace `CopyPipelineRunButton` with PF5 design",
                        "body": "Fixes: !-- For e.g Fixes: -- Solution Description: !-- Describe your code changes in detail and explain the solution -- Remove `react-copy-to-clipboard` dep, remove `CopyPipelineRunButton`, and switch from a monaco-based design to the PatternFly-provided multiline design I considered other options, such as rewriting the `CopyPipelineRunButton` using the PatternFly component; however, the presence of the `textId` prop in PF's `ClipboardCopyButton` highly implies that the intended design should accompany a node in the DOM with an id containing the text to be copied. However, the `EditorField` does not have an ID containing just the text, and copying from its parent would include line numbers. Screen shots / Gifs for design review: !-- If change affects UI in any way, tag @openshift/team-devconsole-ux and add screenshots/gifs -- demo Unit test coverage report: !-- Attach test coverage report -- n/a Test setup: !-- If any setup required to test this PR, mention the details -- n/a Browser conformance: !-- To mark tested browsers, use x -- - x Chrome - x Firefox - Safari"
                    }
                ]
            }
        },
        "epics": {
            "ODC-7716": {
                "summary": "Merge Admin and Dev Perspectives",
                "description": "Epic Goal Base on user analytics many of customers switch back and fourth between perspectives, and average15 times per session. The following steps will be need: Surface all Dev specific Nav items in the Admin Console Disable the Dev perspective by default but allow admins to enable via console setting All quickstarts need to be updated to reflect the removal of the dev perspective Guided tour to show updated nav for merged perpspective Why is this important? We need to alleviate this pain point and improve the overall user experience for our users. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "ODC-7763": {
                "summary": "Automation enhancement for 4.19",
                "description": "Problem: This epic covers the scope of automation-related stories in ODC Goal: Automation enhancements for ODC Why is it important? Use cases: case Acceptance criteria: Automation enhancements as per the perspective merged Tests to be updated according to the default setting of having only Admin perspective Dependencies (External/Internal): Design Artifacts: Exploration: Note:"
            }
        }
    },
    "OpenShift Console": {
        "epics": {
            "CONSOLE-4562": {
                "summary": "OCP 4.20 - Console Dependencies & Tech Debt",
                "description": "Over time, our OpenShift Console has accumulated technical debt in its libraries, frameworks, and underlying infrastructure. This epic is focused on auditing, updating, and standardizing dependencies to the latest supported versions, while ensuring compatibility and minimizing user impact. Addressing this tech debt will: Reduce security vulnerabilities by patching known CVEs Improve performance through optimized libraries Streamline developer onboarding and maintenance Lay the groundwork for future features by aligning on current platform standards Goals & Objectives: Frontend Dependencies Audit third-party UI components (React, PatternFly, etc.) Upgrade to the latest stable major versions Refactor any deprecated APIs Backend Dependencies Update Go modules and middleware libraries Migrate from deprecated frameworks (if applicable) Ensure backward compatibility for REST/gRPC endpoints Infrastructure Harden CI/CD pipelines with up-to-date build agents Refresh container base images (e.g., Red Hat UBI versions) Align Kubernetes manifests with current API versions Acceptance Criteria: All frontend NPM package versions are updated to their latest non-breaking releases, with no failing unit or e2e tests. Backend Go modules have no outdated major versions; existing integration tests pass without regression. CI/CD pipeline definitions use current Docker image tags; all automated builds succeed in consumed staging clusters. No known high- or critical-severity vulnerabilities remain in project dependencies (scan report attached). Documentation updated to reflect new version requirements and rollback procedures."
            },
            "CONSOLE-4325": {
                "summary": "Adopt PatternFly 6 and remove PatternFly 4",
                "description": "Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CONSOLE-4350": {
                "summary": "OCP 4.19 - Console Dependencies & Tech Debt",
                "description": "An epic we can duplicate for each release to ensure we have a place to catch things we ought to be doing regularly but can tend to fall by the wayside."
            },
            "CONSOLE-3945": {
                "summary": "Prepare to update OCP Console React dependency",
                "description": "Epic Goal Update the OCP Console frontend React dependency to a more recent version, as the current version has been end-of-life for over a year. Why is this important? The longer we wait to make this update, the harder it will be. It's important to stay current so that we can be more nimble with tech debt and dependencies. Scenarios As a developer, I am assigned a high-priority feature. I find that React must be updated as part of the feature. I also find that because there are many breaking changes between our version and the latest, an update would be out of scope for this story. The feature must be deferred until we can address the tech debt. A major issue is found in our current version of React, and an update is required. The scope of this update work has ballooned over the time since our last update. We have to drop other important work to prioritize this update and again, other important features or work are deferred. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. OCP Console React dependency and all other tangential dependencies have been updated to an agreed-upon recent version. Other stakeholders, like plugin consumers, should not be affected by this change. Previous Work (Optional): Open questions: Should this be accomplished as a swarm activity where we spend a sprint addressing all blockers? Should we take a cautious approach and resolve blockers over time until we reach a point where an update is feasible? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CONSOLE-4352": {
                "summary": "OCP 4.19 - Address tech debt in frontend/public/components/secrets/create-secret.tsx",
                "description": "Epic Goal Migrate all components to functional components Remove all HOC patterns Break the file down into smaller files Improve type definitions Improve naming for better self-documentation Address any React anti-patterns like nested components, or mirroring props in state. Address issues with handling binary data Add unit tests to these components Acceptance Criteria Refactor secret forms Adding unit tests to these components. Fix Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CONSOLE-4132": {
                "summary": "Replace legacy custom components with PatternFly components",
                "description": "Epic Goal Goal is to locate and replace old custom components with PatternFly components. Why is this important? Custom components require supportive css to mimic the visual theme of PatternFly. Over time these supportive styles have grown and interspersed through the console codebase, which require ongoing efforts to carry along, update and maintain consistency across product areas and packages. Also, custom components can have varying behaviors that diverge from PatternFly components, causing bugs and create discordance across the product. Future PatternFly version upgrades will be more straightforward and require less work. Acceptance Criteria Identify custom components that have a PatternFly equivalent component. Create stories which will address those updates and fixes Update integration tests if necessary. Open questions: ..."
            }
        },
        "stories": {
            "CONSOLE-4542": {
                "summary": "Use PatternFly DescriptionList",
                "description": "AC: custom styles for dl, dt, dd are removed all dl, dt, dd base html elements in the source code are replaced with the PatternFly styled ones",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14966",
                        "type": "pullRequest",
                        "title": "CONSOLE-4542: Remove base `dl` `dd` `dt` CSS",
                        "body": "follow up on to remove unused `dl` `dd` `dt` CSS styles. adding labels for follow up PR: /label px-approved /label docs-approved /label qe-approved code review: /assign @rhamilto"
                    },
                    {
                        "id": "14947",
                        "type": "pullRequest",
                        "title": "CONSOLE-4542: Convert `dl`, `dd`, `dt` to PF `DescriptionList`",
                        "body": "fixes example change: before: !image( after: !image("
                    }
                ]
            },
            "CONSOLE-4540": {
                "summary": "Set as default StorageClass Action",
                "description": "1. Allow setting default StorageClass in UI 2. Currently, the process for changing (from one to another) the default StorageClass involves patching or editing annotations on both SCs manually. 3. The UI shows an indicator of which SC is default, which would imply you could/should be able to set this in UI",
                "GITHUB": [
                    {
                        "id": "14920",
                        "type": "pullRequest",
                        "title": "CONSOLE-4540: add set as default sc action",
                        "body": "DEMO"
                    }
                ]
            },
            "CONSOLE-4538": {
                "summary": "Use PatternFly-recommended 404 page in console",
                "description": "see AC: - 404 page uses above component",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14958",
                        "type": "pullRequest",
                        "title": "CONSOLE-4538: Add error reporting link to `ErrorBoundary` modal",
                        "body": "after: code review: /assign @spadgett adding labels for already approved ticket /label qe-approved /label docs-approved /label px-approved"
                    },
                    {
                        "id": "14912",
                        "type": "pullRequest",
                        "title": "CONSOLE-4538: Use PF component groups for Error states",
                        "body": "for follows the guidelines set out in Originally, I wanted to use the MissingPage( component group; however, there are issues with it in dark theme: adding labels for ticket part of pf6 upgrade /label px-approved /label docs-approved code review: /assign @rhamilto qe review: /assign @yapei 404ErrorPage not sure if we can link to `/search` like this. maybe I should remove that button? before: !( after: !( ErrorBoundary before: !( after: !( !("
                    }
                ]
            },
            "CONSOLE-4524": {
                "summary": "Allow Deletion of Identity Providers (IDPs) via OpenShift Web Console",
                "description": "As an OpenShift administrator, I want to be able to delete identity providers (IDPs) through the web console UI, So that I can manage authentication configurations without manually editing YAML files. Acceptance Criteria: The OpenShift web console provides a UI option to remove an existing IDP. Users can view a list of configured IDPs and select one for deletion. A confirmation prompt appears before deleting an IDP to prevent accidental removals. Appropriate success or error messages are displayed based on the outcome of the deletion. Add integration test RFE:",
                "epic_key": "CONSOLE-4334",
                "GITHUB": [
                    {
                        "id": "14949",
                        "type": "pullRequest",
                        "title": "CONSOLE-4524: Allow Deletion of Identity Providers (IDPs) via OpenShift Web Console",
                        "body": "Screenshots: !Screenshot 2025-04-09 at 9 46 21 AM( !Screenshot 2025-04-09 at 9 46 29 AM( !Screenshot 2025-04-14 at 2 23 38 PM( !Screenshot 2025-04-09 at 9 47 18 AM( !Screenshot 2025-04-14 at 2 23 22 PM( img width=\"1876\" alt=\"Screenshot 2025-04-10 at 10 20 25 AM\" src=\" /"
                    }
                ]
            },
            "CONSOLE-4523": {
                "summary": "Distribute the OpenShift CLI for both RHEL8 and RHEL9",
                "description": "As a developer I want to download rhel8 and rhel9 _oc_ binaries which are now part of the cli-artifacts image which is meing used as builder image for _downloads_ image used used for downloads _oc_ binaries. AC: Update the console-operator code to provide rhel8 and rhel9 _oc_ binaries as part of the ConsoleCLIDownloads CR Add tests RFE - This story is probably a bug, which we should backport to the version in which the cli-artifacts image started to provide additional RHEL binaries.",
                "epic_key": "CONSOLE-4562",
                "GITHUB": [
                    {
                        "id": "2275",
                        "type": "pullRequest",
                        "title": "CONSOLE-4523: Lift Featuregate for the ConsolePlugin ContentSecurityPolicy API",
                        "body": "Sippy dashboard monitoring the OCPFeatureGate:ConsolePluginContentSecurityPolicy( tests /assign @joelanford FYI @spadgett"
                    },
                    {
                        "id": "976",
                        "type": "pullRequest",
                        "title": "CONSOLE-4523: Add rhel8 and rhel9 oc binaries for Linux OS in CLI downloads",
                        "body": "Updating the downloads server in order for it to serve the rhel8 and rhel9 binaries for the Linux OS. Also needed to update `CLIDownloadsSyncController` controller that is responsible for creating the ConsoleCLIDownloads CR of `oc` binaries. Screen: img width=\"1386\" alt=\"Screenshot 2025-04-09 at 13 20 54\" src=\" / Originally I wanted to put all the oc + oc.rhel8 + oc.rhel9 into a single archive, but that would be a huge one, more then 500MB \ud83e\udd2f so I rather added an archive for each binary type. /assign @Mylanos @opayne1 Im open for discussion about the name for the link, cause basically what I did is I've only appended ` - RHEL 8` or ` - RHEL 9` for RHEL binaries (`oc.rhel8` and `oc.rhel9`)"
                    }
                ]
            },
            "CONSOLE-4521": {
                "summary": "Remove old polyfills",
                "description": "As a user, I do not want to load polyfills for browsers that OCP console no longer supports.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14880",
                        "type": "pullRequest",
                        "title": "CONSOLE-4521: Remove old polyfills",
                        "body": "The following polyfills are no longer needed as we dropped Internet Explorer support: - - - - In a follow-up PR, I will remove unused node.js polyfills, after merges. code review: /assign @vojtechszocs QE review: /assign @yapei tech debt, adding labels: /label px-approved /label docs-approved"
                    }
                ]
            },
            "CONSOLE-4517": {
                "summary": "update or remove focus-trap-react",
                "description": "warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react@0.14.x ^15.0.0 ^16.0.0\". warning \" focus-trap-react@6.0.0\" has incorrect peer dependency \"react-dom@0.14.x ^15.0.0 ^16.0.0\".",
                "epic_key": "CONSOLE-3945"
            },
            "CONSOLE-4515": {
                "summary": "update or remove react-modal",
                "description": "warning \" react-modal@3.12.1\" has incorrect peer dependency \"react@^0.14.0 ^15.0.0 ^16 ^17\". warning \" react-modal@3.12.1\" has incorrect peer dependency \"react-dom@^0.14.0 ^15.0.0 ^16 ^17\".",
                "epic_key": "CONSOLE-3945",
                "GITHUB": [
                    {
                        "id": "14874",
                        "type": "pullRequest",
                        "title": "CONSOLE-4515, CONSOLE-4516, CONSOLE-4517: Update `react-modal`, `react-tagsinput`",
                        "body": "update a few react dependencies to prepare for React 18. shockingly there appear to be no breaking changes for these packages removed `focus-trap-react` as it appears to be unused. tech debt, adding labels /label px-approved /label docs-approved"
                    }
                ]
            },
            "CONSOLE-4508": {
                "summary": "Enable CSP tests for console-operator",
                "description": "Part of lifting feature gate for the CSP API we need to enable e2e CSP tests for console-operator.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "969",
                        "type": "pullRequest",
                        "title": "CONSOLE-4508: Enable CSP e2e test",
                        "body": "Enabling the CSP related test case in out CI. The test should only run if the featureGate is set. /assign @TheRealJon @yapei"
                    }
                ]
            },
            "CONSOLE-4507": {
                "summary": "Remove and/or replace Bootstrap styles with PatternFly equivalents where possible",
                "description": "There are a number of `co-` styles in the console from the days of CoreOS. We should remove and/or replace these with PatternFly equivalents where possible.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14954",
                        "type": "pullRequest",
                        "title": "CONSOLE-4507: Clean up catalog view",
                        "body": "Skeleton update Modal updates Improve layout at mobile by moving tabs after tiles at small resolutions"
                    }
                ]
            },
            "CONSOLE-4501": {
                "summary": "Add unit tests for Timestamp component",
                "description": "Add unit tests for the Timestamp component to prevent regressions like AC: A unit test is implemented which tests the functionality of the Timestamp component The unit test exercises both the relative and full date formats.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14799",
                        "type": "pullRequest",
                        "title": "CONSOLE-4501: Add unit test for Timestamp component",
                        "body": "This is a follow-on for OCPBUGS-51202( Add tests to prevent regressions in the Timestamp component."
                    }
                ]
            },
            "CONSOLE-4498": {
                "summary": "Replace checkboxes with Switch in ResourceLog",
                "description": "The checkboxes at the top of the ResourceLog component should be changed to Switches as that same change is being made for the YAMLEditor| AC: Replace CheckBox component in the ResourceLog component with Switch from PF6",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14815",
                        "type": "pullRequest",
                        "title": "CONSOLE-4498: Replace checkboxes with Switch in ResourceLog",
                        "body": "To align with changes in After Note the layout issue at mobile is existing and is the result of an existing issue in PatternFly("
                    }
                ]
            },
            "CONSOLE-4492": {
                "summary": "Align spacing of page elements with PatternFly",
                "description": "Long before PatternFly, the page layout was controlled with `co-m-pane` and `co-m-page` classes. These classes have different padding or margin values than PatternFly. We should update these existing classes to align with PatternFly.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14995",
                        "type": "pullRequest",
                        "title": "CONSOLE-4492: fix: do not render if listpageheader has no props",
                        "body": "/assign @rhamilto"
                    },
                    {
                        "id": "14964",
                        "type": "pullRequest",
                        "title": "CONSOLE-4492: Use `helpText` for help text",
                        "body": "follow up on 14875 fixes spacing regression in certain routes. also use `BreadcrumbItem` for linking to breadcrumbs to reduce dom nodes /assign @rhamilto follow up pr, adding labels: /label px-approved /label docs-approved /label qe-approved"
                    }
                ]
            },
            "CONSOLE-4484": {
                "summary": "use PF6 component for tabs",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14918",
                        "type": "pullRequest",
                        "title": "CONSOLE-4484: Fix `Skip to content` regression",
                        "body": "follow up to fix an a11y regression I caused in The button now links to `content-scrollable` which is the div that contains the main content follow up PR for approved ticket, adding labels /label px-approved /label docs-approved /label qe-approved code review: /assign @rhamilto"
                    },
                    {
                        "id": "14806",
                        "type": "pullRequest",
                        "title": "CONSOLE-4484: follow on fix to ensure Bottom ConsoleNotification are ...",
                        "body": "...visible After !localhost_9000_k8s_cluster_console openshift io~v1~ConsoleNotification_example("
                    },
                    {
                        "id": "14760",
                        "type": "pullRequest",
                        "title": "CONSOLE-4484: Replace custom tabs with PF6 `Tabs` component",
                        "body": "fixes before/after: !image( !image( !image("
                    }
                ]
            },
            "CONSOLE-4481": {
                "summary": "Lift the FeatureGate for the CSP API",
                "description": "The CSP epic was delivered in 4.18 the what steps need to be made in order to lift the FeatureGate. AC: Review the enhancement docs and address the remaining steps in oder to lift the CSP API FeatureGate and create follow up stories.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "975",
                        "type": "pullRequest",
                        "title": "CONSOLE-4481: Add additional CSP test",
                        "body": "/assign @TheRealJon"
                    },
                    {
                        "id": "974",
                        "type": "pullRequest",
                        "title": "CONSOLE-4481: Add junit generation for unit and e2e testing",
                        "body": "In order for prow to contribute tests output to sippy our unit and e2e tests need to generate junit files. For that reason I've rewrote how the unit tests and e2e tests are being executed. img width=\"1913\" alt=\"Screenshot 2025-04-07 at 14 45 51\" src=\" / /assign @TheRealJon @yapei"
                    }
                ]
            },
            "CONSOLE-4448": {
                "summary": "Add the ability to specify a second custom logo for PatternFly 6 api",
                "description": "In PatternFly 6, the colour of the masthead changes depending upon the mode (light vs dark). As a result, a single custom logo may not work for both cases (as is the case with the existing OKD and OpenShift logos as they assume the masthead background is always dark and include white text as a result). We need to add the ability to specify a second logo to account for this. See AC: propose chances writing an enhancement document which should cover the fullstack change - API, console-operator, console update console operator's config API with additional field for defining second custom logo for PF6 add unit tests for the API change",
                "epic_key": "CONSOLE-4325"
            },
            "CONSOLE-4430": {
                "summary": "Automated Content Security Policy testing of Console pages",
                "description": "In Console 4.18 we introduced an initial Content Security Policy| (CSP) implementation (CONSOLE-4263). This affects both Console web application as well as any dynamic plugins loaded by Console. In production, CSP violations are sent to telemetry service for analysis (CONSOLE-4272). We need a reliable way to detect new CSP violations as part of our automated CI checks. We can start with testing the main dashboard page of Console and expand to more pages as necessary. Acceptance criteria: Console project provides a script to test for CSP violations. CSP violation test script does not report any errors for Console.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14675",
                        "type": "pullRequest",
                        "title": "CONSOLE-4430: Content Security Policy E2E testing with Puppeteer & Chrome",
                        "body": "This PR adds a script that tests Console web application for Content Security Policy( violations. Console currently implements CSP via Content-Security-Policy-Report-Only( HTTP response header and uses custom SecurityPolicyViolationEvent( handler to report any CSP violations detected in production env. to telemetry service. Summary of changes - Use Puppeteer( to install and manage a local Chrome for Testing( browser instance. All files related to Puppeteer are placed under `frontend/.puppeteer` directory. Chrome test browser is installed only when necessary, so the typical `yarn install` dependency update flow is not affected. - Run `yarn test-puppeteer-csp` in `frontend` directory to launch the CSP test script. This script assumes that Console Bridge server is already running locally and will test page for CSP violations using Chrome DevTools Protocol( (CDP). Example 1 - test pass !( Example 2 - test fail, after adding inline script in `frontend/public/index.html` !( General test flow 1. Use CDP to spy on page request (`resourceType` = `Document`). Modify the request by adding custom `Test-CSP-Reporting-Endpoint` header, instructing the server to use the given CSP reporting endpoint for testing purposes and then resume the modified request. 2. Use CDP to spy on potential CSP violation report requests (`resourceType` = `CSPViolationReport`). Upon receiving such request, report the error and then fulfill the request with 200 status code - this way, we don't need to implement a real CSP reporting endpoint. 3. Load page and wait until there is no network activity for at least 2s. Treat non-OK HTTP status code as error. As a follow-up, we should add a CI job that invokes `test-csp.sh` as the entry point."
                    }
                ]
            },
            "CONSOLE-4407": {
                "summary": "Update YAML language server and monaco",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14663",
                        "type": "pullRequest",
                        "title": "CONSOLE-4407, CONSOLE-4409: Update monaco and YAML language server, use PF6 CodeEditor",
                        "body": "h2 id=\"feature-fix\"CONSOLE Features and Fixes:/h2 !--The pull request title must be prefixed with a Jira issue or Bugzilla bug in order to be merged. For example: For e.g Features: - CONSOLE-XXXX: title For e.g Jira Bug Fixes: - OCPBUGS-XXXX: title For e.g Bugzilla Bug Fixes: - Bug XXXXXXX: title Combos: - CONSOLE-XXXX, Bug XXXXXXX: title-- h2 id=\"solution-description\"Solution description/h2 !-- Describe your code changes in detail and explain the solution or functionality -- - Switch from react-monaco-editor( to the PatternFly Code editor( - Use monaco-yaml( for YAML language support (still based on yaml-language-server( which supports newer monaco versions - Removed `umd-compat-loader` - Update monaco from 0.28.1 to 0.51.0, which adds a few new features, notably sticky scrolling( - Note that this removes the kbdAlt/kbd+kbdF1/kbd accessibility keyboard shortcut, as the shortcut was seemingly removed in monaco-editor 0.40.0: see the last version with the dialog( and first version without it( - Updated custom CodeEditor theme to use PF6 tokens. Work on adding this theme to PatternFly is underway, see - Fix long time bug where the commands in the command palette are bolded and underlined h2 id=\"screenshots\"Screen shots / gifs / design review:/h2 !-- Add screenshots or gifs for visual changes. Be sure to include before and after where relevant -- Before: !image( !image( After: !image( !image("
                    }
                ]
            },
            "CONSOLE-4400": {
                "summary": "Update to TypeScript 5",
                "description": "Currently console is using TypeScript 4, which is preventing us from upgrading to NodeJS-22. Due to that we need to update TypeScript 5 (not necessarily latest version). AC: Update TypeScript to version 5 Update ES build target to ES-2021 Note: In case of higher complexity we should be splitting the story into multiple stories, per console package.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14620",
                        "type": "pullRequest",
                        "title": "CONSOLE-4400: Upgrade TypeScript to v5"
                    }
                ]
            },
            "CONSOLE-4399": {
                "summary": "Add \"Created Time\" Column to Job Listing in OCP Console",
                "description": "Introduce a \"Created Time\" column to the Job listing in the OpenShift Container Platform (OCP) console to enhance the ability to sort jobs by their creation date. This feature will help users efficiently manage and navigate through numerous jobs, particularly in environments with frequent CronJob executions and a high volume of job runs. Acceptance Criteria: Add a \"Created Time\" column to the Job listing in the OCP console. Display the creation timestamp in a format consistent with the console's date and time standards. Enable sorting of jobs by the \"Created Time\" column.",
                "epic_key": "CONSOLE-4334",
                "GITHUB": [
                    {
                        "id": "14786",
                        "type": "pullRequest",
                        "title": "CONSOLE-4399: Added 'createdTime' in Jobs.tsx"
                    }
                ]
            },
            "CONSOLE-4393": {
                "summary": "Update to NodeJS v22",
                "description": "Current version of NodeJS is in the maintainance mode and we need to update to the next version with long term support which is currently NodeJS v22. Acceptance Criteria: update console builder image update demo-dynamic-plugin base image fix any related build issues",
                "epic_key": "CONSOLE-4562"
            },
            "CONSOLE-4381": {
                "summary": "Adopt PatternFly 6",
                "description": "AC: Vendor PF6 Remove all the unecessary PF5 packages and keep the patternfly/patternfly as a npm dependancy Remove the overpass font from webpack config Follow the PF6 upgrade guide. to automatically identify and fix major issues. Create a followUp stories for addressing the remaining update issues, presumably per console package. Update docs Check",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14729",
                        "type": "pullRequest",
                        "title": "CONSOLE-4381: PatternFly 6 README updates"
                    }
                ]
            },
            "CONSOLE-4380": {
                "summary": "Use alerts rules detail page from monitoring-plugin",
                "epic_key": "OU-224",
                "GITHUB": [
                    {
                        "id": "14614",
                        "type": "pullRequest",
                        "title": "CONSOLE-4380: Remove more already-migrated code"
                    },
                    {
                        "id": "14596",
                        "type": "pullRequest",
                        "title": "CONSOLE-4380: Use AlertsRulesDetailPage from `monitoring-plugin`",
                        "body": "/hold Fixes: !-- For e.g Fixes: -- Solution Description: !-- Describe your code changes in detail and explain the solution -- - Remove `AlertRulesDetailsPage` from console - Add a route in monitoring-plugin to enable it - Remove code related to routes that are no longer provided by console repo Screen shots / Gifs for design review: !-- If change affects UI in any way, tag @openshift/team-devconsole-ux and add screenshots/gifs -- !image( The only change to the AlertsRulesDetailPage in the developer perspective is that it will no longer have a namespace selector. Since the selected alert is already namespaced, it didn't make sense to swap namespaces while focused on a specific AlertRule. Unit test coverage report: !-- Attach test coverage report -- n/a Test setup: !-- If any setup required to test this PR, mention the details -- Host the monitoring dynamic plugin locally by running ```sh ./bin/bridge -plugins monitoring-plugin= -i18n-namespaces=plugin__monitoring-plugin -branding openshift console backend yarn dev console/frontend and monitoring-plugin/web ``` Browser conformance: !-- To mark tested browsers, use x -- - x Chrome - x Firefox - Safari"
                    }
                ]
            },
            "CONSOLE-4379": {
                "summary": "Remove PatternFly 4",
                "description": "Before we can adopt PatternFly 6, we need to drop PatternFly 4. We should drop 4 first so we can understand what impact if any that will have on plugins. AC: Remove PF4 package. Console should not load any PF4 assets during the runtime. Remove PF4 support for DynamicPlugins - SharedModules + webpack configuration",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14671",
                        "type": "pullRequest",
                        "title": "CONSOLE-4379: remove PatternFly 4 shared modules",
                        "body": "Follow on to as this deletion was overlooked."
                    }
                ]
            },
            "CONSOLE-4378": {
                "summary": "Remove PopupKebabMenu and related code",
                "description": "PopupKebabMenu is orphaned and contains a reference to `@patternfly/react-core/deprecated`. It and related code should be removed so we can drop PF4 and adopt PF6.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14594",
                        "type": "pullRequest",
                        "title": "CONSOLE-4378: remove orphaned PopupKebabMenu code",
                        "body": "This code is orphaned and contains a reference to `@patternfly/react-core/deprecated`. It should be removed so we can drop PF4 and adopt PF6."
                    }
                ]
            },
            "CONSOLE-4377": {
                "summary": "Update ActionItemMenu.tsx to use new DropdownItemProps from PatternFly 5",
                "description": "contains a reference to `@patternfly/react-core/deprecated`. In order to drop PF4 and adopt PF6, this reference needs to be removed.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14593",
                        "type": "pullRequest",
                        "title": "CONSOLE-4377: update to use new DropdownItemProps from PatternFly 5",
                        "body": "In order to adopt PatternFly 6, we need to remove references to deprecated items. In this case, it's easiest to simply use the new props interface as it is compatible."
                    }
                ]
            },
            "CONSOLE-4376": {
                "summary": "Remove orphaned ClusterConfigurationDropdownField.tsx and related code",
                "description": "This component was never finished| and should be removed as it includes a reference to `@patternfly/react-core/deprecated`, which blocks the removal of PF4 and the adoption of PF6.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14592",
                        "type": "pullRequest",
                        "title": "CONSOLE-4376: remove orphaned ClusterConfigurationDropdownField code"
                    }
                ]
            },
            "CONSOLE-4081": {
                "summary": "Address tech debt in EditSecret component",
                "description": "The EditSecret component needs to be refactored to address several tech debt issues: Remove Firehose and SecretLoadingWrapper usage Utilize the 'useK8sWatchResource' hook to fetch Integrate SecretLoadingWrapper logic into EditSecret component Improve type definitions",
                "epic_key": "CONSOLE-4352",
                "GITHUB": [
                    {
                        "id": "14853",
                        "type": "pullRequest",
                        "title": "CONSOLE-4081: Refactor EditSecret component",
                        "body": "Possible test scenarios: Test Key/Value Secret 1. Create Key/Value Secret 2. Click on created Key/Value Secret 3. Click on kebab menu, press Edit Secret 4. Correct Key/Value Edit Secret form should be shown. Test Image Pull Secret 1. Create Image Pull Secret 2. Click on created Image Pull Secret 3. Click on kebab menu, press Edit Secret 4. Correct `Edit image pull secret` form should be shown. Test Webhook Secret 1. Create Webhook Secret 2. Click on created Webhook Secret 3. Click on kebab menu, press Edit Secret 4. Correct `Edit webhook secret` form should be shown. As a shared fifth step could be editing the fields and saving the changes. The newly edited data should be visible in the secret now. Currently when you try to edit the Key/Value secret form, the data/value part is incorrectly shown as base64 encoded value, which is not caused by this PR. I addressed this issue in separate PR("
                    }
                ]
            },
            "CONSOLE-4076": {
                "summary": "Address tech debt in SourceSecretForm component",
                "description": "The SourceSecretForm component needs to be refactored to address several tech debt issues: Rename to AuthSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
                "epic_key": "CONSOLE-4352",
                "GITHUB": [
                    {
                        "id": "14633",
                        "type": "pullRequest",
                        "title": "CONSOLE-4076: Address tech debt in SourceSecretForm component"
                    }
                ]
            },
            "CONSOLE-3905": {
                "summary": "Update Webpack package to version 5",
                "description": "As a developer I want to make sure we are running the latest version of webpack in order to take advantage of the latest benefits and also keep current so that future updating is a painless as possible. We are currently on v4.47.0. Changelog: By updating to version 5 we will need to update following pkgs as well: html-webpack-plugin webpack-bundle-analyzer copy-webpack-plugin fork-ts-checker-webpack-plugin AC: Update webpack to version 5 and determine what should be the ideal minor version.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14378",
                        "type": "pullRequest",
                        "title": "CONSOLE-3905: (deps) upgrade to webpack 5",
                        "body": "h2 id=\"feature-fix\"CONSOLE Features and Fixes:/h2 !--The pull request title must be prefixed with a Jira issue or Bugzilla bug in order to be merged. For example: For e.g Features: - CONSOLE-XXXX: title For e.g Jira Bug Fixes: - OCPBUGS-XXXX: title For e.g Bugzilla Bug Fixes: - Bug XXXXXXX: title Combos: - CONSOLE-XXXX, Bug XXXXXXX: title-- h2 id=\"solution-description\"Solution description/h2 !-- Describe your code changes in detail and explain the solution or functionality -- - upgraded `webpack` related dependencies to latest version - updated `formik` from 2.0.3 to 2.1.5 to fix build warning - remove `cache-loader`, `null-loader` because they are deprecated - updated `react-dnd` from 9.4.0 to 11.1.3 to fix build error - switched `ts-loader` to `esbuild-loader` for performance - switch `webpack-virtual-modules` with `val-loader` to fix build warning (and for easier transition to rspack, since it supports most loader APIs) h2 id=\"reviewers-and-assignees\"Reviewers and assignees:/h2 !-- - Tag an OCP console engineer to review the changes. - If there are visual, content, or interaction changes in the PR, tag \"@openshift/team-ux-review\" - If the PR is implementing a story, assign QE, docs, and PX approvers: Console Approver: /assign gh-user QE approver: /assign gh-user Docs approver: /assign gh-user PX approver: /assign gh-user -- n/a h2 id=\"test-cases\"Test cases:/h2 - Does `@console/internal/file-input` still work as expected? - Are there any regressions with `formik` or `react-dnd`? - Do all the Monaco YAML language server features work? - Do all dynamic and static plugins work as expected? - Does everything else work the same? h2 id=\"additional-info\"Additional info:/h2 !-- Add any additional info here -- n/a h2 id=\"screenshots\"Screen shots / gifs / design review:/h2 !-- Add screenshots or gifs for visual changes. Be sure to include before and after where relevant -- n/a"
                    }
                ]
            },
            "CONSOLE-3247": {
                "summary": "Remove react-measure dependency from the console and demo plugin",
                "description": "Since react-measure is not compatible with react 18 we need to remove our react-measure dependency from the console repo. One solution is to follow patternfly and use react-resize-detector: Acceptance criteria: the console repo should no longer depend on react-measure the dynamic demo plugin should no longer depend on react-measure",
                "epic_key": "CONSOLE-3945",
                "GITHUB": [
                    {
                        "id": "14844",
                        "type": "pullRequest",
                        "title": "CONSOLE-3247: Remove `react-measure`",
                        "body": "removes `react-measure` dependency possible test cases: - when clicking the buttons in the add page, do they still work? - when resizing the add page, are there errors? - does the add page show all the same cards as before? tech debt, adding labels /label px-approved /label docs-approved QE review (whoever has time): /assign @yapei /assign @sanketpathak code review: /assign @vikram-raj"
                    }
                ]
            },
            "CONSOLE-4541": {
                "summary": "Deprecate VirtualizedTable and ListPageFilter and useListPageFilter",
                "description": "We plan to remove VirtualizedTable and ListPageFilter and useListPageFilter as PatternFly's Data View Table| is a suitable replacement for users, but first we must deprecrate these components. We should do this in 4.19 so the removal can happen sooner.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14967",
                        "type": "pullRequest",
                        "title": "CONSOLE-4541: deprecate VirtualizedTable and ListPageFilter and useLi...",
                        "body": "...stPageFilter"
                    }
                ]
            },
            "CONSOLE-4516": {
                "summary": "update or remove react-tagsinput",
                "description": "warning \" react-tagsinput@3.19.0\" has incorrect peer dependency \"react@^16.0.0 ^15.0.0 ^0.14.0\".",
                "epic_key": "CONSOLE-3945"
            },
            "CONSOLE-4505": {
                "summary": "Update i18next package",
                "description": "As a developer, I want to update the i18next and react-i18next dependencies in OpenShift Console to the latest compatible versions so that we can ensure better performance, security, and compatibility with new features. Acceptance Criteria: Update i18next and react-i18next to the latest stable versions. Verify compatibility with existing translations and ensure no breaking changes. Test the application to confirm that all internationalization features work as expected. Ensure all integration tests pass successfully. Technical Notes: Check the official i18next release notes Run npm outdated or yarn outdated to check the current and latest versions.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14881",
                        "type": "pullRequest",
                        "title": "CONSOLE-4505: Update i18next package"
                    }
                ]
            },
            "CONSOLE-4504": {
                "summary": "Align LogViewer theme with console theme",
                "description": "Instances of LogViewer are hard coded to use the dark theme. We should make that responsive to the user's choice the same we we are with the CodeEditor|",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14827",
                        "type": "pullRequest",
                        "title": "CONSOLE-4504: set LogViewer theme using console's theme",
                        "body": "After"
                    }
                ]
            },
            "CONSOLE-4503": {
                "summary": "Replace custom Banner with PatternFly equivalent",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14825",
                        "type": "pullRequest",
                        "title": "CONSOLE-4503: Replace custom Banner with PatternFly equivalent",
                        "body": "implements removes `co-global-notification` custom CSS. before: !before( after: !after( /label px-approved QE review: /cc @yapei code review: /cc @rhamilto for testing: - check if the kubeadmin banner looks okay - check if the user impersonation banner looks okay - create ConsoleNotification CR and see if those look okay - check if the telemetry banner looks okay"
                    }
                ]
            },
            "CONSOLE-4496": {
                "summary": "Replace custom Checkbox filter with PatternFly equivalent",
                "description": "The API Explorer Resource details Access review page utilizes a custom Checkbox filter. This custom component is unnecessary as PatternFly offers comparable functionality. We should replace this customer component with a PatternFly one. AC: Replace the CheckBox component for the Switch in the API Explorer",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14807",
                        "type": "pullRequest",
                        "title": "CONSOLE-4496: Replace custom Checkbox filter with PatternFly Switch",
                        "body": "I opted for Switch( over a multi-select toggle group( as `Switch` was a direct 1:1 replacement for `Checkbox` while maintaining the majority of the existing implementation and design. We can certainly revisit if folks disagree. Before After"
                    }
                ]
            },
            "CONSOLE-4464": {
                "summary": "Update PatternFly to official releases",
                "description": "Some of the PatternFly releases in are prereleases. Once final releases are available (v.6.2.0 is scheduled for the end of March), we should update to them. Also update to the same versions.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14979",
                        "type": "pullRequest",
                        "title": "CONSOLE-4464: Update PatternFly to latest"
                    },
                    {
                        "id": "14953",
                        "type": "pullRequest",
                        "title": "CONSOLE-4464: Update PatternFly to 6.2.0",
                        "body": "fixes code review: /assign @rhamilto qe review: /assign @yapei /label px-approved /label docs-approved"
                    },
                    {
                        "id": "14750",
                        "type": "pullRequest",
                        "title": "CONSOLE-4464: bump PatternFly 6 to latest available versions",
                        "body": "We ultimately want to land on the latest official releases (e.g., `6.2.0`), but this gets us closer to those final versions, providing time to vet the changes in the latest prerelease versions."
                    }
                ]
            },
            "CONSOLE-4462": {
                "summary": "Verify -theme-dark classes are still necessary and remove if not",
                "description": "Most of the -theme-dark classes defined in the console code base were for PF5 and are likely unnecessary in PF6 (although the version number was updated). We should evaluate each class and determine if it is still necessary. If it is not, we should remove it.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14891",
                        "type": "pullRequest",
                        "title": "CONSOLE-4462: Remove two more overrides",
                        "body": "the `skip-to-content` override is not required as it's already the PF default: the second one is my opinion that we should align with PatternFly and use their preferred background color. removed `text-uppercase` class as that no longer does anything. I have talked to @nicolethoen regarding the intended the PF topology background color before, and she confirmed with designers that the intended color is \"secondary grey\" before: !( after: !( follow up ticket, adding labels /label px-approved /label docs-approved /label qe-approved"
                    }
                ]
            },
            "CONSOLE-4443": {
                "summary": "Upgrade dynamic-demo-plugin to use PatternFly 6",
                "description": "The dynamic-demo-plugin in the openshift/console All affected components and styles are updated to comply with PatternFly 6 standards. Update integration tests, if necessary Plugin functionality is tested to ensure no regressions.",
                "epic_key": "CONSOLE-4325",
                "GITHUB": [
                    {
                        "id": "14682",
                        "type": "pullRequest",
                        "title": "CONSOLE-4443: Upgrade dynamic-demo-plugin to PatternFly 6",
                        "body": "Plan is to go ahead and merge this with prerelease versions and update to the final versions with (which itself will likely require multiple PRs)."
                    }
                ]
            },
            "CONSOLE-4437": {
                "summary": "Automated Content Security Policy testing in CI",
                "description": "In Console 4.18 we introduced an initial Content Security Policy| (CSP) implementation (CONSOLE-4263). This affects both Console web application as well as any dynamic plugins loaded by Console. In production, CSP violations are sent to telemetry service for analysis (CONSOLE-4272). We need a reliable way to detect new CSP violations as part of our automated CI checks. We can start with testing the main dashboard page of Console and expand to more pages as necessary. Acceptance criteria: Update the release repo, so the CSP violation test script is executed as part of Console CI checks.",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14858",
                        "type": "pullRequest",
                        "title": "CONSOLE-4437: Add CSP test to the test-prow-e2e.sh",
                        "body": "/assign @vojtechszocs @spadgett FYI"
                    }
                ]
            },
            "CONSOLE-4434": {
                "summary": "Update login page",
                "epic_key": "CONSOLE-4325"
            },
            "CONSOLE-4409": {
                "summary": "Refactor CodeEditor to use the PF equivalent",
                "description": "Description As a user already accustomed to PatternFly-based web applications, when I use the CodeEditor within OpenShift Console, I expect the same experience as the PF CodeEditor| Acceptance Criteria The CodeEditor adopts the Patternfly design The YAML language support remains the same Additional Details:",
                "epic_key": "CONSOLE-4132",
                "GITHUB": [
                    {
                        "id": "14835",
                        "type": "pullRequest",
                        "title": "CONSOLE-4409: Update monaco theming and sidebar logic",
                        "body": "follow up PR on issues.redhat.com/browse/CONSOLE-4409( adding labels /label qe-approved /label px-approved /label docs-approved - Fixes bug where PROCESSED_THEME is sometimes `systemDefault` when it shouldn't be - Uses a `Switch` for `CodeEditorField` sidebar toggle after: !image( code review: /cc @rhamilto"
                    }
                ]
            },
            "CONSOLE-4080": {
                "summary": "Address tech debt in KeyValueEntryForm component",
                "description": "The KeyValueEntryForm component needs to be refactored to address several tech debt issues: Rename to OpaqueSecretFormEntry Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
                "epic_key": "CONSOLE-4352"
            },
            "CONSOLE-4079": {
                "summary": "Address tech debt in GenericSecretForm component",
                "description": "The GenericSecretForm component needs to be refactored to address several tech debt issues: Rename to OpaqueSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
                "epic_key": "CONSOLE-4352",
                "GITHUB": [
                    {
                        "id": "14520",
                        "type": "pullRequest",
                        "title": "CONSOLE-4079,CONSOLE-4080: Address tech debt in GenericSecretForm and KeyValueEntryForm components",
                        "body": "h2 id=\"feature-fix\"CONSOLE Features and Fixes:/h2 Refactors GenericSecretForm and KeyValueEntryForm to be functional components !--The pull request title must be prefixed with a Jira issue or Bugzilla bug in order to be merged. For example: For e.g Features: - CONSOLE-XXXX: title For e.g Jira Bug Fixes: - OCPBUGS-XXXX: title For e.g Bugzilla Bug Fixes: - Bug XXXXXXX: title Combos: - CONSOLE-XXXX, Bug XXXXXXX: title-- h2 id=\"solution-description\"Solution description/h2 !-- Describe your code changes in detail and explain the solution or functionality -- This PR refactors the GenericSecretForm and KeyValueEntryForm to use functional components and makes code improvements to fit the standards we currently follow. h2 id=\"test-cases\"Test cases:/h2 !-- Outline any test cases here -- 1. Visit workloads - secrets - create secret - (Key/Value form) - fill form - create secret 2. Visit workloads - secrets - click on any secret of Opaque type - kebab menu - edit secret - edit contents of the form - save h2 id=\"additional-info\"Additional info:/h2 !-- Add any additional info here -- h2 id=\"screenshots\"Screen shots / gifs / design review:/h2 !-- Add screenshots or gifs for visual changes. Be sure to include before and after where relevant --"
                    }
                ]
            },
            "CONSOLE-4077": {
                "summary": "Address tech debt in BasicAuthSubform component",
                "description": "The BasicAuthSubform component needs to be refactored to address several tech debt issues: Rename to BasicAuthSecretForm Refactor into a function component Remove i18n withTranslation HOC pattern Improve type definitions",
                "epic_key": "CONSOLE-4352",
                "GITHUB": [
                    {
                        "id": "14631",
                        "type": "pullRequest",
                        "title": "CONSOLE-4077: Refactor BasicAuthSubform to be functional component",
                        "body": "h2 id=\"feature-fix\"CONSOLE Features and Fixes:/h2 Refactors BasicAuthSubform to be a functional component h2 id=\"solution-description\"Solution description/h2 !-- Describe your code changes in detail and explain the solution or functionality -- h2 id=\"reviewers-and-assignees\"Reviewers and assignees:/h2 h2 id=\"test-cases\"Test cases:/h2 h2 id=\"additional-info\"Additional info:/h2 h2 id=\"screenshots\"Screen shots / gifs / design review:/h2"
                    }
                ]
            },
            "CONSOLE-3960": {
                "summary": "Migrate react-helmet to react-helmet-async",
                "description": "Remove the react-helmet package since it is not supported in React 18. In React 19, we can remove react-helmet-async entirely and use the built-in functionality: AC: remove all usage of react-helmet from the console code remove the react-helmet dependency from the console remove shared modules for react-helmet",
                "epic_key": "CONSOLE-3945",
                "GITHUB": [
                    {
                        "id": "14876",
                        "type": "pullRequest",
                        "title": "CONSOLE-3960: Migrate `react-helmet` to `react-helmet-async`",
                        "body": "Thanks to the `react-helmet-async` authors for making the transition super intuitive :yum: test cases: - do titles update correctly when switching routes? - do the titles update when the brand is changed? (e.g., ROSA, OKD, etc) code review: /assign @TheRealJon qe review: /assign @yapei"
                    }
                ]
            },
            "CONSOLE-3414": {
                "summary": "Inconsistency in the loader/spinner/dots component used throughout the unified console",
                "description": "Inconsistency in the loader/spinner/dots component used throughout the unified console. The dots animation is used widely through the spoke clusters, but it is not a Patternfly component(This component was originally inhereted from CoreOS console). Spoke clusters also uses skeleton states on certain pages, which is a Patternfly component. Hub uses a mix of dots animation first for half a second, and then spinners and skeletons. Currently there is a discussion with PF to update with clearer guidelines. According to the current PF guidelines, we should be using the large spinner if we cannot anticipate the data being loaded, and the skeleton state if we do know. Link to doc|",
                "epic_key": "CONSOLE-4350",
                "GITHUB": [
                    {
                        "id": "14873",
                        "type": "pullRequest",
                        "title": "CONSOLE-3414: Fix flaky `Loading` tests",
                        "body": "fixes this flake: ``` {this.className.includes is not a function TypeError TypeError: this.className.includes is not a function at SVGSVGElement.eval (webpack://@knative-plugin/integration-tests/../../dev-console/integration-tests/support/pages/app.ts:28:32) at Context.eval (webpack://@knative-plugin/integration-tests/../../dev-console/integration-tests/support/pages/app.ts:27:36)} ``` adding labels for follow on ticket: /label px-approved /label docs-approved /label qe-approved code review: /assign @rhamilto"
                    },
                    {
                        "id": "14842",
                        "type": "pullRequest",
                        "title": "CONSOLE-3414: Replace `Loading` with PF `Spinner`",
                        "body": "before: after: tech debt, adding labels /label docs-approved /label px-approved code review: /assign @rhamilto qe review: /assign @yapei"
                    }
                ]
            }
        }
    },
    "OpenShift Node": {
        "stories": {
            "OCPNODE-2340": {
                "summary": "BYOPKI Container runtime config controller implementation",
                "description": "As an openshit developer, I want container runtime config controller to roll out BYOPKI config from ClusterImagePolicy CR to policy.json, so the customer can verifying the image signed with cosign BYOPKI",
                "epic_key": "OCPNODE-3039",
                "GITHUB": [
                    {
                        "id": "4886",
                        "type": "pullRequest",
                        "title": "OCPNODE-2340: update (Cluster)Imagepolicy PKI config to policy.json",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did Update controller code to rollout PKI configuration from ClusterImagePolicy and ImagePolicy CRD to policy.json - How to verify it - enable featuregate `featureSet: DevPreviewNoUpgrade` - script ( certificate chain for PKI - create test clusterimagepolicy or imagepolicy. ```yaml apiVersion: config.openshift.io/v1alpha1 kind: ClusterImagePolicy metadata: name: pki-policy spec: scopes: - example.com policy: rootOfTrust: policyType: PKI pki: caRootsData: base64 encoding of generated root-ca.pem caIntermediatesData: base64 encoding of generated intermediate-ca.pem pkiCertificateSubject: email: subjectAltName email hostname: subjectAltName DNS ``` - Description for the changelog !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "OCPNODE-3020": {
                "summary": "MCO Remove all the cgroupv1 references",
                "description": "This is the continuation of Remove the Cgroupsv1 references from the MCO code Bump ocp/api to include the enum removal of cgroup \"v1\" Remove the cgroupv1 references and the related kArgs settings Modify the UTs, bootstrap e2e tests",
                "epic_key": "OCPNODE-2841",
                "GITHUB": [
                    {
                        "id": "4964",
                        "type": "pullRequest",
                        "title": "OCPNODE-3020: Remove `cgroupv1` references",
                        "body": "Remove the references of `cgroupv1` - Do not support setting of `cgroupMode` field to `v1` of the `nodes.config.openshift.io` object - Remove the `cgroupv1` deprecation message References: API PR - Enhancement Proposal -"
                    }
                ]
            },
            "OCPNODE-2940": {
                "summary": "update admission pieces of o/k && o/cluster-kube-apiserver PRs for comments",
                "description": "and have some updated comments",
                "epic_key": "OCPNODE-2506",
                "GITHUB": [
                    {
                        "id": "1754",
                        "type": "pullRequest",
                        "title": "OCPNODE-2940: add support for minimumKubeletVersion"
                    },
                    {
                        "id": "2201",
                        "type": "pullRequest",
                        "title": "OCPNODE-2940: add minimumkubeletversion admission package",
                        "body": "!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: and developer guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here: 3. Ensure you have added or ran the appropriate tests for your PR: 4. If you want faster PR reviews, read how: 5. If the PR is unfinished, see how to mark it: -- What type of PR is this? !-- Add one of the following kinds: /kind bug /kind cleanup /kind documentation /kind feature Optionally add one or more of the following kinds if applicable: /kind api-change /kind deprecation /kind failing-test /kind flake /kind regression -- What this PR does / why we need it: + admission pieces, as requested by Which issue(s) this PR fixes: !-- Automatically closes linked issue when PR is merged. Usage: `Fixes issue number`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_ -- Fixes Special notes for your reviewer: Does this PR introduce a user-facing change? !-- If no, just write \"NONE\" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\". For more information on release notes see: -- ```release-note ``` Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.: !-- This section can be blank if this pull request does not require a release note. When adding links which point to resources within git repositories, like KEPs or supporting documentation, please reference a specific commit and avoid linking directly to the master branch. This ensures that links reference a specific point in time, rather than a document that may change over time. See here for guidance on getting permanent links to files: Please use the following format for linking documentation: - KEP: link - Usage: link - Other doc: link -- ```docs ```"
                    },
                    {
                        "id": "2104",
                        "type": "pullRequest",
                        "title": "OCPNODE-2940: add minimumkubeletversion package",
                        "body": "What type of PR is this? !-- Add one of the following kinds: /kind bug /kind cleanup /kind documentation /kind feature Optionally add one or more of the following kinds if applicable: /kind api-change /kind deprecation /kind failing-test /kind flake /kind regression -- /kind feature What this PR does / why we need it: Adds an additional admission feature that verifies all kubelets are above a certain version. WIP, TODOs inline Which issue(s) this PR fixes: !-- Automatically closes linked issue when PR is merged. Usage: `Fixes issue number`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_ -- Fixes Special notes for your reviewer: Does this PR introduce a user-facing change? !-- If no, just write \"NONE\" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\". For more information on release notes see: -- ```release-note ``` Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.: !-- This section can be blank if this pull request does not require a release note. When adding links which point to resources within git repositories, like KEPs or supporting documentation, please reference a specific commit and avoid linking directly to the master branch. This ensures that links reference a specific point in time, rather than a document that may change over time. See here for guidance on getting permanent links to files: Please use the following format for linking documentation: - KEP: link - Usage: link - Other doc: link -- ```docs ```"
                    }
                ]
            },
            "OCPNODE-2877": {
                "summary": "API Enhancement Remove CgroupModeV1 from openshift - Enhancement Proposal, API",
                "description": "Remove the CgroupModeV1 config option from the openshift/api repository Ref: Add a CRD validation check on the CgroupMode field of the nodes.config spec to avoid the update to \"v1\" and only allow the \"v2\" and \"\" as valid values. Latest update: Raise a PR with the updated enhancement proposal to handle the removal of cgroupsv1",
                "epic_key": "OCPNODE-2841",
                "GITHUB": [
                    {
                        "id": "2181",
                        "type": "pullRequest",
                        "title": "OCPNODE-2877: Remove support to configure cgroupsv1 in OCP",
                        "body": "- Removing support to configure cgroupsv1 in the OCP clusters. - Removed the enum validation of \"v1\" for the `cgroupMode` field of the `nodes.config.openshift.io` object. - Also added integration tests to validate the enum removal on the `cgroupMode` field Enhancement Proposal Ref: Summary: - This PR allows to block the user from setting `cgroupMode v1` - A change would be added for 4.18 in MCO to set machine-config cluster operator's `Upgradeable=False` when the `cgroupMode` is found to be `v1` and request users to update to `v2` - All the clusters upgrading to 4.19 have to update to the minimum version of 4.18.z containing the above changes. This can be achieved through the cincinnati-graph-data repo"
                    }
                ]
            },
            "OCPNODE-2842": {
                "summary": "API CONTD. Removal of Cgroupv1 code implementation",
                "description": "This story involves the following code changes Implement code changes at MCO Remove cgroupv1 based code Add an upgradeable condition set to false of the cluster operator if the cluster is using cgroupv1 Add a CEL validation to reject the setting of cgroupv1 (based on the latest enhancement proposal) Add test cases to test the removal of cgroupv1 functionality Implement code changes at openshift/api Implement code changes at openshift/kubernetes (if we use admission handler to reject setting of cgroupv1)",
                "epic_key": "OCPNODE-2841",
                "GITHUB": [
                    {
                        "id": "4822",
                        "type": "pullRequest",
                        "title": "OCPNODE-2842: Set Upgradeable=False when cluster is on cgroup v1",
                        "body": "- What I did Added code to set the cluster operator's status to `Upgradeable=False` when a cluster is found to be on cgroup v1 - How to verify it Update the `CgroupMode` field of ndes.config object to `v1` and verify that the MCO cluster operator's status has the `Upgradeable=False` condition - Description for the changelog !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: -- Cgroupsv1 support deprecation condition/message has been added in the last release and this PR helps in updating all the clusters to cgroup v2 before upgrading to ocp 4.19 References: - Enhancement Proposal: - API PR:"
                    }
                ]
            },
            "OCPNODE-2596": {
                "summary": "end to end CI jobs for image signing",
                "description": "This story is to track the CI jobs that need to be added to test `ClusterImagePolicy` and `ImagePolicy` API. coverd configuration: testing the verification coordinates configs from image.config.openshift.io/cluster: support scopes from allowedRegistries ( covered root of trust type: Public key| Rekor) The tests should cover all supported configuration. track future tests OCPNODE-2951",
                "epic_key": "OCPNODE-2619",
                "GITHUB": [
                    {
                        "id": "29572",
                        "type": "pullRequest",
                        "title": "Revert \"OCPNODE-2596: Add SigstoreImageVerification e2e tests\"",
                        "body": "Reverts openshift/origin29530 Per OpenShift policy( we are reverting this breaking change to get CI and/or nightly payloads flowing again. Multiple payload failures due to periodic-ci-openshift-release-master-ci-4.19-e2e-aws-ovn-techpreview-serial failures To unrevert this, revert this PR, and layer an additional separate commit on top that addresses the problem. Before merging the unrevert, please run these jobs on the PR and check the result of these jobs to confirm the fix has corrected the problem: ``` /payload-job periodic-ci-openshift-release-master-ci-4.19-e2e-aws-ovn-techpreview-serial ``` periodic-ci-openshift-release-master-ci-4.19-e2e-aws-ovn-techpreview-serial( started failing in 4.19.0-0.nightly-2025-03-01-032247( Reviewing the event timing indicate they are occurring at the time these tests are running. ``` sig-arch events should not repeat pathologically for ns/openshift-machine-config-operator { 1 events happened too frequently event happened 24 times, something is wrong: namespace/openshift-machine-config-operator hmsg/73d40b8baf machineconfigpool/master - reason/DeferringOperatorNodeUpdate Deferring update of machine config operator node ip-10-0-81-49.ec2.internal (10:53:30Z) result=reject } ```"
                    },
                    {
                        "id": "29530",
                        "type": "pullRequest",
                        "title": "OCPNODE-2596: Add SigstoreImageVerification e2e tests",
                        "body": "This PR adds e2e tests for Clusterimagepolicy and imagepolicy (`SigstoreImageVerification` tech preview featuregate)"
                    }
                ]
            },
            "OCPNODE-2339": {
                "summary": "Add fields to ClusterImagePolicy API for BYOPKI",
                "description": "As an openshift developer, I want to extend the fields of ClusterImagePolicy CRD for BYOPKI verification, so the containerruntimeconfig controller can roll out the configurationto policy.json for verification.",
                "epic_key": "OCPNODE-2269",
                "GITHUB": [
                    {
                        "id": "2088",
                        "type": "pullRequest",
                        "title": "OCPNODE-2339: Add PKI field to (cluster)imagepolicy",
                        "body": "Add PKI to clusterimagepolicy with DevPreview featuregate SigstoreImageVerificationPKI. This field allows verification of image signature created by cosign BYOPKI feature. The enhancement: hold this PR before"
                    }
                ]
            }
        },
        "epics": {
            "OCPNODE-3039": {
                "summary": "Tech Preview Support BYOPKI for image verification in OCP",
                "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPNODE-2841": {
                "summary": "Remove Cgroup v1 from OCP in 4.19",
                "description": "OCP/Telco Definition of Done Epic Goal Remove the support for cgroup v1 in 4.19 Why is this important? Without dependant components like systemd, RHCOS moving away from cgroups v1 it is important for the node to make this move as well. Scenarios As a system administrator I would like to make sure my cluster doesn't use cgroup v1 from 4.19 onwards Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPNODE-2506": {
                "summary": "GA User Namespaces",
                "description": "Epic Goal Prepare user namespaces for GA by enhancing SCC support and testing Why is this important? Enable nested containers use cases and enhance security Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPNODE-2619": {
                "summary": "Move ClusterImagePolicy, ImagePolicy to v1",
                "description": "OCP/Telco Definition of Done Epic Goal This epic tracks the work needed to move the ClusterImagePolicy API from v1alpha1 to v1 in OCP 4.19. It does not include any new feature requests, which will be tracked by other epics-just the API upgrade process, CI jobs and related tasks. The workflow to move to GA: v1 types in o/api client-go to generate v1 Add v1 manifest to payload and update MCO to v1 APIs update featuregate to default enabled add the v1 manifest to the payload remove the v1alpha1 manifest from the payload Wait 1-2 weeks - feature promotion in o/api And do those all in order. For 3 here, those PRs that need to be simul-merged Why is this important? Moving the ClusterImagePolicy API to a stable version v1 and announcing that OpenShift now supports Sigstore verification are key steps in helping customers strengthen their software supply chain. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPNODE-2269": {
                "summary": "Dev Preview Support BYOPKI for image verification in OCP",
                "description": "OCP/Telco Definition of Done Epic Goal Support BYOPKI for image verification in OCP Why is this important? As an administrator of an independent org, I would like to verify our container images using our own CA. Scenarios Verify container images using own CA Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Cloud": {
        "stories": {
            "OCPCLOUD-2895": {
                "summary": "AWS Add ElasticFabricAdapter to conversion library",
                "description": "Background We implemented `ElasticFabricAdapter` upstream. We will get this feature in openshift after the linked rebase card merges. We need to update the conversion library to convert the new NetworkInterfaceType field between MAPI and CAPPI. Steps Add conversion for NetworkInterfaceType to the conversion library Stakeholders Cluster Infra Definition of Done NetworkInterfaceType is being converted Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2809",
                "GITHUB": [
                    {
                        "id": "280",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2895: Add AWS networkInterfaceType to conversion",
                        "body": "This PR enables the conversion of networkInterfaceType that was added to CAPI for feature parity."
                    }
                ]
            },
            "OCPCLOUD-2860": {
                "summary": "Core CAPI should support machine to node annotations propagation",
                "description": "User Story As a user I want to be able to add annotations to my machines, and have them propagate to the nodes. This will allow me to use the labels for other tasks e.g selectors. Background _Currently, MAPI supports propagating labels from machines to nodes, but CAPI does not. When we move to CAPI we will lose this feature._ _See Relevant upstream issues: Steps Understand why the discrepancy exists Determine how much work it would be for the NodeLink controller to copy the annotations Chat with upstream to see if the idea of unrestricted annotation propagation through some mechansim is palletable. Come back to the group and decide a course of action. Stakeholders Our users, who currently have this feature. Definition of Done - Code is implemented to synchronize annotations from a CAPI Machine to a Node - Our manifests for CAPI include the \"--additional-sync-machine-annotations=.\" argument. The fully generated manifests are at",
                "epic_key": "OCPCLOUD-2706",
                "GITHUB": [
                    {
                        "id": "233",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2860: Enable propagation of Machine annotations to Nodes",
                        "body": "Includes a cherry pick of the upstream release-1.9 backport PR."
                    }
                ]
            },
            "OCPCLOUD-2716": {
                "summary": "Core Handle Machine owner references translation between MAPI and CAPI",
                "description": "Background Presently, the mapi2capi and capi2mapi code cannot handle translations of owner references. We need to be able to map CAPI/MAPI machines to their correct CAPI/MAPI MachineSet/CPMS and have the owner references correctly set. This requires identifying the correct owner and determining the correct UID to set. This will likely mean extending the conversion utils to be able to make API calls to identify the correct owners. Owner references for non-MachineSet types should still cause an error. Steps Add a client to the conversion util constructors (members of the conversion structs?) OR handle this outside of the conversion library? Work out the correct way to convert MachineSet/CPMS owner references between namespaces Stakeholders Cluster Infra Definition of Done Owner references are correctly converted between MAPI and CAPI machines Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "271",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2716: Handle Machine owner references translation between MAPI and CAPI",
                        "body": "This PR adds support to MachineSync and MachineSetSync controller for converting OwnerReferences in mirrored MachineSets, Machines and InfraMachines have correct ownerReferences."
                    }
                ]
            },
            "OCPCLOUD-2713": {
                "summary": "AWS Handle credentials secret conversion to CAPI",
                "description": "Background In MAPA, we provide a way for a user to specify the credentials to use for creating and managing AWS resources. This secret comes from a CredentialsRequest and is created either by CCO, or manually, with the name `aws-cloud-credentials`. Any Machine/MachineSet referencing these credentials is effectively using the \"default\". In CAPA, the default is to us the cluster identity ref to work out what to do. When not specified, it will fallback to using the controllers role, which we populate today using a credentials request. Therefore, any Machine using the default in MAPA/CAPA, has an option to be converted across. Where we then have an issue, is converting non-standard credentials. If any user has created a non-standard credential, we must set a static identity| that would then be used for the entire cluster. We must work out how to message about this/how to handle this. Initially, we can block the conversion and suggest a KCS to allow the user to set the AWSCluster IdentityRef, once the identity ref is configured, we can ignore the credentials secret. Steps Implement detection and conversion of the default credentials secret as described above Add logic to detect non-default credentials and return an appropriate error message Create a KCS to explain the steps necessary to use a custom credential Stakeholders Cluster Infra Definition of Done Credentials secrets are converted/users are told what to do/how to convert across Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2809",
                "GITHUB": [
                    {
                        "id": "282",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2713: Handle credential fields in both mapi2capi and capi2mapi",
                        "body": "See individual commits for each piece of work. Also modified the test.sh script so that I didn't have to run all test suites every time."
                    }
                ]
            },
            "OCPCLOUD-2712": {
                "summary": "AWS Understand security groups differences in MAPA/CAPA",
                "description": "Background When converting CAPI2MAPI, we convert CAPA's `AdditionalSecurityGroups` into the security groups for MAPA. While this looks correct, there are also fields like `SecurityGroupOverrides` which when present currently, would cause an error. We need to understand how security groups work today in MAPA, compare that to CAPA, and be certain that we are correctly handling the conversion here. Is CAPA doing anything else under the hood? Is it currently applying extra security groups that are standard that would otherwise cause issues? Steps Understand how security groups work in CAPA and MAPA Determine if our current conversion of security groups is appropriate and understand the role of securityGroupOverrides Update documentation/make appropriate changes to the security groups conversion based on the above findings. Stakeholders Cluster infra Definition of Done We are confident that converted machines behave correctly with respect to the security group configuration. Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2809",
                "GITHUB": [
                    {
                        "id": "279",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2712: Update comments based on Security Group Investigation",
                        "body": "Hello! This is a PR to update some of the comments left behind during the conversion work. The investigation discovered that the current behaviour is what we want, so I just updated some comments to leave information for future reference. Thanks!"
                    }
                ]
            },
            "OCPCLOUD-2648": {
                "summary": "Handle deletion mechanics of MAPI/CAPI migration machine sync controller",
                "description": "Background To ensure higher level objects continue to operate as expected (cluster-autoscaler,mhc) we define a synchronisation of deletions for MAPI and CAPI mirrors. The behaviours outlined below will ensure that the resources continue to operate as expected and should be implemented in both the Machine and MachineSet controllers. Behaviours Ensure sync.machine.openshift.io/finalizer is present on both copies of mirrored resources Propagate deletionTimestamp from authoritative resource to non-authoritative Propagate deletionTimestamp if non-authoritative and has an owner reference (eg Machine owned by MachineSet) Remove sync finalizer from both resources when authoritative deletion finalizer is removed (start with non-authoritative) Steps Implement finalizer addition and removal in Machine and MachineSet synchronization controllers based on behaviours outlined above Stakeholders Cluster Infra Definition of Done Deletion of MAPI/CAPI resources is synchronised as defined above Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "278",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2648: Machine sync controller deletion logic",
                        "body": "This change adds deletion logic for Machines. It introduces a sync finalizer to coordinate synchronization of mirrored resources. If we are reconciling deletions MAPI - CAPI, we ensure the MAPI specific finalizers are removed before removing our sync finalizer. If we are reconciling deletions CAPI - MAPI, we ensure that the CAPI InfraMachine has it's specific (per provider) finalizer removed by the CAPI controllers, before removing our sync finalizer. This must happen before the CAPI Machine controller will remove the CAPI machine deletion finalizer. Once we observe this, we remove our sync finalizer allowing deletion of mirrored objects. This has been tested in both directions on a live cluster."
                    }
                ]
            },
            "OCPCLOUD-2647": {
                "summary": "Implement CAPI to MAPI Machine conversion",
                "description": "Background To enable CAPI MachineSets to still mirror MAPI MachineSets accurately, and to enable MAPI MachineSets to be implemented by CAPI MachineSets in the future, we need to implement a way to convert CAPI Machines back into MAPI Machines. These steps assume that the CAPI Machine is authoritative, or, that there is no MAPI Machines. Behaviours If no Machine exists in MAPI But the CAPI Machine is owned, and that owner exists in MAPI Create a MAPI Machine to mirror the CAPI Machine MAPI Machines should set authority to CAPI on create If a MAPI Machine exists Convert infrastructure template from InfraMachine to providerSpec Update spec and status fields of MAPI Machine to reflect CAPI Machine On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement conversion based on the behaviours outlined above using the CAPI to MAPI conversion library Stakeholders Cluster Infra Definition of Done When a CAPI MachineSet scales up and is mirrored in MAPI, the CAPI Machine gets mirrored into MAPI Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "265",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2647: CAPI to MAPI Machine Synchronisation"
                    },
                    {
                        "id": "239",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2647: Updates machine-api-migration cmd",
                        "body": "Updates the machine-api-migration cmd to: - No longer crash, due to using a client from an uninitialised manager - Have the appropriate schemes registered (so far) - Stop using MetricsBindAddr, which is removed in 1.9 and use DiagnosticsAddress instead. This avoids a port clash when starting the manager. Now the command runs without crashing, we won't cause the `cluster-capi-operator` deployment to crashloop. ```bash I1127 13:42:59.722330 35247 main.go:201 MachineAPIMigration: starting AWS controllers I1127 13:42:59.722605 35247 main.go:245 Starting manager I1127 13:42:59.722849 35247 server.go:208 \"Starting metrics server\" logger=\"controller-runtime.metrics\" I1127 13:42:59.722874 35247 server.go:83 \"starting server\" name=\"health probe\" addr=\":9441\" I1127 13:42:59.723277 35247 leaderelection.go:254 attempting to acquire leader lease openshift-cluster-api/machine-api-migration-leader... I1127 13:42:59.898958 35247 server.go:247 \"Serving metrics server\" logger=\"controller-runtime.metrics\" bindAddress=\":8443\" secure=true ```"
                    }
                ]
            },
            "OCPCLOUD-2645": {
                "summary": "Implement MAPI to CAPI Machine conversion",
                "description": "Background For the Machine controller, we need to implement a forward conversion, converting the MachineAPI Machine to ClusterAPI. This will involve creating the CAPI Machine if it does not exist, and managing the Infrastructure Machine. This card covers the case where MAPI is currently authoritative. Behaviours Create Cluster API mirror if not present CAPI mirror should be paused on create Names of mirror should be 1:1 with original Manage InfraMachine creation by converting MAPI providerSpec InfraMachine should be named based on the name of the Machine (to mirror CAPI behaviour) InfraMachine should have appropriate owner references and finalizers created Ensure CAPI Machine spec and status overwritten with conversion from MAPI Ensure Labels/Annotations copied from MAPI to CAPI On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement behaviours described above to convert MAPI Machines to CAPI Machines using conversion library Stakeholders Cluster Infra Definition of Done MAPI Machines create paused CAPI mirrors Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "258",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2645: MAPI to CAPI Machine synchronization",
                        "body": "Follow up on to perform the machine synchronisation."
                    }
                ]
            },
            "OCPCLOUD-2564": {
                "summary": "Implement migration controller to handle authority transitions",
                "description": "Background describes a process in which the handover of the authoritative API is transitioned between Machine API and Cluster API. This process involves checking for a synchonized condition on the Machine API resource and relying on the synchronized generation and the Paused condition on both Machine API and Cluster API resoucres. A control loop should be built to handle just the transitional logic. Keeping the control loop separate will make this logic easy to reason about and easy to test. The actual sync of resources will be handled in a separate, isolated loop. It will: For each resource that supports the authoritative API Wait for spec.authoritativeAPI to be different to status.authoritativeAPI (user initiated a migration) Set the status.authoritativeAPI to migrating Wait for the previous authoritative API resource to report Paused condition true Check the synchronized condition and synchronizedGeneration fields are up to date Move the status authoritativeAPI to match the spec If the synchronized generation is not up to date, wait until the sync loop updates it Behaviours Watch Machines/MachineSets for .spec.authoritativeAPI not equal .status.authoritativeAPI Should set .status.authoritativeAPI to match .spec.authoritativeAPI when .status.authoritativeAPI is empty Exit here Check Synchronized condition on MAPI resource True, exit if not Move status.authoritativeAPI to Migrating If moving away from CAPI, pause the CAPI resource Wait on Paused condition True on old authoritative resource Check Syncrhonized condition and .status.synchronizedGeneration are up to date on MAPI resource, exit if not Error if condition False Requeue later if synchronizedGeneration not up to date Add appropriate Finalizer to new authoritative API Requeue here until we observe this in cache Remove Finalizer from old authoritative API Requeue here until we observe this in cache Move status.authoritativeAPI to match spec.authoritativeAPI and reset status.synchronizedGeneration If moving to CAPI, unpause the CAPI resource Steps Create a new control loop in the Cluster-CAPI-Operator repo Implement the logic as described above Add tests to test the transitional behaviour using envtest Stakeholders Cluster Infra Definition of Done Sync loop for migration is implemented and tested Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "268",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2564: add migration controllers",
                        "body": "This PR adds the MAPI-CAPI migration controllers, resposible for handling the authoritativeness switching between MAPI and CAPI."
                    }
                ]
            },
            "OCPCLOUD-2500": {
                "summary": "Update CAS to recognize both upstream and openshift scale from zero annotations",
                "description": "User Story As a developer, in order to deprecate the old annotations, we will need to carry both for at least one release cycle. Updating the CAO to apply the upstream annotations, and the CAS to accept both (preferring upstream), will allow me to properly deprecate the old annotations. Background to help the transition to the upstream scale from zero annotations, we need to have the CAS recognize both sets of annotations, preferring the upstream, for at least one release cycle. this will allow us to have a window of deprecation on the old annotations. Steps update CAS to recognize both annotations add a unit test to ensure prioritization works properly Stakeholders openshift eng Definition of Done CAS can recognize both sets of annotations Docs n/a Testing unit testing for priority behavior",
                "epic_key": "OCPCLOUD-2136",
                "GITHUB": [
                    {
                        "id": "335",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2500: Update to recognize both upstream and openshift ScaleFromZero annotations",
                        "body": "What type of PR is this? !-- /kind feature -- What this PR does / why we need it: Hello! This is a PR to update the autoscaler to recognize both the upstream and openshift ScaleFromZero annotations, while favouring the upstream annotations if they exist. Thanks! Does this PR introduce a user-facing change? !-- If no, just write \"NONE\" in the release-note block below. If yes, a release note is required: Enter your extended release note in the block below. If the PR requires additional action from users switching to the new release, include the string \"action required\". For more information on release notes see: -- ```release-note ``` Additional documentation e.g., KEPs (Kubernetes Enhancement Proposals), usage docs, etc.: !-- This section can be blank if this pull request does not require a release note. When adding links which point to resources within git repositories, like KEPs or supporting documentation, please reference a specific commit and avoid linking directly to the master branch. This ensures that links reference a specific point in time, rather than a document that may change over time. See here for guidance on getting permanent links to files: Please use the following format for linking documentation: - KEP: link - Usage: link - Other doc: link -- ```docs ```"
                    }
                ]
            },
            "OCPCLOUD-2202": {
                "summary": "Each cluster should have a Cluster object in the openshift-cluster-api namespace",
                "description": "Background The CAPI operator should ensure that, for clusters that are upgraded into a version of openshift supporting CAPI, that a Cluster object exists in the openshift-cluster-api namespace with the name as the infratructure ID of the Cluster. The cluster spec should be populated with the reference to the infrastructure object and the status should be updated to reflect that the control plane is initialized. Steps Extend the existing cluster| controller to manage the Cluster resource within CAPI operator Ensure that on supported platforms it populates a Cluster object for the cluster Add documentation to the CAPI operator to describe the controller and its operation Add testing to track the operation of the controller Ensure the controller does not interfere with Cluster resources that were not created by it Stakeholders Cluster Infra Definition of Done When I install a tech preview cluster, I should be able to `oc get cluster -n openshift-cluster-api` and have a result returned without any action on my part Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2634",
                "GITHUB": [
                    {
                        "id": "236",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2202: Create and manage core cluster object",
                        "body": "Hello! This is a PR to ensure that each cluster should have a cluster object in the `openshift-cluster-api` namespace, as well as updating the unit/e2e tests to reflect this. Thank you!"
                    }
                ]
            },
            "OCPCLOUD-2880": {
                "summary": "Implement MachineSet label selector and Machine Label MAPI-CAPI Conversion",
                "description": "Background MAPI MachineSets have label selector to machine Label mapping. There are CAPI equivalents for these labels. During MachineSet/Machine conversion we should convert to the MAPI/CAPI equivalent. {code:java} machine.openshift.io/cluster-api-cluster: clusterID --- cluster.x-k8s.io/cluster-name: clusterID{code} {code:java} machine.openshift.io/cluster-api-machine-type: role machine.openshift.io/cluster-api-machine-role: role --- node-role.kubernetes.io/role: \"\" {code} {code:java} machine.openshift.io/cluster-api-machineset: clusterID-role-region --- cluster.x-k8s.io/set-name:clusterID-role-region {code} Steps Look into upstream code to determine if there are any possible values for the above labels that we have not accounted for. Determine if there is difference between MAPI type and role label and if they can be consolidated into one CAPI label. Add the conversion to the conversion library. Stakeholders Cluster Infra Definition of Done Conversion library converts between MAPI-CAPI for labels an Label selector on machines and machinesets Docs Add docs requirements for this card Testing Add unit tests",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "281",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2880: label selector and labels conversion, fixes to sync controllers and conversions",
                        "body": "This PR contains: - assorted fixes to the sync controllers and the conversion libraries that I picked up while running the migration controllers doing full, real, back and forth MAPI-CAPI-MAPI migrations of machines and machine sets. - full labels and annotations syncing between MAPI and CAPI, while also adding node labels and annotations propagation from CAPI machines down to their nodes - machine sets selector bidirectional conversion - improved diffing and diffs readability for machines and machine sets - temporary hardcoded credentials secret conversion support for the default case to allow for migrations to momentarily work - adds several `TODO(docs)` for lossy conversions that we might need to document (cc. @jeana-redhat)"
                    }
                ]
            },
            "OCPCLOUD-2824": {
                "summary": "Configure rebasebot periodics to run on cluster api repositories",
                "description": "User Story As a developer I want rebasebot to periodically update cluster api repositories. Background Created this card to track merging my PR that was hit with few blockers. We ran the required rebases as rehearsal jobs bud did not merge the PR. Steps Move capi manifest gen hook script from the open PR to cluster-capi-operator Configure latests source fetching after implemented in rebasebot (OCPCLOUD-2757) Merge Stakeholders Cluster Infra Definition of Done Cluster api rebases run as periodic ci job",
                "epic_key": "OCPCLOUD-2593",
                "GITHUB": [
                    {
                        "id": "257",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2824: Add manifest generation script for rebasebot",
                        "body": "This script will be run by rebasebot during rebase of the CAPI repositories. The location is specified as parameter to it. It is downloaded from this repo and executed on the repo that is being rebased."
                    }
                ]
            },
            "OCPCLOUD-2718": {
                "summary": "AWS Handle no MAPI ebs volumesize",
                "description": "Background VolumeSize on the block device mapping spec in MAPA is currently optional (and if is not set we send an empty value to AWS and let it choose for us), where it is required and a minimum of 8gb in CAPA. We need to determine an appropriate behaviour for when the value is unset. Steps Check historically on the installer to see what value it typically has set (in the AMI, and if that changed overtime) Determine an appropriate minimum size for the root volume in OpenShift When not set, default the CAPA volume size to an appropriate value based on the above Adjust conversion logic based on the above Stakeholders Cluster Infra Definition of Done Machines with no volume size in MAPI can be converted to CAPI Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2809",
                "GITHUB": [
                    {
                        "id": "260",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2718: Add default VolumeSize for MAPI to CAPI conversion",
                        "body": "Hello! This is a draft PR to get feedback for the default VolumeSize for the MAPI to CAPI conversion for AWS. Thanks!"
                    }
                ]
            },
            "OCPCLOUD-2680": {
                "summary": "Core CAPI should support machine to node label propagation",
                "description": "User Story As a user I want to be able to add labels/taints/annotations to my machines, and have them propagate to the nodes. This will allow me to use the labels for other tasks e.g selectors. Background _Currently, MAPI supports propagating labels from machines to nodes, but CAPI does not. When we move to CAPI we will lose this feature._ _See Relevant upstream issues: Steps Understand why the discrepancy exists Determine how much work it would be for the NodeLink controller to copy the labels Chat with upstream to see if the idea of unrestricted label propagation through some mechansim is palletable. Come back to the group and decide a course of action. Stakeholders Our users, who currently have this feature. Definition of Done - Code is implemented upstream to sync labels from a Machine to a Node - Our manifests include the \"--additional-sync-machine-labels=.\" argument. The generated manifests are at",
                "epic_key": "OCPCLOUD-2706",
                "GITHUB": [
                    {
                        "id": "229",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2680: Enable machine to node propagation",
                        "body": "This change contains two commits: - Enable label syncing from Machines to Nodes. Maps to and - ~~Enable annotation syncing from Machines to Nodes. Maps to & this will be handled in a separate PR so as to unblock OCPCLOUD-2680( ~~As of this writing, OCPCLOUD-2680( is merged upstream, but is not yet in a released version. It should be in v1.9.5 when that is out.~~ We now have the necessary commit in our downstream fork. ~~If someone takes over this PR or has to split it up, you can get the same generated manifests with an updated version of CAPI with these commands (you'll need to include the updated `Makefile` and the two new kustomize files):~~ ``` cd openshift PROVIDER_VERSION=v1.9.4 make ocp-manifests ```"
                    },
                    {
                        "id": "231",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2680: Merge (068c0f3) into master"
                    }
                ]
            },
            "OCPCLOUD-2644": {
                "summary": "Implement MAPI to CAPI MachineSet conversion",
                "description": "Background For the MachineSet controller, we need to implement a forward conversion, converting the MachineAPI MachineSet to ClusterAPI. This will involve creating the CAPI MachineSet if it does not exist, and managing the Infrastructure templates. This card covers the case where MAPI is currently authoritative. Behaviours Create Cluster API mirror if not present CAPI mirror should be paused on create Names of mirror should be 1:1 with original Manage InfraTemplate creation by converting MAPI providerSpec InfraTemplate naming should be based on hash to be able to deduplicate InfraTemplate naming should be based on parent resources InfraTemplate should have ownerReference to CAPI MachineSet If template has changed, remove ownerReference from old template. If no other ownerReferences, remove template. Should be identifiable as created by the sync controller (annotated?) Ensure CAPI MachineSet spec and status overwritten with conversion from MAPI Ensure Labels/Annotations copied from MAPI to CAPI On failures Set Synchronized condition to False and report error on MAPI resource On success Set Synchronized condition to True on MAPI resource Set status.synchronizedGeneration to match the auth resource generation Steps Implement MAPI to CAPI conversion by leveraging library for conversion and applying above MachineSet level rules Stakeholders Cluster Infra Definition of Done When a MAPI MachineSet exists, a CAPI MachineSet is created and kept up to date if there are changes Docs Add docs requirements for this card Testing Explain testing that will be added",
                "epic_key": "OCPCLOUD-2120",
                "GITHUB": [
                    {
                        "id": "237",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2644: MAPI to CAPI MachineSet synchronization",
                        "body": "These change: - Implements MAPI to CAPI sync - Builds out test suite"
                    }
                ]
            },
            "OCPCLOUD-2642": {
                "summary": "Setup OCP build of Azure Service Operator",
                "description": "User Story The Cluster API provider Azure has a deployment manifest that deploys Azure service operator from mcr.microsoft.com/k8s/azureserviceoperator:v2.6.0 image. We need to set up OpenShift builds of the operator and update the manifest generator to use the OpenShift image. Background Azure have split the API calls out of their provider so that they now use the service operator. We now need to ship service operator as part of the CAPI operator to make sure that we can support CAPZ. Steps -Request for the ASO repo to be created and build in Openshift- -Set up release repo configuration and basic testing for ASO repo- -Set up ART build for ASO repo- Fill out prodsec survey ( Update the manifest generator in cluster-capi-operator to replace the image with our image stream. Manifest generator should know how to filter parts of ASO so that we only ship the parts of Azure care about Create manifests for deploying the subset of required ASO that we need Stakeholders Cluster Infrastructure Definition of Done CAPZ deploys OpenShift version of ASO Docs Testing",
                "GITHUB": [
                    {
                        "id": "235",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2642: Reference ASO image for installation",
                        "body": "Draft until merges and the image is actually built."
                    }
                ]
            }
        },
        "epics": {
            "OCPCLOUD-2809": {
                "summary": "MAPI/CAPI Feature Parity (AWS) (Tech Preview)",
                "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPCLOUD-2706": {
                "summary": "MAPI/CAPI Feature Parity (Core) (Tech Preview)",
                "description": "OCP/Telco Definition of Done Epic Goal To bring MAPI and CAPI to feature parity and unblock conversions between MAPI and CAPI resources Why is this important? Blocks migration to Cluster API Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPCLOUD-2780": {
                "summary": "Support AWS Capacity Blocks for ML in MAPI",
                "description": "OCP/Telco Definition of Done Epic Goal MAPI supports CapacityReservations, but it has the missing functionality that supports AWS capacity bloks. Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "125",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2780: select instance 'MarketType' option",
                        "body": "This PR added the support for selecting the Instance MarketType option i.e. OnDemand, CapacityBlock, and Spot. By choosing option CapacityBlock, instance support capacity blocks for ML provided by AWS."
                    },
                    {
                        "id": "1329",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2780: Validate aws marketType webhook",
                        "body": "add the webhook validation for the \"MarketType\" API field of \"AwsMachineProviderSpec\". Supported PR in"
                    }
                ]
            },
            "OCPCLOUD-2120": {
                "summary": "Implement Migration core for MAPI to CAPI (Tech Preview)",
                "description": "OCP/Telco Definition of Done Epic Goal Create the core/common tooling needed to enable the migration designed in OCPCLOUD-1578 To allow providers to individually migrate from MAPI to CAPI Implementation plan in Why is this important? We need to build out the core so that development of the migration for individual providers can then happen in parallel Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPCLOUD-2136": {
                "summary": "Update autoscaling annotations to accommodate upstream keys",
                "description": "Epic Goal Update the scale from zero autoscaling annotations on MachineSets to conform with the upstream keys, while also continuing to accept the openshift specific keys that we have been using. Why is this important? This change makes our implementation of the cluster autoscaler conform to the API that is described in the upstream community. This reduces the mental overhead for someone that knows kubernetes but is new to openshift. This change also reduces the maintenance burden that we carry in the form of addition patches to the cluster autoscaler. By changing our controllers to understand the upstream annotations we are able to remove extra patches on our fork of the cluster autoscaler, making future maintenance easier and closer to the upstream source. Scenarios A user is debugging a cluster autoscaler issue by examining the related MachineSet objects, they see the scale from zero annotations and recognize them from the project documentation and from upstream discussions. The result is that the user is more easily able to find common issues and advice from the upstream community. An openshift maintainer is updating the cluster autoscaler for a new version of kubernetes, because the openshift controllers understand the upstream annotations, the maintainer does not need to carry or modify a patch to support multiple varieties of annotation. This in turn makes the task of updating the autoscaler simpler and reduces burden on the maintainer. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Scale from zero autoscaling must continue to work with both the old openshift annotations and the newer upstream annotations. Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - OpenShift code and tests merged: link to meaningful PR or GitHub Issue DEV - OpenShift documentation merged: link to meaningful PR or GitHub Issue DEV - OpenShift build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - OpenShift documentation merged: link to meaningful PR please note, the changes described by this epic will happen in OpenShift controllers and as such there is no \"upstream\" relationship in the same sense as the Kubernetes-based controllers."
            },
            "OCPCLOUD-2634": {
                "summary": "(Infrastructure) Cluster generation for Cluster API platforms",
                "description": "OCP/Telco Definition of Done Epic Goal To add support for generating Cluster and Infrastructure Cluster resources on Cluster API based clusters Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPCLOUD-2889": {
                "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
                "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OCPCLOUD-2882": {
                "summary": "GCP - Add support to deploy Confidential VMs using AMD SEV-SNP",
                "description": "Epic Goal Add support to deploy Confidential VMs on GCP using AMD SEV-SNP technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using SEV-SNP technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "110",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2882, OCPCLOUD-2889: Provision AMD SEV-SNP and Intel TDX confidential instances",
                        "body": "As in upstream cluster-api-provider-gcp's: - - - Other relevant patches: - openshift/api patch: - Machine api operator patch:"
                    },
                    {
                        "id": "1326",
                        "type": "pullRequest",
                        "title": "OCPCLOUD-2882, OCPCLOUD-2889: support AMD SEV_SNP and TDX confidential computing machines on GCP",
                        "body": "Implements the same as in - - - openshift/api PR:"
                    }
                ]
            },
            "OCPCLOUD-2593": {
                "summary": "Rebasebot: lifecycle hooks",
                "description": "Epic Goal Rebasebot| needs to support customizable system for running repository specific tooling before/during/afeter rebase. This is primarily required for automatic rebases of CAPI provider repositories. Other uses for this feature are expected. Why is this important? Further automation of our rebase process will allow us to focus more on development instead of maintenance. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Application Platform Engineering": {
        "stories": {
            "OAPE-163": {
                "summary": "As a developer, I want to add e2e tests for MachineNamePrefix when the field is reset",
                "epic_key": "OAPE-142",
                "GITHUB": [
                    {
                        "id": "355",
                        "type": "pullRequest",
                        "title": "OAPE-163: Extend E2E tests for MachineNamePrefix when the field is reset",
                        "body": "Extend E2E tests for MachineNamePrefix when the field is reset"
                    }
                ]
            },
            "OAPE-148": {
                "summary": "As a developer, I want to promote MachineNamePrefix feature to default featureset",
                "description": "Promote the _CPMSMachineNamePrefix_ featuregate to default featureset and ensure that the gate is enabled by default.",
                "epic_key": "OAPE-142",
                "GITHUB": [
                    {
                        "id": "2254",
                        "type": "pullRequest",
                        "title": "OAPE-148: Promote CPMSMachineNamePrefix feature gate to default feature set",
                        "body": "Promote `CPMSMachineNamePrefix` feature gate to default feature set"
                    }
                ]
            },
            "OAPE-147": {
                "summary": "As a developer, I want to add e2e tests for OnDelete update strategy",
                "description": "Add E2E tests (both pre-submit and periodic) for MachineNamePrefix with OnDelete update strategy",
                "epic_key": "OAPE-142",
                "GITHUB": [
                    {
                        "id": "353",
                        "type": "pullRequest",
                        "title": "OAPE-147: Add E2E tests for MachineNamePrefix with OnDelete update strategy",
                        "body": "This PR adds E2E tests (both pre-submit and periodic) for `MachineNamePrefix` feature with `OnDelete` update strategy. The existing tests work only for `RollingUpdate` strategy. This PR extends the tests added through: - - Part of :"
                    }
                ]
            },
            "OAPE-126": {
                "summary": "Include CPMSMachineNamePrefix feature-gate name in e2e tests",
                "description": "Include `CPMSMachineNamePrefix` feature gate name in e2e tests. This is required for sippy tool to filter the e2e tests specific to this featuregate. xRef:",
                "epic_key": "OAPE-142",
                "GITHUB": [
                    {
                        "id": "351",
                        "type": "pullRequest",
                        "title": "OAPE-126: Include CPMSMachineNamePrefix feature-gate name in e2e tests",
                        "body": "Include `CPMSMachineNamePrefix` feature gate name in e2e tests. This is required for sippy tool to filter the e2e tests specific to this featuregate. xRef:"
                    }
                ]
            },
            "OAPE-96": {
                "summary": "As a developer, I want to bump o/api into o/kubernetes repo to update API godoc",
                "description": "As a developer, I want to bump o/api into o/kubernetes repo to update API godoc added in OAPE-94",
                "epic_key": "OAPE-26",
                "GITHUB": [
                    {
                        "id": "2217",
                        "type": "pullRequest",
                        "title": "OAPE-96: UPSTREAM: drop: bump openshift/api@107848b719c5",
                        "body": "!-- Thanks for sending a pull request! Here are some tips for you: 1. If this is your first time, please read our contributor guidelines: and developer guide 2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here: 3. Ensure you have added or ran the appropriate tests for your PR: 4. If you want faster PR reviews, read how: 5. If the PR is unfinished, see how to mark it: -- !-- Add one of the following kinds: /kind bug /kind cleanup /kind documentation /kind feature Optionally add one or more of the following kinds if applicable: /kind api-change /kind deprecation /kind failing-test /kind flake /kind regression -- What this PR does / why we need it: Bump `openshift/api` to vendor Which issue(s) this PR fixes: !-- Automatically closes linked issue when PR is merged. Usage: `Fixes issue number`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_ -- Fixes"
                    }
                ]
            },
            "OAPE-94": {
                "summary": "As a developer, I want to update the API godoc to document that manual intervention is required for using externalCertificate field",
                "description": "Update API godoc to document that manual intervention is required for using {{{}.spec.tls.externalCertificate{}}}. Something simple like: \"The Router service account needs to be granted with read-only access to this secret, please refer to openshift docs for additional details.\"",
                "epic_key": "OAPE-26",
                "GITHUB": [
                    {
                        "id": "2159",
                        "type": "pullRequest",
                        "title": "OAPE-94: Update API doc for route externalCertificate",
                        "body": "Update API godoc to document that manual intervention is required for using `.spec.tls.externalCertificate`."
                    }
                ]
            },
            "OAPE-92": {
                "summary": "As a developer, I want to promote the feature to default featureset",
                "description": "Promote the _RouteExternalCertificate_ featuregate to default featureset and ensure that the gate is enabled by default.",
                "epic_key": "OAPE-26",
                "GITHUB": [
                    {
                        "id": "2268",
                        "type": "pullRequest",
                        "title": "OAPE-92: Promote RouteExternalCertificate feature gate to default feature set",
                        "body": "Promote `RouteExternalCertificate` feature gate to default feature set for both SelfManaged and HyperShift."
                    }
                ]
            },
            "OAPE-91": {
                "summary": "As a developer, I want to add E2E tests for the entire feature",
                "description": "Add E2E tests for the entire feature in _openshift/origin_ to ensure feature stability.",
                "epic_key": "OAPE-26",
                "GITHUB": [
                    {
                        "id": "29651",
                        "type": "pullRequest",
                        "title": "OAPE-91: Route ExternalCertificate: Skip e2e jobs for bare metal platform",
                        "body": "We are seeing failures in `metal` platforms, so we are temporarily skipping the tests in metal. - E2E PR:"
                    },
                    {
                        "id": "29499",
                        "type": "pullRequest",
                        "title": "OAPE-91: Add E2E tests for Route ExternalCertificate feature",
                        "body": "Implements end-to-end tests for routes configured with ExternalCertificate feature. These tests cover the following scenarios: - Validation and pre-requisite of the feature - Router serviceaccount should have permission to read the secret. - The route and the referenced secret must exist in the same namespace. - The secret should be of type `kubernetes.io/tls`. - Route with `Passthrough` termination is not supported. - Both external certificates and inline certificates are not allowed. - With a valid setup the router should support external certificate. - Multiple routes can refer a common secret. - Secret delete, re-create and update scenarios. - With RBAC permissions validation. - Route update scenarios (with validation checks) - To use a new external certificate. - To use the same external certificate. - To remove and again re-add the same external certificate. Part of:"
                    }
                ]
            },
            "OAPE-78": {
                "summary": "As a developer, I want to add techpreview periodic jobs for all cloud providers to signal feature stabilization",
                "description": "To be able to gather test data for this feature, we will need to introduce tech preview periodics, so we need to duplicate each of and add the techpreview configuration. It's configured as an env var, so copy each job, add the env var, and change the name to include -techpreview as a suffic env: FEATURE_SET: TechPreviewNoUpgrade",
                "epic_key": "OAPE-16",
                "GITHUB": [
                    {
                        "id": "338",
                        "type": "pullRequest",
                        "title": "OAPE-78: Add e2e periodic job for machine name prefix",
                        "body": "- This is required to gather test data for feature. - Techpreview periodics job: , - Part of:"
                    }
                ]
            },
            "OAPE-19": {
                "summary": "As a developer, I want to support prefix name formats to control plane machines via CPMS-operator",
                "description": "Utilize the new field added in openshift/api and add the implementation so that custom name (prefix) formats can be assigned to Control Plane Machines via CPMS. All the changes should be behind the feature gate. This prefix will supersede the current usage of the control plane label and role combination we use today The names must still continue to be suffixed with charsidx as this is important to the operation of CPMS Add unit tests and E2E tests.",
                "epic_key": "OAPE-16",
                "GITHUB": [
                    {
                        "id": "332",
                        "type": "pullRequest",
                        "title": "OAPE-19: Support customized control plane machine names with prefix",
                        "body": "This PR introduces the ability to customize the naming format of Control Plane Machines. This is achieved through a new `machineNamePrefix` field in the ControlPlaneMachineSet spec, gated behind the `CPMSMachineNamePrefix` feature gate. When feature gate is enabled and `machineNamePrefix` field is specified, it allows users to specify a prefix that will be used in `getMachineName`. The resulting machine name will consist of the provided prefix, followed by a randomly generated string of 5 characters and the machine index. Otherwise, it will fall back to the default naming convention. - Implementes : - API PR: - Part of: OAPE-19("
                    }
                ]
            },
            "OAPE-18": {
                "summary": "As a developer, I want to vendor openshift/api changes into cpms-operator",
                "description": "Bump openshift/api to vendor machineNamePrefix field and CPMSMachineNamePrefix feature-gate into cpms-operator",
                "epic_key": "OAPE-16",
                "GITHUB": [
                    {
                        "id": "333",
                        "type": "pullRequest",
                        "title": "OAPE-18: Bump openshift/api to vendor machineNamePrefix field",
                        "body": "Bump `openshift/api` to vendor `machineNamePrefix` field and `CPMSMachineNamePrefix` feature-gate - Part of : OAPE-18( - Implementes : - API PR:"
                    }
                ]
            }
        },
        "epics": {
            "OAPE-142": {
                "summary": "GA Ability to assign custom name formats to Control Plane Machines via CPMS",
                "description": "OCP/Telco Definition of Done Epic Goal Placeholder to track GA activities for OAPE-16 feature. Why is this important? ... Scenarios ... Acceptance Criteria Moving the feature to default feature set. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "OAPE-26": {
                "summary": "GA Support router to load secrets",
                "description": "Placeholder to track GA work for CFE-811"
            },
            "OAPE-16": {
                "summary": "TP Ability to assign custom name formats to Control Plane Machines via CPMS",
                "description": "Epic Goal Provide a new field to the CPMS that allows to define a Machine name prefix This prefix will supersede the current usage of the control plane label and role combination we use today The names must still continue to be suffixed with chars-idx as this is important to the operation of CPMS Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. DEV - Downstream code and tests merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "Network Observability": {
        "stories": {
            "NETOBSERV-2023": {
                "summary": "Implement a quickstart for netobserv operator",
                "description": "OCP Console provides quickstarts as CRD to add items under the help menu Those quickstarts can be linked to the getting started card on the dashboard overview page The quickstart should contain - the steps to install and configure netobserv - an overview of the resources usage and the capabilities",
                "epic_key": "NETOBSERV-1940",
                "GITHUB": [
                    {
                        "id": "955",
                        "type": "pullRequest",
                        "title": "NETOBSERV-2023: Implement a quickstart for netobserv operator",
                        "body": "Initial quickstart for Network Observability Operator"
                    }
                ]
            },
            "NETOBSERV-2029": {
                "summary": "Implement a CLI Download in OCP Core",
                "description": "Similarly to the quickstart, the console also allow to create a CR to add a CLI download section: We should add by default in OCP a CR about netobserv CLI",
                "epic_key": "NETOBSERV-1940",
                "GITHUB": [
                    {
                        "id": "958",
                        "type": "pullRequest",
                        "title": "NETOBSERV-2029: add netobserv ConsoleCLIDownload",
                        "body": "Add netobserv CLI download link"
                    }
                ]
            }
        },
        "epics": {
            "NETOBSERV-1940": {
                "summary": "Console plugin configuration and status panel",
                "description": "Epic Goal Create a new view in netobserv console plugin to configure FlowCollector and get some status information. This must address some of the limitations related to the generic OLM install form, which lacks of flexibility in organizing the UI elements to offer a good UX. This may also include a resource footprint calculator that would help users taking more informed decisions before installing the FlowCollector (nice to have - might be a follow-up and/or a separate tool) Finally, once the FlowCollector is installed, some basic status information will be provided such as: Current FC status (components readiness etc.) FC Status warnings (same warnings as in the validation webhook) Extra warnings / recommendations, such as recommending to install Kafka depending on the number of nodes LokiStack status / readiness when relevant A FlowMetrics section with list of installed metrics + estimated cardinality + actual cardinality Links to netobserv dashboards Why is this important? Make it easier, less intimidating, to configure: despite installation being well documented, having a good UX for guiding should make users happier. Current OLM form is too limited. Also, all the options offered are often seen as intimidating. Remove uncertainty before installation: many users are afraid of the potential cost in resource footprint and want to anticipate it. A calculator will help reduce/remove the uncertainty. Increase visibility of issues: some issues might remain unnoticed, such as configuration warnings or high metrics cardinality. We want to make them more visible."
            }
        }
    },
    "Network Edge": {
        "stories": {
            "NE-2017": {
                "summary": "Enable GatewayAPIController feature gate in Default feature set",
                "description": "The goal of this story is to enable the GatewayAPIController feature gate| in the Default feature set. Currently, this feature gate is enabled only in the DevPreview and TechPreview feature sets. This should be the final implementation story in the epic, ensuring that all required implementation tasks are completed beforehand. Additionally, before enabling GatewayAPIController in the Default feature set, we must have at least five origin tests in place, as required (see for implementation of the tests).",
                "epic_key": "NE-1942",
                "GITHUB": [
                    {
                        "id": "2284",
                        "type": "pullRequest",
                        "title": "NE-2017: Enable GatewayAPIController featuregate in Default featureset",
                        "body": "Promote the `GatewayAPIController` feature gate to GA by enabling it in the default feature set. This featuregate governs the OpenShift Gateway API controller implementation, which is currently based on Istio."
                    }
                ]
            },
            "NE-2008": {
                "summary": "Add GRPCRoute tests to compliance testing",
                "epic_key": "NE-1117",
                "GITHUB": [
                    {
                        "id": "1208",
                        "type": "pullRequest",
                        "title": "NE-2008: Add GRPC conformance tests",
                        "body": "Add GRPCRoute to the SupportedFeatures list. Increase max time to consistency to 180s for these tests. Increase total run time of test to 20m max. Co-authored-by: Hongan Li lihongan"
                    }
                ]
            },
            "NE-1994": {
                "summary": "Verify that Istio's manual deployment feature is not enabled",
                "description": "Add a test to cluster-ingress-operator's E2E tests to verify that Istio is configured not to allow manual deployment.",
                "epic_key": "NE-1933",
                "GITHUB": [
                    {
                        "id": "1204",
                        "type": "pullRequest",
                        "title": "NE-1994: Add E2E test for Istio manual deployment",
                        "body": "Add a new test to verify that Istio is configured not to allow manual deployment."
                    }
                ]
            },
            "NE-1970": {
                "summary": "Defining FeatureGate e2e tests: GatewayAPIController featuregate",
                "description": "After reviewing the existing GWAPI e2e tests| for GatewayAPIController featuregate and QE new designed cases in Polarion, we will cover below tests in Origin e2e: 1 OSSM installation succeed (check related resources) and default gatewayclass is accepted. 2 can create and delete custom gatewayclass (delete custom should not affect default and istiod-openshift-gateway) 3 Gateway object could be created (check LoadBalancer service, DNSRecords) 4 HTTPRoute object could be created (using separated gateway) 5 (negative) The OSSM related resource gets recreated after delete it (Subscription, Istio) 6 (negative) The Gateway related resource gets recreated after delete it (LoadBalancer service, DNSRecords...)",
                "epic_key": "NE-1942",
                "GITHUB": [
                    {
                        "id": "29670",
                        "type": "pullRequest",
                        "title": "NE-1970: 5 e2e origin tests for gatewayApiController featureGate",
                        "body": "PR for Network Ingress and DNS team for the 5 e2e tests to graduate featureGate to GA JIRA: Test Scenarios: Confirm that OLM and OSSM resources are created Confirm that default gatewayClass is accepted Confirm that a custom-gatewayclass can be created, deleted and not affect the state of the OSSM resources A Custom gateway Object can be created A HttpRoute can be created with no errors using a custom-gateway"
                    }
                ]
            },
            "NE-1969": {
                "summary": "Updates to CIO logic for Gateway API CRD Management",
                "description": "What? On OCP 4.19 onward we will ensure the Gateway API CRDs are present a specific version with its own feature gate which will default to true. If we can not ensure the CRDs are present at the expected version we will mark the cluster degraded. Why? See the description of NE-1898. How? The Cluster Ingress Operator (CIO)",
                "epic_key": "NE-1898",
                "GITHUB": [
                    {
                        "id": "1205",
                        "type": "pullRequest",
                        "title": "NE-1969: Set Degraded=True if unmanaged Gateway API CRDs exist",
                        "body": "Set the ingress cluster operator\u2019s `Degraded` status to `true` if unmanaged Gateway API CRDs exist on the cluster. An unmanaged Gateway API CRD is one with \"gateway.networking.k8s.io\" or \"gateway.networking.x-k8s.io\" in its `spec.group` field, and not managed by the gatewayapi controller. This PR uses the cluster operator\u2019s `status.extension` field to store the names of unmanaged CRDs. The gatewayapi controller writes to this field, while the status controller reads it and updates the ingress cluster operator\u2019s `Degraded` status accordingly."
                    },
                    {
                        "id": "1202",
                        "type": "pullRequest",
                        "title": "NE-1969: Add \"v1\" version to OpenShift GatewayClass controller name",
                        "body": "Add the \"v1\" suffix to the GatewayClass controllerName to allow for alternative OpenShift Gateway API implementations in the future."
                    }
                ]
            },
            "NE-1953": {
                "summary": "Validating Admission Policy for Gateway API CRD Management",
                "description": "What? The purpose of this task is to provide API validation on OCP that blocks upgrades to Gateway API CRDs from all entities except the platform itself. Why? See the description of NE-1898. How? We will use a Validating Admission Policy (VAP) Blocking in the VAP should occur at the group level, meaning only the CIO is capable of creating or changing any CRDs across the entire group at any version. As such this VAP will block access to ALL Gateway API CRDs, not just the ones we use (GatewayClass, Gateway, HTTPRoute, GRPCRoute, ReferenceGrant). Note that this means experimental APIs (e.g. TCPRoute, UDPRoute, TLSRoute) and older versions of APIs (e.g. v1beta1.HTTPRoute) are restricted as well from creation/modification. The effect should be that only the standard versions of GatewayClass, Gateway, HTTPRoute, GRPCRoute and ReferenceGrant (at the time of writing, these fully represent the standard APIs| are present and nobody can modify those, or deploy any others. This VAP should be deployed alongside the CIO manifests, such that it is deployed along with the CIO itself. Prior Art Example of a VAP restricting actions to a single entity: Helpful Links Here's where the current operator manifests can be found:",
                "epic_key": "NE-1898",
                "GITHUB": [
                    {
                        "id": "1200",
                        "type": "pullRequest",
                        "title": "NE-1953: Add experimental Gateway API group to Validating Admission Policy",
                        "body": "This PR adds `gateway.networking.x-k8s.io` to the match condition of the operator's Validating Admission Policy. This change ensures that modifications to experimental Gateway API CRDs can be made only by the ingress operator."
                    },
                    {
                        "id": "1192",
                        "type": "pullRequest",
                        "title": "NE-1953: Add Validating Admission Policy for Gateway API CRDs",
                        "body": "This PR introduces a validating admission policy that restricts modifications to CRDs from the \"gateway.networking.k8s.io\" group, allowing only the cluster ingress operator's service account to make changes. This ensures that the core OpenShift retains exclusive ownership of the Gateway API implementation, preventing modifications from any add-ons (whether third-party or Red Hat-owned). The policy and its corresponding binding will be managed by CVO."
                    }
                ]
            },
            "NE-1934": {
                "summary": "Bump to OSSM 3.0",
                "description": "Update cluster-ingress-operator to install OSSM 3.0.",
                "epic_key": "NE-1933",
                "GITHUB": [
                    {
                        "id": "1152",
                        "type": "pullRequest",
                        "title": "NE-1934: Bump to OSSM 3.0 for Gateway API support",
                        "body": "With OSSM 3, the Maistra Istio Operator is replaced with new operator based on the upstream Sail Operator( and the ServiceMeshControlPlane CRD is replaced by the Istio CRD. Vendor the sail-operator API: go mod edit -replace github.com/imdario/mergo=github.com/imdario/mergo@v0.3.5 go get github.com/istio-ecosystem/sail-operator/api/v1 go mod tidy go mod vendor Note that vendoring sail-operator requires the mergo override. OSSM 3.0 is based on Istio 1.24, which supports Gateway API v1.2.1. Copy in the updated Gateway API CRDs: curl --silent --output-dir pkg/manifests/assets/gateway-api/ --remote-name ' Update the conformance tests to reflect the features that Istio 1.24 supports. Update the gatewayclass controller to create a subscription for the OSSM 3 Operator (which is in a separate channel from the OSSM 2 operator) and to create an Istio CR instead of a ServiceMeshControlPlane CR. Currently, OSSM 3.0 is Tech Preview( so the subscription is actually for the latest Sail Operator nightly build. Update client initialization, RBAC, and tests that used the Maistra APIs from OSSM 2 to use the sail-operator API for OSSM 3."
                    }
                ]
            },
            "NE-1907": {
                "summary": "GWAPI Implement OSSM Subscription Pinning",
                "epic_key": "NE-1817",
                "GITHUB": [
                    {
                        "id": "1112",
                        "type": "pullRequest",
                        "title": "NE-1907: Manage OSSM operator subscription manually to ensure a compatible version is installed",
                        "body": "Istio and the Gateway API CRDs need to be in sync to work. The CRDs are baked into a particular openshift release, so this change updates the ingress operator to install a compatible version with the CRDs it already installs."
                    }
                ]
            },
            "NE-1870": {
                "summary": "Submit PR to fix OWNERS files in openshift/origin",
                "epic_key": "NE-1865",
                "GITHUB": [
                    {
                        "id": "29247",
                        "type": "pullRequest",
                        "title": "NE-1870: Fix the Network Ingress & DNS OWNERS files and add aliases to OWNERS_ALIASES",
                        "body": "Fix the Network Ingress & DNS OWNERS files and add aliases to OWNERS_ALIASES. modified: OWNERS_ALIASES new file: test/extended/dns/OWNERS modified: test/extended/router/OWNERS"
                    }
                ]
            },
            "NE-1790": {
                "summary": "Enable Dynamic Configuration Manager",
                "description": "The goal of this user story is to combine the code from the smoke test user story into an implementation PR. Since multiple gaps were discovered a feature gate will be needed to ensure stability of OCP before the feature can be enabled by default.",
                "epic_key": "NE-879",
                "GITHUB": [
                    {
                        "id": "1174",
                        "type": "pullRequest",
                        "title": "NE-1790: Follow up to enable Dynamic Configuration Manager feature gate",
                        "body": "- Ingress controller: passed reconciler config to `desiredRouterDeployment`. - TestDesiredRouterDeploymentDynamicConfigManager: added test case for invalid max servers. Follow-up to - - - - -"
                    }
                ]
            },
            "NE-2009": {
                "summary": "Enable GatewayAPI feature gate in Default feature set",
                "description": "The goal of this story is to enable the GatewayAPI feature gate This should be the final implementation story in the epic, ensuring that all required implementation tasks are completed beforehand. Additionally, before enabling GatewayAPI in the Default feature set, we must have at least five origin tests in place, as required.",
                "epic_key": "NE-1898",
                "GITHUB": [
                    {
                        "id": "2281",
                        "type": "pullRequest",
                        "title": "NE-2009: Re-enable GatewayAPI featuregate in Default featureset",
                        "body": "This PR re-applies which was reverted PR which fixed the missing `GatewayAPI` featuregate in `Default` on Hypershift: PR which added HyperShift conformance tests to the api presubmites:"
                    },
                    {
                        "id": "2261",
                        "type": "pullRequest",
                        "title": "NE-2009: Enable GatewayAPI featuregate in Default featureset",
                        "body": "Promote `GatewayAPI` featuregate which covers the GatewayAPI CRD lifecycle management implemented in the cluster ingress operator."
                    },
                    {
                        "id": "1221",
                        "type": "pullRequest",
                        "title": "NE-2009: Relax pod bound validating admission rule for HyperShift",
                        "body": "In HyperShift, the ingress operator runs on the management cluster and communicates with the hosted cluster via a generated Kubeconfig. This setup breaks the validating admission rule that enforces requests to originate from a pod within `openshift-ingress-operator-gatewayapi-crd-admission` VAP. This commit adds a dedicated VAP manifest for HyperShift clusters, which relaxes the rule mentioned above. Slack discussion: link( Failed HyperShift conformance test: link( Ingress operator logs: logs( Error from the logs: ``` ValidatingAdmissionPolicy 'openshift-ingress-operator-gatewayapi-crd-admission' with binding 'openshift-ingress-operator-gatewayapi-crd-admission' denied request: this user must have both \\\"authentication.kubernetes.io/node-name\\\" and \\\"authentication.kubernetes.io/pod-name\\\" claims, failed to create CRD httproutes.gateway.networking.k8s.io: customresourcedefinitions.apiextensions.k8s.io \\\"httproutes.gateway.networking.k8s.io\\\" is forbidden: ValidatingAdmissionPolicy 'openshift-ingress-operator-gatewayapi-crd-admission' with binding 'openshift-ingress-operator-gatewayapi-crd-admission' denied request: this user must have both \\\"authentication.kubernetes.io/node-name\\\" and \\\"authentication.kubernetes.io/pod-name\\\" claims, ```"
                    },
                    {
                        "id": "1216",
                        "type": "pullRequest",
                        "title": "NE-2009: Move VAP to Default featureset",
                        "body": "The `openshift-ingress-operator-gatewayapi-crd-admission` Validating Admission Policy is bound to `GatewayAPI` featuregate, which has been promoted to GA ( This PR removes the `feature-set` annotation from manifests related to VAP, this effectively makes them part of the `Default` featureset."
                    }
                ]
            },
            "NE-1968": {
                "summary": "Defining FeatureGate e2e tests: GatewayAPI featuregate",
                "description": "see thread: and the tests would be covered in Origin are: Verify Gateway API CRDs and esnure required CRDs should already be installed Verify Gateway API CRDs and ensure existing CRDs can not be deleted Verify Gateway API CRDs and ensure existing CRDs can not be updated Verify Gateway API CRDs and ensure CRD of standard group can not be created Verify Gateway API CRDs and ensure CRD of experimental group is not installed Verify Gateway API CRDs and ensure CRD of experimental group can not be created",
                "epic_key": "NE-1942",
                "GITHUB": [
                    {
                        "id": "29597",
                        "type": "pullRequest",
                        "title": "NE-1968: add e2e tests for FeatureGate GatewayAPI",
                        "body": "Implements e2e tests for GatewayAPI featuregate. These tests cover the following scenarios: - Verify Gateway API CRDs and esnure required CRDs should already be installed - Verfiy Gateway API CRDs and ensure existing CRDs can not be deleted - Verify Gateway API CRDs and ensure existing CRDs can not be updated - Verify Gateway API CRDs and ensure CRD of standard group can not be created - Verify Gateway API CRDs and ensure CRD of experimental group is not installed - Verify Gateway API CRDs and ensure CRD of experimental group can not be created requires"
                    }
                ]
            },
            "NE-1957": {
                "summary": "Test DNS Records creation for Gateways with unique and overlapping hostnames",
                "description": "Test how cluster ingress operator handles DNS record creation when multiple Gateways are created with unique and overlapping hostnames, to ensure the correct DNS resolution. Test cases: - Create multiple Gateways that have listeners with the same hostname, as well as with differing hostnames and test DNS flow. Expectation: DNS should create separate DNSRecords for each unique hostname. - Create a Gateway with Listeners that don't have a hostname: Expectation: No DNSRecord should be created. Requests with empty hostname value are processed as a fallback, after the least specific matches are exhausted. - Gateway listener trying to claim a fully qualified hostname (abc.apps.DNS config base domain) that would match a .apps.DNS config base domain ingress controller wildcard. Which endpoint tries to serve the request to abc.apps.DNS config base domain? Expectation: DNSRecord for abc.apps.DNS config base domain gets created. Traffic for the most specific match should be resolved to the gateway. - DNSRecords handling when the associated Gateway is deleted or hostname was deleted or modified on the Gateway. Expectation: DNSRecords associated with the removed Gateway also get deleted.",
                "epic_key": "NE-1769",
                "GITHUB": [
                    {
                        "id": "1213",
                        "type": "pullRequest",
                        "title": "NE-1957: Add Gateway API DNS Feature e2e tests",
                        "body": "This PR adds basic e2e tests for the Gateway API DNS feature. The tests create gateways in different configurations and check for expected DNSRecords to be present or not existing, depending on the test case. The test cases covered are: - Multiple gateways with listeners of the same hostname. - Multiple gateways with listeners with overlapping hostnames. - Creating a gateway with multiple listeners and then updating and deleting the listeners to ensure the DNSRecords are reconciled accordingly. - Gateway with a listener with no hostname."
                    }
                ]
            },
            "NE-1954": {
                "summary": "Implement GatewayAPI Controller featuregate",
                "description": "What? Add a new featuregate for OSSM installation, and move OSSM installation from the existing GatewayAPI feature gate to the new separate featuregate, so we have one featuregate only for CRDs and one featuregate only for installing OSSM. This will help us with staging component releases.",
                "epic_key": "NE-1898",
                "GITHUB": [
                    {
                        "id": "2219",
                        "type": "pullRequest",
                        "title": "NE-1954: Introduce GatewayAPIController feature gate",
                        "body": "Previously, the \"GatewayAPI\" feature gate managed both the GatewayAPI CRDs and the Gateway Controller. However, with the introduction of Gateway CRD lifecycle management ( these responsibilities were separated. A dedicated feature gate now controls the Gateway Controller to distinguish its production readiness from that of the CRDs."
                    },
                    {
                        "id": "1198",
                        "type": "pullRequest",
                        "title": "NE-1954: Implement GatewayAPIController feature gate",
                        "body": "This PR bumps `openshift/api` to get GatewayAPIController feature gate( and implements it by skipping the start of the gatewayclass and service-dns controllers when it's disabled."
                    }
                ]
            },
            "NE-1936": {
                "summary": "Bump cluster-ingress-operator to Kubernetes 1.32 for 4.19",
                "description": "Description of problem The openshift/cluster-ingress-operator repository| vendors k8s.io/ v0.31.1. OpenShift 4.19 is based on Kubernetes 1.32. Version-Release number of selected component (if applicable) 4.19. How reproducible Always. Steps to Reproduce Check Actual results The k8s.io/ packages are at v0.31.1. Expected results The k8s.io/ packages are at v0.32.0 or newer.",
                "epic_key": "NE-1935",
                "GITHUB": [
                    {
                        "id": "1184",
                        "type": "pullRequest",
                        "title": "NE-1936: Bump k8s.io dependencies to v0.32.1",
                        "body": "- Upgraded k8s.io/api, k8s.io/apiextensions-apiserver, and k8s.io/client-go to v0.32.1. - Updated sigs.k8s.io/controller-runtime to v0.20.1 (compatible with k8s v0.32). - Go version was implicitly bumped to 1.23.0 as part of the dependency updates. Commands used for the upgrade: ``` go get k8s.io/api@v0.32.1 go get k8s.io/apiextensions-apiserver@v0.32.1 go mod edit -replace k8s.io/client-go=k8s.io/client-go@v0.32.1 go get sigs.k8s.io/controller-runtime@v0.20.1 go mod tidy go mod vendor ```"
                    }
                ]
            },
            "NE-1908": {
                "summary": "Add instructions for keepalived-ipfailover image testing",
                "GITHUB": [
                    {
                        "id": "198",
                        "type": "pullRequest",
                        "title": "NE-1908: Add instructions for keepalived-ipfailover image testing",
                        "body": "Removed old README file and added new README file for keepalived-ipfailover."
                    }
                ]
            },
            "NE-1871": {
                "summary": "Update Gateway API Feature Gate to include Tech Preview",
                "description": "Update the existing feature gate to enable Gateway API in clusters with either the DevPreviewNoUpgrade or TechPreviewNoUpgrade feature set.",
                "epic_key": "NE-1747",
                "GITHUB": [
                    {
                        "id": "2081",
                        "type": "pullRequest",
                        "title": "NE-1871: Promote GatewayAPI to Tech Preview",
                        "body": "Enhancement: Gateway API with Cluster Ingress Operator("
                    }
                ]
            },
            "NE-1277": {
                "summary": "Add Gateway API to must-gather results",
                "description": "GWAPI and istio logs are not in the must-gather reports. Add Gateway API resources and possibly OSSM resources to the operator's relatedObjects field.",
                "epic_key": "NE-1117",
                "GITHUB": [
                    {
                        "id": "933",
                        "type": "pullRequest",
                        "title": "NE-1277: status: Add Gateway API objects to relatedObjects",
                        "body": "`pkg/operator/controller/names.go` (`GlobalOperatorsNamespace`): New const for the \"openshift-operators\" namespace. (`ServiceMeshSubscriptionName`): Use `GlobalOperatorsNamespace`. `pkg/operator/controller/status/controller.go` (`New`): Add a watch for subscriptions so that the status controller updates `relatedObjects` when the OSSM subscription is added or removed. (`Config`): Add `GatewayAPIEnabled` field. (`Reconcile`): If `GatewayAPIEnabled` is true, add resources related to Gateway API to `relatedObjects`. If `haveOSSMSubscription` is true, add resources that require the subscription. (`operatorState`): Add `haveOSSMSubscription` field. (`getOperatorState`): Set `haveOSSMSubscription`. `pkg/operator/operator.go` (`New`): Watch the \"openshift-operators\" namespace, and specify `GatewayAPIEnabled` in the status controller's config."
                    }
                ]
            }
        },
        "epics": {
            "NE-1942": {
                "summary": "Operator and Origin E2E tests for the gateway controller and CRD life-cycle management e2e testing automation",
                "description": "Use cases: As a developer I would like to test for unacceptable failures that exist in the Gateway API with Ingress product. This Epic is a place holder for stories regarding e2e and unit tests that are missing for old features and to determine whether OSSM 3.x TP2 bugs affect us before they are fixed in GA. There is already one epic for DNS and test cases should be added for any new features in the release. Write and run test cases that are currently missing."
            },
            "NE-1117": {
                "summary": "Gateway controller implementation",
                "description": "Epic Goal Add Gateway API via Istio Gateway implementation as GA in future release Problem: As an administrator, I would like to securely expose cluster resources to remote clients and services while providing a self-service experience to application developers. GA: A feature is implemented as GA so that developers can issue an update to the Tech Preview MVP and: can no longer change APIs without following a deprecating or backwards compatibility process. are required to fix bugs customers uncover must support upgrading the cluster and your component provide docs provide education to CEE about the feature must also follow Red Hat's support policy for GA Why is this important? Reduces the burden on Red Hat developers to maintain IngressController and Route custom resources Brings OpenShift ingress configuration more in line with standard Kubernetes APIs Demonstrates Red Hat\u2019s leadership in the Kubernetes community. Scenarios ... Acceptance Criteria Gateway API and Istio Gateway are in an acceptable standing for GA Istio Gateway installation without sidecars enabled Decision completed on whether a new operator is required, especially for upgrade and status reports Decision completed on whether Ingress-Gateway (or Route-Gateway) translation is needed Enhancement Proposals, Migration details, Tech Enablement, and other input for QA and Docs as needed API server integration, Installation, CI, E2E tests, Upgrade details, Telemetry as needed TBD Dependencies (internal and external) OSSM release schedule aligned with OpenShift's cadence, or workaround designed ...tbd Previous Work (Optional): Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "NE-1933": {
                "summary": "Bump to OSSM 3.0.0",
                "description": "Epic Goal The logic in cluster-ingress-operator that installs and configures OSSM 2.y should be updated to install and configure OSSM 3.y. Why is this important? The GA release of the OpenShift Gateway API feature will be based on OSSM 3.y. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (+) Priority is set by engineering. - (+) Epic must be Linked to a Parent Feature. - (-) Target version+ must be set. - (+) Assignee must be set. - (+) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown. - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) 1. OSSM 3.0 (currently in Tech Preview). Previous Work 1. NE-1105. Open questions 1. Will any further changes be required between OSSM 3.0 Tech Preview and OSSM 3.0 GA? Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "NE-1898": {
                "summary": "CRD Lifecycle Management for Gateway API",
                "description": "Overview Gateway API in upstream Kubernetes. OpenShift Service Mesh (OSSM) Microshift and OpenShift AI OCP will be fully in charge of managing the life-cycle of the Gateway API enacts a process called \"CRD Management Succession\" to ensure the transfer of control occurs safely, which includes multiple pre-upgrade checks and CIO startup checks. Acceptance Criteria If not present the Gateway API CRDs should be deployed at the install-time of a cluster, and management thereafter handled by the platform Any existing CRDs not managed by the platform should be removed, or management and control transferred to the platform Only the platform can manage or make any changes to the Gateway API CRDs, others will be blocked Documentation about these APIs, and the process to upgrade to a version where they are being managed needs to be provided Cross-Team Coordination The organization as a whole needs to be made aware of this as new projects will continue to pop up with Gateway API support over the years. This includes (but is not limited to) OSSM Team (Istio) Connectivity Link Team (Kuadrant) MicroShift Team OpenShift AI Team (KServe) Importantly our cluster infrastructure work with Cluster API (CAPI) OCPCLOUD-2114 Investigate lifecycle of Cluster API APIs within OpenShift"
            },
            "NE-1817": {
                "summary": "Pin OSSM Subscription to a Compatible Version for Gateway API Support",
                "description": "Template: Networking Definition of Planned Epic Goal: Guarantee a compatible OSSM version is installed for Gateway API Why is this important? The ingress operator manages OSSM operator and Istio configuration for Gateway API support, but these resources can change in future releases in potentially incompatible ways. In order to guarantee that the OSSM and Istio versions are compatible with the ingress operator in a given OpenShift release, the ingress operator should install a known good version, rather than the latest. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Needs to be OSSM 3.x.x+ so that we're more up to date with upstream Istio ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. NE-1326: Investigate OSSM subscription so we can minimize surprise interoperability issues Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "NE-1865": {
                "summary": "Tech Debt Fix OWNERS files in openshift/origin",
                "description": "Add a NID alias to OWNERS_ALIASES and update the OWNERS file in test/extended/router and add OWNERS file to test/extended/dns"
            },
            "NE-879": {
                "summary": "Enable the dynamic config manager",
                "description": "Goal To make the current implementation of the HAProxy config manager should not be used to reduce the impact of the feature. - Limit dynamic server allocation -- Set the maximum number of dynamic servers| to a minimal value to prevent high resource consumption. - Provide customer opt-out -- Offer customers a handler to opt out of the default config manager implementation."
            },
            "NE-1260": {
                "summary": "Operator CI job for the upstream Gateway API conformance test suite",
                "description": "The team agrees, we should be running the upstream GWAPI conformance tests, as they are readily available and we are an integration product with GWAPI. We need to answer these questions asked at the March 23, 2023 GWAPI team meeting: Would it make sense to do it as an optional job in the cluster-ingress-operator? Is OSSM running the Gateway Conformance test in their CI? Review what other implementers do with conformance tests to understand what we should do (Do we fork the repo? Clone it? Make a new repo?)",
                "GITHUB": [
                    {
                        "id": "1176",
                        "type": "pullRequest",
                        "title": "NE-1260: Add Makefile target to run Gateway API conformance tests",
                        "body": "This PR adds new Makefile target and enables the upstream gateway-api conformance test. Once this get merged, the follow up will add optional pre-submits job to run conformance test."
                    }
                ]
            },
            "NE-1769": {
                "summary": "Operator E2E tests for gateway DNS management",
                "description": "Use cases: As a customer I would like to understand how and when DNS records are created for my Gateway API resources. As a developer I would like to fix any issues that are not acceptable, or document those that cannot be resolved. See Enhancement Proposal - new controller to manage dns records for gateway listeners and Write and run unit test cases to find answers to the following about the current Gateway API DNS reconciliation: Does it work with multiple Gateways? Create multiple Gateways that have listeners with same hostname, as well as with differing hostnames and test DNS flow. What happens for a Gateway with Listeners that don't have a hostname? -Does it ignore Services that are not associated with the Gateway controller?- What happens if a Gateway listener tries to claim a name (abc.apps.example.com) that would match a .apps.example.com ingress controller wildcard. Which endpoint tries to serves the request to abc.apps.example.com? Other important factors TBD. Acceptance Criteria: new unit test cases, documentation, and update to enhancement document if needed."
            },
            "NE-1935": {
                "summary": "Update components to use Kubernetes 1.32 packages",
                "description": "Epic Goal Bump vendored Kubernetes packages (k8s.io/api, k8s.io/apimachinery, k8s.io/client-go, etc.) to v0.32.0 or newer version. Why is this important? Keep vendored packages up to date. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to ToDo status - (+) Priority+ is set by engineering - (x) Epic must be Linked to a Parent Feature - (-) Target version must be set - (+) Assignee must be set - (x) Enhancement Proposal is Implementable - (+) No outstanding questions about major work breakdown - (+) Are all Stakeholders known? Have they all been notified about this item? - (+) Does this epic affect SD? Have they been notified? (View plan definition for current suggested assignee) Acceptance Criteria CI - MUST be running successfully with tests automated -Release Technical Enablement - Provide necessary release enablement details and documents.- Dependencies (internal and external) 1. Other vendored dependencies (such as openshift/api and controller-runtime) may also need to be updated to Kubernetes 1.32. Previous Work (Optional) 1. NE-1875. Open questions None. Done Checklist CI - CI is running, tests are automated and merged. -Release Enablement link to Feature Enablement Presentation- DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue -DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue- DEV - Downstream build attached to advisory: link to errata -QE - Test plans in Polarion: link or reference to Polarion- -QE - Automated tests merged: link or reference to automated tests- -DOC - Downstream documentation merged: link to meaningful PR-"
            },
            "NE-1747": {
                "summary": "Graduate \"GatewayAPI\" featuregate to TechPreviewNoUpgrade",
                "description": "As a developer, I need a featuregate to develop behind so that the Gateway API work does not impact other development teams until tests pass. The featuregate is currently in the DevPreviewNoUpgrade featureset. We need to graduate it to the TechPreviewNoUpgrade featureset to give us more CI signal and testing. Ultimately the featuregate needs to graduate to GA (default on) once tests pass so that the feature can GA. See also: - - -"
            }
        }
    },
    "OpenShift Top Level Product Strategy": {
        "description": "Holds all top-level Market Problems, Features, and some Epics that were committed during release product planning.",
        "features": {
            "OCPPLAN-7878": {
                "summary": "NetEdge - Maintainability and Debugability & Tech Backlog",
                "description": "tldr: three basic claims, the rest is explanation and one example We cannot improve long term maintainability solely by fixing bugs. Teams should be asked to produce designs for improving maintainability/debugability. Specific maintenance items (or investigation of maintenance items), should be placed into planning as peer to PM requests and explicitly prioritized against them. While bugs are an important metric, fixing bugs is different than investing in maintainability and debugability. Investing in fixing bugs will help alleviate immediate problems, but doesn't improve the ability to address future problems. You (may) get a code base with fewer bugs, but when you add a new feature, it will still be hard to debug problems and interactions. This pushes a code base towards stagnation where it gets harder and harder to add features. One alternative is to ask teams to produce ideas for how they would improve future maintainability and debugability instead of focusing on immediate bugs. This would produce designs that make problem determination, bug resolution, and future feature additions faster over time. I have a concrete example of one such outcome of focusing on bugs vs quality. We have resolved many bugs about communication failures with ingress by finding problems with point-to-point network communication. We have fixed the individual bugs, but have not improved the code for future debugging. In so doing, we chase many hard to diagnose problem across the stack. The alternative is to create a point-to-point network connectivity capability. this would immediately improve bug resolution and stability (detection) for kuryr, ovs, legacy sdn, network-edge, kube-apiserver, openshift-apiserver, authentication, and console. Bug fixing does not produce the same impact. We need more investment in our future selves. Saying, \"teams should reserve this\" doesn't seem to be universally effective. Perhaps an approach that directly asks for designs and impacts and then follows up by placing the items directly in planning and prioritizing against PM feature requests would give teams the confidence to invest in these areas and give broad exposure to systemic problems. ---- Relevant links: Documentation: Edge Diagnostics Scratchpad the SDN team's diagnostic guide. Linux Performance OpenShift Router Reload Technical Overview on Access. How to collect worker metrics to troubleshoot CPU load, memory pressure and interrupt issues and networking on worker nodes in OCP 4 on Mojo, results from OpenShift scalability testing. Scalability and performance OCP 3.11 documentation about the manual performance configuration that was possible in OCP 3. Timing web requests with cURL and Chrome some useful tcpdump commands. OpenShift SDN - Networking design document for improved status condition reporting. Observability tips for HAProxy analysis using tshark. The PCP Book: A Complete Documentation of Performance Co-Pilot brief guide to using SystemTap on RHCOS. Troubleshooting throughput issues Red Hat Enterprise Linux Network Performance Tuning Guide (PDF) a diagnostic built into the kube-apiserver operator. Diagnostic tools: dropwatch to check NIC configuration. iovisor/bcc: BCC - Tools for BPF-based Linux IO analysis, networking, monitoring, and more to gather timing information about HTTP/HTTPS connections. route-monitor a programmable packet generator. OpenTracing node-problem-detector by Brendan Gregg. DTrace SystemTap cheatsheet (PDF) kubectl plugin for tcpdump & Wireshark. ironcladlou/ditm network diagnostic and visualization tool. ali a general stress-loading tool (CPU, filesystem, network, ...). mb is an example of diagnosing DNS latency/timeouts. BZ1829779 Investigation is an example of diagnosing misconfigured DNS for an external LB. Debugging network stalls on Kubernetes| from the GitHub Blog, about diagnosing Kubernetes performance issues related to ksoftirqd."
            }
        }
    },
    "OpenShift Monitoring": {
        "stories": {
            "MON-4207": {
                "summary": "Bump prometheus-operator to v0.81.0 downstream",
                "description": "Bump prometheus-operator to v0.81.0 downstream",
                "GITHUB": [
                    {
                        "id": "2593",
                        "type": "pullRequest",
                        "title": "MON-4207: Bump prometheus-operator to v0.81.0",
                        "body": "!-- Don't forget about CHANGELOG if this affects the end user! Changelog entry format: - PR-id(PR-URL) Monitoring Component ... PR-id Id of your pull request. PR-URL URL of your PR Component Component affected by your changes such as deps bump, alerts changes and any user facing changes. Example: - 741( Bump thanos components to v0.11.0 release -- I added CHANGELOG entry for this change. x No user facing changes, so no entry in CHANGELOG was needed."
                    },
                    {
                        "id": "329",
                        "type": "pullRequest",
                        "title": "MON-4207: Bump openshift/prometheus-operator to v0.81.0",
                        "body": "Executed manual steps as syncbot action failing due to merge conflict ``` git fetch --tags if ! git merge refs/tags/v0.81.0 --no-edit; then git checkout --theirs CHANGELOG.md Documentation VERSION bundle.yaml example go.mod go.sum scripts/go.mod scripts/go.sum pkg git checkout --ours git add CHANGELOG.md Documentation VERSION bundle.yaml example go.mod go.sum scripts/go.mod scripts/go.sum pkg git merge --continue fi go mod tidy go mod vendor if -f scripts/rh-manifest.sh ; then bash scripts/rh-manifest.sh git add rh-manifest.txt git diff --cached --exit-code update rh-manifest.txt\" fi ```"
                    }
                ]
            },
            "MON-4126": {
                "summary": "Add fallbackScrapeProtocol to ScrapeClass",
                "description": "Add fallbackScrapeProtocol to ScrapeClass Prometheus-Operator And have it set downstream. (no need to wait for a prom-operator release, we can cherry-pick it one merged, that would be considered an extra test for the change)",
                "epic_key": "MON-4103",
                "GITHUB": [
                    {
                        "id": "2590",
                        "type": "pullRequest",
                        "title": "MON-4126: set fallbackScrapeProtocol: 'PrometheusText1.0.0' as default for all UWM Prometheus targets for backward compatibility with Prometheus v2 until a better migration process is available",
                        "body": "!-- Don't forget about CHANGELOG if this affects the end user! Changelog entry format: - PR-id(PR-URL) Monitoring Component ... PR-id Id of your pull request. PR-URL URL of your PR Component Component affected by your changes such as deps bump, alerts changes and any user facing changes. Example: - 741( Bump thanos components to v0.11.0 release -- I added CHANGELOG entry for this change. No user facing changes, so no entry in CHANGELOG was needed."
                    },
                    {
                        "id": "324",
                        "type": "pullRequest",
                        "title": "MON-4126: bot Bump openshift/prometheus-operator to v0.80.0",
                        "body": "Description This is an automated version bump from CI. The logs for this run can be found in the syncbot repo actions( If you wish to perform this manually, execute the following commands from openshift/prometheus-operator repo: ``` git fetch --tags if ! git merge refs/tags/v0.80.0 --no-edit; then git checkout --theirs CHANGELOG.md Documentation VERSION bundle.yaml example go.mod go.sum scripts/go.mod scripts/go.sum pkg git checkout --ours git add CHANGELOG.md Documentation VERSION bundle.yaml example go.mod go.sum scripts/go.mod scripts/go.sum pkg git merge --continue fi go mod tidy go mod vendor if -f scripts/rh-manifest.sh ; then bash scripts/rh-manifest.sh git add rh-manifest.txt git diff --cached --exit-code update rh-manifest.txt\" fi ```"
                    }
                ]
            },
            "MON-3866": {
                "summary": "Create separate metrics client cert for metrics server",
                "description": "For the issue we had identified that we need to have separate metrics client cert for metrics server but for that we need to add approver for metrics-server |",
                "epic_key": "MON-4098",
                "GITHUB": [
                    {
                        "id": "2536",
                        "type": "pullRequest",
                        "title": "MON-3866: create separate metrics client cert for metrics server",
                        "body": "reviving requires - x test deleting the secret --- !-- Don't forget about CHANGELOG if this affects the end user! Changelog entry format: - PR-id(PR-URL) Monitoring Component ... PR-id Id of your pull request. PR-URL URL of your PR Component Component affected by your changes such as deps bump, alerts changes and any user facing changes. Example: - 741( Bump thanos components to v0.11.0 release -- I added CHANGELOG entry for this change. No user facing changes, so no entry in CHANGELOG was needed."
                    },
                    {
                        "id": "148",
                        "type": "pullRequest",
                        "title": "MON-3866: chore: csr: Add approver for monitoring CSRs issued for metrics-server",
                        "body": "Also updates the library-go dep to get Currently, cluster-policy-controller only approves CSR with `prometheus-k8s` as the certificate subject. With we need to issue a separate certificate for `metrics-server` SA. (we want to remove the dependency on prometheus' + ease/improve client identification)"
                    }
                ]
            }
        },
        "epics": {
            "MON-4103": {
                "summary": "Prometheus 3 integration",
                "description": "Ref:"
            },
            "MON-4043": {
                "summary": "Configuring external Alertmangers with proxy_url",
                "description": "Proposed title of this feature request Configuring external Alertmangers with proxy_url What is the nature and description of the request? Currently , there is no way to set the proxy_url when adding external alertmanagers instances. Why does the customer need this? (List the business requirements) Customer would like to add external alermanger instance on a disconnected cluster, so proxying the prometheus-alertmanager connection is needed. List any affected packages or components. Prometheus",
                "GITHUB": [
                    {
                        "id": "2580",
                        "type": "pullRequest",
                        "title": "MON-4043: Configuring external Alertmangers with proxy_url",
                        "body": "!-- Don't forget about CHANGELOG if this affects the end user! Changelog entry format: - PR-id(PR-URL) Monitoring Component ... PR-id Id of your pull request. PR-URL URL of your PR Component Component affected by your changes such as deps bump, alerts changes and any user facing changes. Example: - 741( Bump thanos components to v0.11.0 release -- I added CHANGELOG entry for this change. No user facing changes, so no entry in CHANGELOG was needed."
                    }
                ]
            },
            "MON-4098": {
                "summary": "Metrics Server Post GA 2",
                "description": "This epic is to track stories that are not completed in MON-3865"
            }
        }
    },
    "Machine Config Operator": {
        "stories": {
            "MCO-1648": {
                "summary": "Add MCN to must-gather",
                "description": "With the GA of MCN, we will need to have the objects in must-gathers for debugging. Done when: MachineConfigNode objects are included in must-gathers|",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "487",
                        "type": "pullRequest",
                        "title": "MCO-1648: MCO-1649: Add `machineconfignodes` and `pinnedimagesets` to must-gather",
                        "body": "This adds the `MachineConfigNodes` and `PinnedImageSets` resources to must gathers. To test: 1. Launch a 4.19 cluster with tech preview enabled. ``` launch 4.19.0-0.ci-2025-04-28-053740 aws,techpreview ``` 2. Apply a PIS. detailssummaryExample PIS/summary pre apiVersion: machineconfiguration.openshift.io/v1 kind: PinnedImageSet metadata: name: test-pinned labels: machineconfiguration.openshift.io/role: \"worker\" spec: pinnedImages: - name: quay.io/openshift-release-dev/ocp-release@sha256:513cf1028aa1a021fa73d0601427a0fbcf6d212b88aaf9d76d4e4841a061e44e - name: quay.io/openshift-release-dev/ocp-release@sha256:61eae2d261e54d1b8a0e05f6b5326228b00468364563745eed88460af04f909b /pre /details ``` $ oc apply -f pis-file ``` 3. Create the must-gather from the updated `gather_ppc` script. ``` ./collection-scripts/gather_ppc ``` 4. See the resources in the created must-gather. ``` must-gather -- cluster-scoped-resources ---- machineconfiguration.openshift.io ... ------ machineconfignodes -------- ip-10-0-0-9.ec2.internal.yaml -------- ip-10-0-58-49.ec2.internal.yaml -------- ip-10-0-7-211.ec2.internal.yaml -------- ip-10-0-43-205.ec2.internal.yaml -------- ip-10-0-66-94.ec2.internal.yaml -------- ip-10-0-78-97.ec2.internal.yaml ... ------ pinnedimagesets -------- test-pinned.yaml ```"
                    }
                ]
            },
            "MCO-1645": {
                "summary": "Remove `replace` line in `go.mod` of MCO repo",
                "description": "To simultaneously merge the V1 MCN API and updates to the MCN origin tests Once the simultaneous merges are complete and the API is properly bumped in the client-go repo (MCO-1644), the `replace` statements should be removed and the API and client-go versions should point to the merged commits in the openshift/machine-config-operator repo. Done when: PR pointing to the correct API and client-go commits is created and merged in the `openshift/machine-config-operator` repo Cleanup mentioned in this| comment is completed",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "5011",
                        "type": "pullRequest",
                        "title": "MCO-1645: API & client-go bumps for MCN V1 API updates",
                        "body": "- What I did Bump the API dependency to & the client-go dependency to Steps taken: - `go get github.com/openshift/api@9b80d67473bc15d85312ffda09dcde489c8e0545` - `go mod tidy` - `go get github.com/openshift/client-go@5f55ff6979a17ab49a12d181aa3c542372d6917d` - `go mod tidy` - `go mod vendor` - `go mod verify` - How to verify it Tests should pass. - Description for the changelog MCO-1645( API & client-go bumps for MCN V1 API updates"
                    }
                ]
            },
            "MCO-1635": {
                "summary": "Add runbook for HighOverallControlPlaneMemory alert",
                "description": "MCO will send an alert when a node for 1 hour, when all control plane node have extremely high memory usage The alerts describes the following summary: - Memory utilization across all control plane nodes is high, and could impact responsiveness and stability. description: - Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd may be slow to respond. To fix this, increase memory of the control plane nodes. It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for HighOverallControlPlaneMemory alert Created runbook link is accessible to cluster admin with HighOverallControlPlaneMemory alert",
                "epic_key": "MCO-111",
                "GITHUB": [
                    {
                        "id": "5005",
                        "type": "pullRequest",
                        "title": "MCO-1635: Add new runbook for HighOverallControlPlaneMemory to alert",
                        "body": "- What I did Added a runbook I wrote to the HighOverallControlPlaneMemory Alert - How to verify it Trigger HighOverallControlPlaneMemory on cluster. Alert should now display associated runbook. - Description for the changelog Added to alert as a runbook_url."
                    }
                ]
            },
            "MCO-1615": {
                "summary": "Implement node degraded functionality in MCN conditions",
                "description": "Currently, the MachineConfigNode object does not clearly reflect when a node is in a degraded state. Rather, it continues showing the MCN conditions that were last updated during the failed node upgrade. An example of a node failing an upgrade in the \"AppliedFilesAndOS\" phase can be seen here: {code:java} Name: ip-10-0-1-244.ec2.internal Namespace: Labels: none Annotations: none API Version: machineconfiguration.openshift.io/v1alpha1 Kind: MachineConfigNode Metadata: Creation Timestamp: 2025-03-24T12:40:07Z Generation: 3 Owner References: API Version: v1 Kind: Node Name: ip-10-0-1-244.ec2.internal UID: 7137be14-1e41-40a7-91ff-50da0c5693f6 Resource Version: 91058 UID: 9445d162-b0ea-407a-9a82-bba4db7d78db Spec: Config Version: Desired: rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Node: Name: ip-10-0-1-244.ec2.internal Pool: Name: worker Status: Conditions: Last Transition Time: 2025-03-24T12:40:11Z Message: All pinned image sets complete Reason: AsExpected Status: False Type: PinnedImageSetsProgressing Last Transition Time: 2025-03-24T15:58:47Z Message: Update is Compatible. Reason: UpdateCompatible Status: True Type: UpdatePrepared Last Transition Time: 2025-03-24T15:59:47Z Message: Updating the Files and OS on disk as a part of the in progress phase Reason: AppliedFilesAndOS Status: Unknown Type: UpdateExecuted Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the UpdatePostActionComplete phase Reason: NotYetOccurred Status: False Type: UpdatePostActionComplete Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: Uncordoned Node as part of completing upgrade phase Reason: Uncordoned Status: False Type: UpdateComplete Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: In desired config . Resumed normal operations. Reason: Resumed Status: False Type: Resumed Last Transition Time: 2025-03-24T15:58:47Z Message: Update Compatible. Post Cfg Actions : Drain Required: true Reason: UpdatePreparedUpdateCompatible Status: True Type: UpdateCompatible Last Transition Time: 2025-03-24T15:59:45Z Message: Drained node. The drain is complete as the desired drainer matches current drainer: drain-rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Reason: UpdateExecutedDrained Status: True Type: Drained Last Transition Time: 2025-03-24T15:59:47Z Message: Applying files and new OS config to node. OS will not need an update. SSH Keys will not need an update Reason: UpdateExecutedAppliedFilesAndOS Status: Unknown Type: AppliedFilesAndOS Last Transition Time: 2025-03-24T15:58:52Z Message: Cordoned node. The node is reporting Unschedulable = true Reason: UpdateExecutedCordoned Status: True Type: Cordoned Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the RebootedNode phase Reason: NotYetOccurred Status: False Type: RebootedNode Last Transition Time: 2025-03-24T12:40:11Z Message: This node has not yet entered the ReloadedCRIO phase Reason: NotYetOccurred Status: False Type: ReloadedCRIO Last Transition Time: 2025-03-24T15:58:46Z Message: Node ip-10-0-1-244.ec2.internal needs an update Reason: Updated Status: False Type: Updated Last Transition Time: 2025-03-24T12:41:09Z Message: Action during update to rendered-worker-cb3673914e9994a198f0a92079c46ffc: UnCordoned node. The node is reporting Unschedulable = false Reason: UpdateCompleteUncordoned Status: False Type: Uncordoned Last Transition Time: 2025-03-24T12:40:11Z Message: All is good Reason: AsExpected Status: False Type: PinnedImageSetsDegraded Config Version: Current: rendered-worker-cb3673914e9994a198f0a92079c46ffc Desired: rendered-worker-49ecb3b4e784c0a32c04ded0430e5398 Observed Generation: 4 Events: none {code} As can be seen, this does not clearly show that something went wrong with the update. Instead it looks like the upgrade is still proceeding. This impacts our ability to use MCN to power other functionality and will likely lead to customer confusion. A `MachineConfigNodeNodeDegraded` status condition was added as part of the MCN API updates in MCO-1543. This story involves implementing the functionality to populate this condition on a node degrade. Some bugs, including OCPBUGS-44290 and OCPBUGS-52828, have been opened due to issues resulting from MCN not reporting node degradation clearly. Further, MCN clearly reporting on node degraded is needed to power the functionality for MCO-1228, which is part of the status reporting GA. Done when: MCN clearly reports node degrade statuses using the `MachineConfigNodeNodeDegraded` condition",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "4977",
                        "type": "pullRequest",
                        "title": "MCO-1615: MCN status update degraded",
                        "body": "- What I did This change adds the logic to properly report the already existing NodeDegraded condition as part of the MCN status. The logic, based on a function defer, simple checks if the MCD `update` failed and report accordingly. - How to verify it - Deploy a cluster created from an image that contains this change. - Create a MC that is invalid (but reconciliable). The next one is a good example: ```yaml apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: labels: machineconfiguration.openshift.io/role: worker name: 99-invalid-mc spec: config: ignition: version: 3.5.0 storage: files: - contents: source: data:text/plain;charset=utf-8;base64,SSBhbSB2ZXJzaW9uIDEK path: /usr/lib/test-file34.txt ``` - Wait for the MCN to start the update - When the MCP is set as `Degraded` check the status of the MCN that started the update and ensure the `NodeDegraded` condition is set to `True` and that the error message is descriptive - Remove the freshly added conflictive MC - Wait for the MCP to recover from the `Degraded` state - Check back the MCN and ensure the `NodeDegraded` condition is set to `False` - Description for the changelog Report MCN's `NodeDegraded` condition based on the MCD update result."
                    }
                ]
            },
            "MCO-1594": {
                "summary": "Update tests in origin to use explicit opt-out option",
                "description": "The origin tests should be: updated to use the new API from disable the ownerref test as we no longer plan to degrade in that manner in the default-on behavior This should land before MCO-1584| lands.",
                "epic_key": "MCO-1361",
                "GITHUB": [
                    {
                        "id": "29598",
                        "type": "pullRequest",
                        "title": "MCO-1594: updates for ManagedBootImageStatus",
                        "body": "This PR modifies boot image update tests: - Use the explicit opt-out knob when opting out - Skip the degrade test temporarily - ensure cleanups are done in success or fail"
                    }
                ]
            },
            "MCO-1584": {
                "summary": "Implement boot image updates by default on GCP and AWS",
                "description": "This will have to be implemented after lands. The default opt-in should only take place if no boot image configuration currently exists on the cluster.",
                "epic_key": "MCO-1361",
                "GITHUB": [
                    {
                        "id": "4902",
                        "type": "pullRequest",
                        "title": "MCO-1584: Opt-in boot image updates for AWS & GCP by default",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did - The operator will begin to opt-in clusters for boot image updates by default on AWS and GCP, except for cases where the cluster already has an existing boot image update configuration. - The operator will populate a new API field called `ManagedBootImagesStatus` in the `MachineConfiguration` object's status. This will reflect the `spec.managedBootImages` of the object. When the `spec.managedBootImages` is empty, it will reflect the cluster defaults. On platforms that have no boot image updates support, the `managedBootImagesStatus` field will be empty. Currently, only AWS and GCP will be opt-ed in by default. Following are some examples: Scenario: No admin configuration and the current release does not opt-in by default: ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: None ``` Scenario: No admin configuration and the current release does opt-in by default: ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: All ``` Regardless of the default-on behavior of the release, if the admin were to add a configuration, the status must reflect that in the next update. ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: managedBootImages: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: Partial partial: machineResourceSelector: matchLabels: {} status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: Partial partial: machineResourceSelector: matchLabels: {} ``` Scenario: When the current release does not have boot image updates support: ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: status: ``` - How to verify it - Bring up a cluster with this PR on the GCP/AWS platform. You should see the cluster come up with boot image updates enabled. Edit the `MachineConfiguration` to opt-out the cluster by using \"None\" example above. You can also set it to a different configuration such as the partial mode. The operator should not attempt to overwrite this configuration. - Repeat the above procedure on any other platform, you should not see the operator attempt to opt-in the cluster to boot image updates by default. - Bring up a cluster on GCP/AWS on a build without this PR. Set the `MachineConfiguration.Spec.ManagedBootImages` object to a value of your choosing. Upgrade the cluster to a build with this PR and check the `MachineConfiguration` object. The operator should not attempt to overwrite the configuration you chose pre-upgrade. - Bring up a cluster on GCP/AWS on a build without this PR. Leave `MachineConfiguration.Spec.ManagedBootImages` field undefined. Upgrade the cluster to a build with this PR and check the `MachineConfiguration` object. The operator should have opt-ed in your cluster to boot image updates."
                    }
                ]
            },
            "MCO-1579": {
                "summary": "Bump ignition to v2.20.0",
                "description": "This story covers all the needed work from the code side that needs to be done to support the 3.5 ignition spec. To support 3.5 we need to, from a high level perspective: Bump the ignition dependency to v2.20.0, that contains the 3.5 types. Switch all imports that points to 3.4 to point to github.com/coreos/ignition/v2/config/v3_4/types. Create the conversion logic and update the existing ones: convertIgnition34to22 changes to convertIgnition35to22 convertIgnition22to34 changes to convertIgnition22to35 create convertIgnition35to34 Update UTs to reflect above changes and to cover the new function. Done When: The code points to the new ignition release The code uses ignition 3.5 as the default version The UT tests are updated to match the changes for the already existing code plus the new conversion functions",
                "epic_key": "MCO-1234",
                "GITHUB": [
                    {
                        "id": "4903",
                        "type": "pullRequest",
                        "title": "MCO-1579: Bump ignition to 3.5",
                        "body": "- What I did The machine-config-operator now accepts Ignition 3.5 configs and allows nodes to reach the MCS asking for 3.5 config or earlier versions, generated by converting the 3.5 configuration to the requested version, if supported. To achieve this the following changes were done in this PR: - Bump the Ignition dependency to v2.20.0, that implements Ignition 3.5 - Change all the code (mainly imports) to use as the default Ignition config type the 3.5 one instead of the 3.4 - Add the conversion methods for 3.5 to 2.2 and 3.5 to 3.4 - Add the 3.5 version in the MCS - Update the UTs to test 3.5 like any previous version - How to verify it - Deploy a cluster created from an image that contains this change. - Check that the rendered configurations are both, master and worker, Ignition 3.5 ones. - Ensure that all the generated MCs are Ignition 3.5 too. The kubelet MCs can be used as reference. - Create a new Ignition 3.5 MC for the worker pool and check that both, the user defined MC and the worker rendered config are Ignition 3.5. - Follow this( section in the HACKING.md and curl the MCs to get the config for both pools in versions: 3.5, 3.4 and 2.2. - Description for the changelog Bump of the default Ignition version from 3.4 to 3.5. All configurations are now stored, by default, in Ignition 3.5."
                    }
                ]
            },
            "MCO-1544": {
                "summary": "API 2/6 Adapt MCO code to use MCN\u2019s updated v1alpha1 API",
                "description": "The second step in GAing the MCN API is pulling the updated v1alpha1 API worked on in MCO-1543 into the MCO code. This will allow for testing the final API design in the MCO before the API is graduated to V1. Done when: V1alpha1 API for MCN is pulled into the MCO (openshift/machine-config-operator) Code references and MCN functionality are updated according to API changes Tests are updated and passing Requires: MCO-1543 (Create finalized v1alpha1 MCN API)",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "4962",
                        "type": "pullRequest",
                        "title": "MCO-1544: Update MCO code to use MCN\u2019s updated v1alpha1 API",
                        "body": "- What I did - Version bumps - API bump pulls in changes from & - Client-go bump pulls in changes from - Pull in changes made to the MCN API. - Remove references to statuses `MachineConfigNodeUpdateCompatible` and `MachineConfigNodeUpdateReloaded`. - Remove references to `LastFailedGenerationErrors` field. Add new `LastFailedGenerationError` field. - Generalize the message when `MachineConfigNodePinnedImageSetsDegraded` condition is `True`. - Remove unused `getPinnedImageSetSpecForPools` function to cleanup remaining `MachineConfigNodeSpecPinnedImageSet` references. - How to verify it 1. Launch 4.19 cluster with this code and the API code part of with tech preview enabled. Note that the API PR version is needed due to the v1 CRD deployments. Using ClusterBot, an example launch command is: ``` launch 4.19,openshift/machine-config-operator4962,openshift/api2257 aws,techpreview ``` 2. Observe updated fields in MCN objects. _`UPDATECOMPATIBLE` and `RELOADEDCRIO` should not be in the output of `oc get machineconfignode -o wide`._ ```console $ oc get machineconfignode -o wide NAME POOLNAME DESIREDCONFIG CURRENTCONFIG UPDATED UPDATEPREPARED UPDATEEXECUTED UPDATEPOSTACTIONCOMPLETE UPDATECOMPLETE RESUMED UPDATEDFILESANDOS CORDONEDNODE DRAINEDNODE REBOOTEDNODE UNCORDONEDNODE ip-10-0-125-181.us-west-1.compute.internal master rendered-master-d51baee5aae85f41def583ae900f00d3 rendered-master-d51baee5aae85f41def583ae900f00d3 True False False False False False False False False False False ip-10-0-29-132.us-west-1.compute.internal master rendered-master-d51baee5aae85f41def583ae900f00d3 rendered-master-d51baee5aae85f41def583ae900f00d3 True False False False False False False False False False False ip-10-0-5-15.us-west-1.compute.internal worker rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 True False False False False False False False False False False ip-10-0-59-235.us-west-1.compute.internal worker rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 True False False False False False False False False False False ip-10-0-59-8.us-west-1.compute.internal master rendered-master-d51baee5aae85f41def583ae900f00d3 rendered-master-d51baee5aae85f41def583ae900f00d3 True False False False False False False False False False False ip-10-0-92-170.us-west-1.compute.internal worker rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 rendered-worker-1ddf0545a57e0ea7eaebd1fb6a15bc99 True False False False False False False False False False False ``` _The conditions list of a MachineConfigNode should not include items with type `UpdateCompatible` or `ReloadedCRIO`._ ```console $ oc describe machineconfignode/ip-10-0-110-176.us-west-2.compute.internal Name: ip-10-0-110-176.us-west-2.compute.internal Namespace: Labels: none Annotations: none API Version: machineconfiguration.openshift.io/v1alpha1 Kind: MachineConfigNode Metadata: Creation Timestamp: 2025-04-03T22:08:41Z Generation: 2 Owner References: API Version: v1 Kind: Node Name: ip-10-0-110-176.us-west-2.compute.internal UID: f3d946e9-24d4-4b96-b4a7-4536d79c73a8 Resource Version: 18006 UID: 7f03657f-d52a-4a48-b835-370f22fc773a Spec: Config Version: Desired: rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb Node: Name: ip-10-0-110-176.us-west-2.compute.internal Pool: Name: worker Status: Conditions: Last Transition Time: 2025-04-03T22:08:41Z Message: All pinned image sets complete Reason: AsExpected Status: False Type: PinnedImageSetsProgressing Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the UpdatePrepared phase Reason: NotYetOccurred Status: False Type: UpdatePrepared Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the UpdateExecuted phase Reason: NotYetOccurred Status: False Type: UpdateExecuted Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the UpdatePostActionComplete phase Reason: NotYetOccurred Status: False Type: UpdatePostActionComplete Last Transition Time: 2025-04-03T22:09:55Z Message: Action during update to rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb: Uncordoned Node as part of completing upgrade phase Reason: Uncordoned Status: False Type: UpdateComplete Last Transition Time: 2025-04-03T22:09:55Z Message: Action during update to rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb: In desired config . Resumed normal operations. Reason: Resumed Status: False Type: Resumed Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the Drained phase Reason: NotYetOccurred Status: False Type: Drained Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the AppliedFilesAndOS phase Reason: NotYetOccurred Status: False Type: AppliedFilesAndOS Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the Cordoned phase Reason: NotYetOccurred Status: False Type: Cordoned Last Transition Time: 2025-04-03T22:08:41Z Message: This node has not yet entered the RebootedNode phase Reason: NotYetOccurred Status: False Type: RebootedNode Last Transition Time: 2025-04-03T22:09:55Z Message: Node ip-10-0-110-176.us-west-2.compute.internal Updated Reason: Updated Status: True Type: Updated Last Transition Time: 2025-04-03T22:09:55Z Message: Action during update to rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb: UnCordoned node. The node is reporting Unschedulable = false Reason: UpdateCompleteUncordoned Status: False Type: Uncordoned Last Transition Time: 2025-04-03T22:08:41Z Message: All is good Reason: AsExpected Status: False Type: PinnedImageSetsDegraded Config Version: Current: rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb Desired: rendered-worker-8c82b2aa30e5b7d400201f40ebd660fb Observed Generation: 3 Events: none ``` 3. Apply a valid MC and watch the MCN object throughout the update to see `MachineConfigNodeUpdateRebooted` as a parent phase. - Note that there are now two update paths: - If an update requires a reboot, the post action complete phase will not transition through the unknown -- true -- false phase, only the rebooted node phase will. - If an update does not require a reboot, the post action complete phase will transition through the unknown -- true -- false phase, and the rebooted node phase will not. - To see the following outcomes, apply an MC that requires a node reboot. - To see the post action complete phase transition, trigger an update that will not cause a reboot of the node (ex. a node disruption policy update). _Before the update_ ``` Last Transition Time: 2025-04-03T22:08:12Z Message: This node has not yet entered the RebootedNode phase Reason: NotYetOccurred Status: False Type: RebootedNode ``` _During the reboot_ ``` Message: Upgrade requires a reboot. Reason: RebootedNode Status: Unknown Type: RebootedNode ``` _After successful reboot_ ``` Message: Node has rebooted Reason: RebootedNode Status: True Type: RebootedNode ``` 4. Apply an invalid PIS, such as the one below. ``` apiVersion: machineconfiguration.openshift.io/v1 kind: PinnedImageSet metadata: name: test-pinned labels: machineconfiguration.openshift.io/role: \"worker\" spec: pinnedImages: - name: quay.io/rh-ee-ijanssen/machine-config-operator@sha256:65d3a308767b1773b6e3ead2ec1bcae499dde6ef085753d7e20e685f78841079 - name: quay.io/rh-ee-ijanssen/machine-config-operator@sha256:fd3692eff21338e900a244dfe62152c959b84d73f2dd4503893de0f3fae61b0b ``` _The error message in the `PinnedImageSetsDegraded` MCN conditions and PIS status reference should now be a generalized message and the `LastFailedGenerationErrors` field should now be a string type field called `LastFailedGenerationError`._ ``` $ oc describe machineconfignode ip-10-0-36-180.us-east-2.compute.internal ... Status: Conditions: Last Transition Time: 2025-04-03T12:27:06Z Message: node is prefetching images: ip-10-0-36-180.us-east-2.compute.internal Reason: ImagePrefetch Status: True Type: PinnedImageSetsProgressing ... Last Transition Time: 2025-04-03T12:27:07Z Message: One or more PinnedImageSet is experiencing an error. See PinnedImageSet list for more details Reason: PrefetchFailed Status: True Type: PinnedImageSetsDegraded ... Pinned Image Sets: Desired Generation: 2 Last Failed Generation: 2 Last Failed Generation Error: failed to execute podman manifest inspect for \"quay.io/rh-ee-ijanssen/machine-config-operator@sha256:65d3a308767b1773b6e3ead2ec1bcae499dde6ef085753d7e20e685f78841079\": exit status 125 Name: test-pinned ``` 5. Apply a valid PIS, such as the one below. ``` apiVersion: machineconfiguration.openshift.io/v1 kind: PinnedImageSet metadata: name: test-pinned labels: machineconfiguration.openshift.io/role: \"worker\" spec: pinnedImages: - name: quay.io/openshift-release-dev/ocp-release@sha256:513cf1028aa1a021fa73d0601427a0fbcf6d212b88aaf9d76d4e4841a061e44e - name: quay.io/openshift-release-dev/ocp-release@sha256:61eae2d261e54d1b8a0e05f6b5326228b00468364563745eed88460af04f909b ``` _The MCN conditions and PIS status reference should report as before._ _Note that the new MCN API validation requires that `LastFailedGenerationError` is populated when a `LastFailedGeneration` value is reported. Since the default `LastFailedGeneration` of 0 always showed on a successful PIS application (as tested in cluster version `4.19.0-0.nightly-arm64-2025-03-21-080021`), an empty string for `LastFailedGenerationError` is now also being reported. This will not be fixed in this PR as it is a result of an existing bug (see OCPBUG-54592( ``` ... Status: Conditions: Last Transition Time: 2025-04-03T11:53:06Z Message: All pinned image sets complete Reason: AsExpected Status: False Type: PinnedImageSetsProgressing ... Last Transition Time: 2025-04-03T11:53:06Z Message: All is good Reason: AsExpected Status: False Type: PinnedImageSetsDegraded ... Pinned Image Sets: Current Generation: 1 Desired Generation: 1 Last Failed Generation: 0 Last Failed Generation Error: Name: test-pinned ... ``` - Description for the changelog MCO-1544( Update MCO code to use MCN\u2019s updated v1alpha1 API"
                    }
                ]
            },
            "MCO-1543": {
                "summary": "API 1/6 Create finalized v1alpha1 MCN API",
                "description": "The first step in GAing the MCN API is finalizing the v1alpha1 API. This will allow for testing of the final API design before the API is graduated to V1. Since there are a fair amount of changes likely to be made for the MCN API, making our changes in v1alpha1 first seems to follow the API team\u2019s preference| of V1 API graduations only having minor changes. Done when: V1alpha1 API for MCN is finalized The MCN API fields are all properly documented All design decisions are appropriately documented",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "2201",
                        "type": "pullRequest",
                        "title": "MCO-1543: Update v1alpha1 MCN API",
                        "body": "This is intended to be the final update to the the v1alpha1 MCN API before the API is graduated to V1. Notable updates: - The `Status.PinnedImageSets.LastFailedGenerationErrors` list type field is being removed and a replacement `Status.PinnedImageSets.LastFailedGenerationError` string type field is being added, as a PinnedImageSet error will be a single error string. - More information on the intended use of this and the `Status.PinnedImageSets.LastFailedGeneration` field can be found in this Slack thread( - The original behavior of the fields can be seen in this comment( - Some statuses were removed as they are no longer as valuable. - A `MachineConfigNodeNodeDegraded` status was added and will alert the user of a node degraded state. Relevant information: - Original enhancement( - WIP Updated Enhancement("
                    }
                ]
            },
            "MCO-1522": {
                "summary": "API 2/4 Adapt MCO code to use PIS\u2019s GA API",
                "description": "The second step in GAing PinnedImageSets is adapting the MCO code (in openshift/machine-config-operator) to use the V1 (GA) API created in MCO-1521. Done when: MCO code is pointing to the GA PIS API Code references and functionality are updated according to API changes Requires: MCO-1521 (Create GA PIS API)",
                "epic_key": "MCO-1258",
                "GITHUB": [
                    {
                        "id": "4934",
                        "type": "pullRequest",
                        "title": "MCO-1522: Adapt MCO to use PIS V1 API",
                        "body": "Bumps the API version Bumps the client-go version Modify the MCC, MCD and Tests to include the V1 PIS API"
                    }
                ]
            },
            "MCO-1521": {
                "summary": "API 1/4 Create GA PIS API",
                "description": "The first step in GAing PinnedImageSets is creating a V1 (GA) API for the feature. A related API PR to use as a reference for the expected work can be found here| Done when: The PIS API properly documents all fields included in the GA API Any design decisions/field changes are appropriately documented/tracked The PIS V1 API PR is reviewed and merged into master of openshift/api",
                "epic_key": "MCO-1258",
                "GITHUB": [
                    {
                        "id": "2198",
                        "type": "pullRequest",
                        "title": "MCO-1521: Promote PinnedImageSet to GA",
                        "body": "Promotes machineconfiguration/v1alpha1 PinnedImageSet API to machineconfiguration/v1 PIS API"
                    }
                ]
            },
            "MCO-1519": {
                "summary": "API 4/6 Adapt MCO & origin code to use MCN\u2019s V1 API",
                "description": "The fourth step in GAing the MCN API is pulling the V1 API created in MCO-1518 into the MCO & origin code. Related PRs to use as references for the expected work can be found here for the MCO repo Done when: MCO code is pointing to the V1 MCN API Origin code is pointing to the V1 MCN API Code references and functionality are updated according to the API needs Requires: MCO-1518 (Create GA MCN API) This involves a simultaneous merge of the MCO, origin, and API",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "4992",
                        "type": "pullRequest",
                        "title": "MCO-1519: Adapt MCO code to use V1 MCN API",
                        "body": "Merge Plan - This should merge _simultaneously with_ - - - What I did This updates references to MachineConfigNode's v1alpha1 API to instead reference the v1 API created in - How to verify it - Validate MCN functionality in a live cluster using the following Clusterbot launch command: ```launch 4.19,openshift/api2273,openshift/machine-config-operator4992 gcp,techpreview``` - Origin tests should run. See payload tests run on - Description for the changelog MCO-1519( update MCN API references from v1alpha1 to v1"
                    },
                    {
                        "id": "29701",
                        "type": "pullRequest",
                        "title": "MCO-1519: Adapt MCN tests to use V1 API",
                        "body": "Merge Plan: - This should be merged _simultaneously with_ - - Work included: - Bump API & client-go versions pull in changes for created V1 MCN API ( - Replace `v1alpha1` references with `v1` - Skip all MCN & PIS tests on hypershift - See this thread( for more information on hypershift support for these features. Payload testing: To test the changes made to the test, payload tests should be run with the & PRs to test the three PRs to include as part of the simul-merge. Test locally: To run the test locally, run them against a cluster created with the API built from and the MCO built from An example Clusterbot message for creating such a cluster is: `launch 4.19,openshift/api2273,openshift/machine-config-operator4992 gcp,techpreview`. _Serial suite tests (MCN):_ ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialShould properly transition through MCN conditions on rebootless node update apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialShould properly transition through MCN conditions on rebootless node update apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` _Serial suite tests (PIS):_ ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:PinnedImagesOCPFeatureGate:MachineConfigNodesSerial All Nodes in a custom Pool should have the PinnedImages even after Garbage Collection apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:PinnedImagesOCPFeatureGate:MachineConfigNodesSerial All Nodes in a Custom Pool should have the PinnedImages in PIS apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:PinnedImagesOCPFeatureGate:MachineConfigNodesSerial All Nodes in a standard Pool should have the PinnedImages PIS apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:PinnedImagesOCPFeatureGate:MachineConfigNodesSerial Invalid PIS leads to degraded MCN in a standard Pool apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:PinnedImagesOCPFeatureGate:MachineConfigNodesSerial Invalid PIS leads to degraded MCN in a custom Pool apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` _Parallel suite tests:_ ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should have MCN properties matching associated node properties for nodes in default MCPs apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly block MCN updates by impersonation of the MCD SA apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly block MCN updates from a MCD that is not the associated one apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly update the MCN from the associated MCD apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` _Local only tests:_ ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialSlowShould properly report MCN conditions on node degrade apigroup:machineconfiguration.openshift.io\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialSlowShould properly create and remove MCN on node creation and deletion apigroup:machineconfiguration.openshift.io\" ```"
                    }
                ]
            },
            "MCO-1509": {
                "summary": "Inject /etc/containers into OCL build pod from rendered MachineConfig",
                "description": "To make the OCL build process more consistent as well as enabling it to work in a disconnected environment, we should inject the contents of the /etc/containers directory into the builder pod. These files arre managed by the container-runtime-config controller.path' /etc/containers/registries.conf /etc/crio/crio.conf.d/00-default /etc/machine-config-daemon/policy-for-old-podman.json /etc/containers/policy.json $ oc get mc/99-master-generated-registries -o yaml grep \"/etc/containers\" /etc/containers/storage.conf /etc/containers/registries.conf /etc/containers/policy.json /etc/containers/registries.d/sigstore-registries.yaml{noformat} As an implementation detail, the most straightforward way to do this would be to have the BuildRequest object may need to copy those files into place or adjust an SELinux context before starting the build, since that was needed for the /etc/pki/entitlements functionality to work. Although that might also not be necessary since these files were passed through to the build context whereas /etc/containers is more for configuring Buildah itself. Unknowns at the time of this writing: Should the ControllerRuntimeConfig| be considered as part of this? Is /etc/containers/storage.conf appropriate to inject into the builder pod? Done When: The contents of /etc/containers are mounted into the builder pod. The unknowns are resolved and additional files, if any, are injected into the builder pod.",
                "epic_key": "MCO-1507",
                "GITHUB": [
                    {
                        "id": "4971",
                        "type": "pullRequest",
                        "title": "MCO-1509: Enable OCL for disconnected workflow",
                        "body": "Closes !-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did Grab the /etc/containers/registries.conf and /etc/containers/policy.json files from the node and mount it into the builder pod so that buildah can use it during the build process. Note: mirrors only work for image pulls and not for image push. So the mirror here will be used when doing the FROM BaseOSImage part in the Containerfile during the build. - How to verify it Replace the value of `BaseOSImage` with a registry/image that doesn't exist. Set up an IDMS, something like this with the mirror and source configured: ``` apiVersion: config.openshift.io/v1 kind: ImageDigestMirrorSet metadata: name: builder-test spec: imageDigestMirrors: - mirrors: - quay.io/umohnani8/mirror-base source: quay.io/umohnani8/og-base mirrorSourcePolicy: AllowContactingSource ``` Create a MachineOSConfig. The mirrors should be resolved and the build process should pull the base image from your mirrored location. I verified this by looking at the build logs and by ensure my \"source\" registry/image didn't actually exist. So the only way it could pull the image is from the mirror. Note mirrors only work on image pulls and not for image push. So the registry to push to should be the actual registry and not something that is mirrored. I don't think there is a way for me to add an e2e test for this, so will have to rely on QE testing for this feature. - Description for the changelog Mount the registries.conf and policy.json with mirror configuration the build pod so it can used during the image build process. !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "MCO-1501": {
                "summary": "Support Custom MCPs in MCN",
                "description": "The machine config node functionality should fully support custom machine config pools. Investigation from MCO-1299 highlights that custom MCPs are not supported. This story will be done when the following is complete: Custom pools should be reflected in the \"POOLNAME\" column when a user runs {code:java} $ oc get machineconfignode {code} Users should be able to check the status of a specific, custom MCP using {code:java} $ oc get machineconfignodes $(oc get machineconfignodes -o json select(.spec.pool.name==\"pool_name\")|.metadata.name') {code} MCNs (as seen when running oc describe machineconfignode/node-name) should reflect the custom pools a node is a part of under Spec.Pool.Name",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "4876",
                        "type": "pullRequest",
                        "title": "MCO-1501: Add support for custom MCPs in MCN",
                        "body": "- What I did This work adds support for custom MCPs in MCN. The update makes use of the previously existing `GetPrimaryPoolForNode` function to get the pool a node is associated with. - How to verify it _Test for standard case_ 1. Create a custom MCP named `infra` and add a worker node to the new MCP. 2. View the `machineconfignode` object for the `infra` node and check that the pool name matches. ``` $ oc describe machineconfignode node-name ... Spec: ... Pool: Name: infra ``` 3. Check that the pool names are properly populated for all `machineconfignode`. ``` $ oc get machineconfignode NAME POOLNAME DESIREDCONFIG CURRENTCONFIG UPDATED ip-10-0-101-100.us-west-2.compute.internal infra rendered-infra-xxxxx rendered-infra-xxxxx True ... ``` _Test node with multiple labels_ 1. Create 2 MCPs named `infra` and `custom` and add one worker node to each new MCP. 2. Add the `custom` label to the the node part of the `infra` MCP. ``` oc label node node-name node-role.kubernetes.io/custom= ``` 3. View the `machineconfignode` object for the node part of the `infra` pool and check that the pool name is properly populating as `infra`. _Note that the node will not be a part of `custom`, as that label was added after the node joined the `infra` pool and the node can only be a part of one MCP._ ``` $ oc describe machineconfignode node-name ... Spec: ... Pool: Name: infra ``` 4. Check that the pool names are properly populated for all `machineconfignode`. ``` $ oc get machineconfignode NAME POOLNAME DESIREDCONFIG CURRENTCONFIG UPDATED ip-10-0-101-100.us-west-2.compute.internal infra rendered-infra-xxxxx rendered-infra-xxxxx True ip-10-0-113-251.us-west-2.compute.internal custom rendered-custom-xxxxx rendered-custom-xxxxx True ... ... ``` 5. Remove the `infra` label from the infra node. This will force the node to become part of the `custom` MCP due to that label on the node. ``` oc label node node-name-2 node-role.kubernetes.io/infra- ``` 6. View the `machineconfignode` object for the node previously part of the `infra` pool and check that the pool name is now properly populating as `custom`. ``` $ oc describe machineconfignode node-name ... Spec: ... Pool: Name: custom ``` 7. Check that the pool names are properly populated for all `machineconfignode`. ``` $ oc get machineconfignode NAME POOLNAME DESIREDCONFIG CURRENTCONFIG UPDATED ip-10-0-101-100.us-west-2.compute.internal custom rendered-custom-xxxxx rendered-custom-xxxxx True ip-10-0-113-251.us-west-2.compute.internal custom rendered-custom-xxxxx rendered-custom-xxxxx True ... ``` _Test by deleting MCP_ 1. Create a custom MCP named `custom` and add a worker node to the new MCP. Validate the `machineconfignode` for the node in the new MCP. 2. Remove the custom pool labels from the nodes and delete the `custom` MCP. This will force the node previously part of the pool to become part of the default `worker` pool. ``` $ oc label node node-name node-role.kubernetes.io/custom- $ oc delete mcp/custom ``` 3. View the `machineconfignode` object for the node part previously part of the `custom` pool and check that the pool name is now properly populating as `worker`. ``` $ oc describe machineconfignode node-name ... Spec: ... Pool: Name: worker ``` 4. Check that the pool names are properly populated for all `machineconfignode`. ``` $ oc get machineconfignode NAME POOLNAME DESIREDCONFIG CURRENTCONFIG UPDATED ip-10-0-101-100.us-west-2.compute.internal worker rendered-worker-xxxxx rendered-worker-xxxxx True ... ``` - Description for the changelog MCO-1501( Added support for custom MCPs in MCN"
                    }
                ]
            },
            "MCO-1492": {
                "summary": "Add Runbook for SystemMemoryExceedsReservation",
                "description": "MCO will send an alert when a node for 15 minutes, a specific node is using more memory than is reserved. The alerts describes the following \"summary: \"Alerts the user when, for 15 minutes, a specific node is using more memory than is reserved\" description: \"System memory usage of \\ $value | humanize on \\ $labels.node exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased ( when running nodes with high numbers of pods (either due to rate of change or at steady state).\"\" It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for SystemMemoryExceedsReservation alert Created runbook link is accessible to cluster admin with SystemMemoryExceedsReservation alert",
                "epic_key": "MCO-111",
                "GITHUB": [
                    {
                        "id": "4832",
                        "type": "pullRequest",
                        "title": "MCO-1492: Add new runbook for SystemMemoryExceedsReservation to alert",
                        "body": "- What I did Added a runbook I wrote to the Alert - How to verify it Trigger SystemMemoryExceedsReservation on cluster. Alert should now display associated runbook. - Description for the changelog Added to alert as a runbook_url."
                    }
                ]
            },
            "MCO-1485": {
                "summary": "Boot Image Controller should attempt to upgrade stub ignition to spec 3",
                "description": "The boot image controller should should ensure `-user-data` stub secrets are at least spec 3. This requires the cert management work to land first. To ensure maximum coverage and minimum risk, we will only attempt to upgrade stub secrets that are currently spec 2. While we could potentially upgrade all stubs to the newest(which at the moment is 3.4.0) supported by the MCO, this may cause issues like for some boot images that only support early spec 3 ignition(certain older boot images can only do 3.0.0 and 3.1.0 ignition). Newer boot images can support all spec 3 stubs, so to preserve scaling ability as much as we can, we'll leave spec 3 stubs as is for the moment.",
                "epic_key": "MCO-1361",
                "GITHUB": [
                    {
                        "id": "4885",
                        "type": "pullRequest",
                        "title": "MCO-1485: MCO:1486: MCO-1324: Attempt stub Ignition upgrade",
                        "body": "- What I did - If a `MachineSet` enrolled for boot image updates has a spec 2 Ignition stub, the boot image controller will attempt to upgrade it to the latest Ignition version used by the MCO(which at the moment, is `3.4.0`). - If the upgrade fails, the boot image controller will degrade and abort the boot image update. When an upgrade is successful, the timestamp and new version is annotated on the secret object. - The boot image controller will no longer hardcode an update enrolled `MachineSet` to `-managed` stub. The e2e tests have been adjusted accordingly. - As is now fixed, we can directly use the `releaseVersion` in the `coreos-bootimages` configmap instead of the one written into the configmap by the MCO. - Added an e2e to verify the stub upgrade behavior - How to verify it - Manually edit the Ignition stub in a `-user-data` secret to use a spec 2 stub. This link( should give you an idea of what a spec 2 stub should look like. Reference the secret in a `MachineSet` object and then opt-in said `MachineSet` for boot image updates. - Check the MCC logs to ensure that the stub upgrade successfully took place. This should also be apparent by examining the contents and the annotation on the `-user-data` secret. - Scale up a node that uses this Ignition stub. This should succeed. Error modes: I haven't been able to test this, because I'm not sure how to create an un-upgradeable 2.2 stub. Once could reference this \"invalid stub\" in a `MachineSet` enrolled for boot image updates, and this should result in a degrade."
                    }
                ]
            },
            "MCO-1482": {
                "summary": "machine-config ClusterOperator stays Upgradeable=True as new nodes are added",
                "description": "Implementing RFE-3017. As a bare Story, without a Feature or Epic, because I'm trying to limit the amount of MCO-side paperwork required to get my own RFE itch scratched. As a Story and not a NO-ISSUE pull, because OCP QE had a bit of trouble handling mco4637| when I went NO-ISSUE on that one, and I think this might be worth a 4.19 release note.",
                "GITHUB": [
                    {
                        "id": "4760",
                        "type": "pullRequest",
                        "title": "MCO-1482: pkg/operator/status: Drop PoolUpdating as an Upgradeable=False condition",
                        "body": "956e7874dc (4012) had added the \"this should no longer trigger when adding a node to a pool\" comment, but unfortunately, it's still triggering. For example, in this serial 4.19 run1: ```console $ curl -s Feature:MachinesSerial Managed cluster should grow and decrease when scaling different machineSets simultaneously Timeout:30mapigroup:machine.openshift.io Suite:openshift/conformance/serial\" passed: (5m42s) 2024-12-16T00:57:49 \"sig-cluster-lifecycleFeature:MachinesSerial Managed cluster should grow and decrease when scaling different machineSets simultaneously Timeout:30mapigroup:machine.openshift.io Suite:openshift/conformance/serial\" ``` confirmed via MCC logs: ```console $ curl -s Pool workerzone=us-central1-f: node ci-op-k8c03v6z-9149a-r27w7-worker-f-t7rmb: changed annotation machineconfiguration.openshift.io/currentConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:35.430252 1 node_controller.go:584 Pool workerzone=us-central1-f: node ci-op-k8c03v6z-9149a-r27w7-worker-f-t7rmb: changed annotation machineconfiguration.openshift.io/desiredConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:36.174629 1 node_controller.go:584 Pool workerzone=us-central1-a: node ci-op-k8c03v6z-9149a-r27w7-worker-a-f7hkj: changed annotation machineconfiguration.openshift.io/currentConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:36.174738 1 node_controller.go:584 Pool workerzone=us-central1-a: node ci-op-k8c03v6z-9149a-r27w7-worker-a-f7hkj: changed annotation machineconfiguration.openshift.io/desiredConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:41.296273 1 node_controller.go:584 Pool workerzone=us-central1-b: node ci-op-k8c03v6z-9149a-r27w7-worker-b-554bt: changed annotation machineconfiguration.openshift.io/currentConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:41.296306 1 node_controller.go:584 Pool workerzone=us-central1-b: node ci-op-k8c03v6z-9149a-r27w7-worker-b-554bt: changed annotation machineconfiguration.openshift.io/desiredConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:47.106173 1 node_controller.go:584 Pool workerzone=us-central1-c: node ci-op-k8c03v6z-9149a-r27w7-worker-c-hshj2: changed annotation machineconfiguration.openshift.io/currentConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 I1216 00:55:47.106201 1 node_controller.go:584 Pool workerzone=us-central1-c: node ci-op-k8c03v6z-9149a-r27w7-worker-c-hshj2: changed annotation machineconfiguration.openshift.io/desiredConfig = rendered-worker-6d0e61dc44f24db3272625b901024ed2 ``` In this commit, I'm dropping the code that had been moving the ClusterOperator to `Upgradeable=False` on `PoolUpdating` entirely, instead of hoping that it doesn't trip. I haven't dug into why the code had still been tripping. But we want to stay `Upgradeable=True` while new nodes scale in, because clusters where nodes are joining should still be able to update to 4.(y+1). There are node-vs.-control-plane skew issues that should block updates to 4.(y+1), but they're enforced by the Kube API server operator (openshift/cluster-kube-apiserver-operator/pull/1199), and don't need the MCO chipping in. 1: - Description for the changelog The machine-config ClusterOperator no longer goes `Upgradeable=False` on `PoolUpdating` when new Nodes join the cluster."
                    }
                ]
            },
            "MCO-1457": {
                "summary": "Clean up bootstrap MCS CA & TLS cert objects for management",
                "description": "The CA/cert generated by the installer is not currently managed and also does not preserve the signing key; so the cert controller we are adding in the MCO(leveraged from library-go), throws away everything and starts fresh. Normally this happens fairly quickly so both the MCS and the -user-data secrets are updated together. However, in certain cases(such as agent based installations) where a bootstrap node joins the cluster late, it will have the old CA from installer, and unfortunately the MCS will have a TLS cert signed by the new CA - resulting in invalid TLS cert errors. To account for such cases, we have to ensure the first CA embedded in any machine is matching the format expected by the cert controller. To do this, we'll have to do the following in the installer: -Have the bootstrap MCC generate the CA/TLS cert using the cert controller, and populate them into the right places(this card)- -Make changes in the installer to remove the creation of the CA/cert, since the bootstrap MCC will now handle this( Template out all root-ca artifacts in the format expected by the library-go cert controller. This would involve adding certain annotations on the artifacts(with respect to validity of the cert and some other ownership metadata) The root CA signing key is currently discarded by the installer, so this will have to be a new template in the installer.",
                "epic_key": "MCO-1208",
                "GITHUB": [
                    {
                        "id": "9309",
                        "type": "pullRequest",
                        "title": "MCO-1457: Clean up MCS CA & TLS cert objects for management",
                        "body": "This PR adds and fixes up a few templates related to the MCS CA/TLS, so that it matches the format expected by the cert controller being added to the MCO in With these template changes in place, the cert controller of the MCO should no longer cause an immediate rotation when it comes up, preventing issues such as"
                    }
                ]
            },
            "MCO-1451": {
                "summary": "Merge \"Zack's Scripts\", clean up hack scripts",
                "epic_key": "MCO-1298",
                "GITHUB": [
                    {
                        "id": "4718",
                        "type": "pullRequest",
                        "title": "MCO-1451: Merge \"Zack's Scripts\"",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did - How to verify it - Description for the changelog !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "MCO-1449": {
                "summary": "Add Runbook for MCDPivotError",
                "description": "MCD will send an alert when a node failes to pivot to another MachineConfig. This could prevent an OS upgrade from succeeding. The alert contains the information on what logs to look for The alerts describes the following \"Error detected in pivot logs on \\ $labels.node , upgrade may be blocked. For more details: oc logs -f -n \\ $labels.namespace \\ $labels.pod -c machine-config-daemon \" It is possible that admin may not be able to interpret exact action to be taken after looking at MCD pod logs. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for MCDPivotError alert Created runbook link is accessible to cluster admin with MCDPivotError alert",
                "epic_key": "MCO-111",
                "GITHUB": [
                    {
                        "id": "4771",
                        "type": "pullRequest",
                        "title": "MCO-1449: Add MCDPivotError runbook to prometheus rules",
                        "body": "Please provide the following information: - What I did Added a runbook I wrote to the MCDPivotError Alert - How to verify it Trigger MCDPivotError on cluster. Alert should now display associated runbook. - Description for the changelog Added to alert as a runbook_url."
                    }
                ]
            },
            "MCO-1443": {
                "summary": "Graduate MOSB/MOSC API to v1",
                "description": "To make OCL ready for GA, the first step would be graduating the MCO's APIs from v1alpha1 to v1. This requires changes in the openshift/api repo.",
                "epic_key": "MCO-1316",
                "GITHUB": [
                    {
                        "id": "2090",
                        "type": "pullRequest",
                        "title": "MCO-1443: Promote onclusterbuild to GA",
                        "body": "Opened for testing. Based on guidance this is currently the first step: create new v1 API, gate remains off, v1 API is excluded from the image manifests Also adds in:"
                    }
                ]
            },
            "MCO-1437": {
                "summary": "Inherit from global pull secret if baseImagePullSecret field is not specified",
                "description": "As a follow up to the one field we identified that is best updated pre-GA is to make the baseImagePullSecret optional. The builder pod should have the logic to fetch from baseImagePullSecret if the user does not specify this via a MachineOSConfig object.",
                "epic_key": "MCO-1316",
                "GITHUB": [
                    {
                        "id": "4756",
                        "type": "pullRequest",
                        "title": "MCO-1437: MCO-1476: MCO-1477: MCO-1284: Adapt MCO to OCL v1 API",
                        "body": "This PR does the following: - bumps client-go to capture OCL v1 API helpers - reconciles machine-os-builder and the operator to handle OCL v1 API - reconciles existing units/e2es to handle OCL v1 API - adds a global pull secret cloning mechanism since `BaseImagePullSecret` is optional in v1 API How to test/verify: - TechPreview should be enabled - All existing units/e2es should pass - OCL workflows should be tested with v1 CRs(v1alpha1 CRDs will no longer work). Note: This PR should be tested and merged along with Clusterbot incantation: ``` launch openshift/machine-config-operator4756,openshift/api2134 techpreview ```"
                    }
                ]
            },
            "MCO-1416": {
                "summary": "Separate OCL e2e tests into new test suite",
                "description": "The original scope of this task is represented across this story & MCO-1494. With OCL GA'ing soon, we'll need a blocking path within our e2e test suite that must pass before a PR can be merged. This story represents the first stage in creating the blocking path: Migrate the tests from e2e-gcp-op-techpreview into a new test suite called e2e-ocl. This can be done by moving the tests in the MCO repo from the test/e2e-techpreview folder to a new test/e2e-ocl folder. There might be some minor cleanups such as fixing duplicate function names, etc. but it should be fairly straightforward to do. Make a new e2e-gcp-op-ocl job to call the newly created e2e-ocl test suite. This test should first be added as optional for 4.19 so it can be stability tested before it is made blocking for 4.18 & 4.19. This will require a PR to the openshift/release repo to call the new test for 4.19 & master. This should be a pretty straightforward config change.",
                "epic_key": "MCO-828",
                "GITHUB": [
                    {
                        "id": "4750",
                        "type": "pullRequest",
                        "title": "MCO-1416: Update timeouts & improve debuggability of ocl e2e tests",
                        "body": "- What I did - Undo timeout updates made here( - Pull in Zack's work( that updates the OCL e2e test suite \"to leave objects in-situ whenever the test suite is being run in OpenShift CI\" with the intent to capture the test cluster's state to improve test debugging. - How to verify it Verified in Zack's PR( - Description for the changelog MCO-1416( Update timeouts & improve debuggability of ocl e2e tests"
                    },
                    {
                        "id": "4730",
                        "type": "pullRequest",
                        "title": "MCO-1416: Increase timeouts in OCL e2e tests",
                        "body": "- What I did Increase the timeouts in different stages of the ocl e2e tests. - How to verify it N/A - Description for the changelog MCO-1416( Increase timeouts in OCL e2e tests"
                    },
                    {
                        "id": "4699",
                        "type": "pullRequest",
                        "title": "MCO-1416: Create OCL e2e Test Suite & Remove Unneeded Helper",
                        "body": "- What I did - Moved the OCL test files from the `test/e2e-techpreview` folder to a new `test/e2e-ocl` folder. - Removed the unneeded `MustHaveFeatureGatesEnabled` function. - How to verify it Test the Makefile update by running `make test-e2e-ocl`. ```bash $ make test-e2e-ocl --quiet Installing go-junit-report 2024/11/21 09:47:57 All required featuregates OnClusterBuild MachineConfigNodes present! === RUN TestOnClusterBuildsOnOKD helpers_test.go:630: OCP detected, skipping test TestOnClusterBuildsOnOKD helpers_test.go:631: --- SKIP: TestOnClusterBuildsOnOKD (0.08s) === RUN TestOnClusterBuildsCustomPodBuilder ... ``` - Description for the changelog MCO-1416( Separate OCL e2e tests from tech preview and remove unneeded helper"
                    }
                ]
            },
            "MCO-1165": {
                "summary": "Regression BuildController should have a rebuild function",
                "description": "REGRESSION We need to reinvent the wheel for triggering rebuild functionality and the rebuild mechanism as pool labeling and annotation is no longer a favorable way to interact with layered pools There are a few situations in which a cluster admin might want to trigger a rebuild of their OS image in addition to situations where cluster state may dictate that we should perform a rebuild. For example, if the custom Dockerfile changes or the machine-config-osimageurl changes, it would be desirable to perform a rebuild in that case. To that end, this particular story covers adding the foundation for a rebuild mechanism in the form of an annotation that can be applied to the target MachineConfigPool. What is out of scope for this story is applying this annotation in response to a change in cluster state (e.g., custom Dockerfile change). Done When: BuildController is aware of and recognizes a special annotation on layered MachineConfigPools (e.g., {{{}machineconfiguration.openshift.io/rebuildImage{}}}). Upon recognizing that a MachineConfigPool has this annotation, BuildController will clear any failed build attempts, delete any failed builds and their related ephemeral objects (e.g., rendered Dockerfile / MachineConfig ConfigMaps), and schedule a new build to be performed. This annotation should be removed when the build process completes, regardless of outcome. In other words, should the build success or fail, the annotation should be removed. optional BuildController keeps track of the number of retries for a given MachineConfigPool. This can occur via another annotation, e.g., machineconfiguration.openshift.io/buildRetries=1 . For now, this can be a hard-coded value (e.g., 5), but in the future, this could be wired up to an end-user facing knob. This annotation should be cleared upon a successful rebuild. If the rebuild is reached, then we should degrade.",
                "epic_key": "MCO-828",
                "GITHUB": [
                    {
                        "id": "4694",
                        "type": "pullRequest",
                        "title": "MCO-1165: rebuild annotation",
                        "body": "- What I did I added the ability for a cluster admin to request that the currently active MachineOSBuild be rebuilt. This is done by adding the annotation `machineconfiguration.openshift.io/rebuild` to the MachineOSConfig. - How to verify it 1. Bring up an OpenShift cluster. 2. Opt into on-cluster layering by creating a MachineOSConfig. 3. Wait for the build to complete. 4. Add the annotation `machineconfiguration.openshift.io/rebuild` onto the MachineOSConfig. 5. The controller should delete the current MachineOSBuild and a new one should be started in its place. - Description for the changelog Adds rebuild functionality for OCL"
                    }
                ]
            },
            "MCO-81": {
                "summary": "MCD: emit earlier events to warn about failing drains",
                "description": "In newer versions of OCP, we have changed our draining mechanism to only fail after 1 hour. This also means that the event which captures the failing drain was also moved to the failure at the 1hr mark. Today, upgrade tests oft fail with timeouts related to drain errors (PDB or other). There exists no good way to distinguish what pods are failing and for what reason, so we cannot easily aggregate this data in CI to tackle issues related to PDBs to improve upgrade and CI pass rate. If the MCD, upon a drain run failure, emits the failing pod and reason (PDB, timeout) as an event, it would be easier to write a test to aggregate this data. Context in this thread:",
                "epic_key": "MCO-1298",
                "GITHUB": [
                    {
                        "id": "4726",
                        "type": "pullRequest",
                        "title": "MCO-81: Emit events to warn about failing drains",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did Add a couple of events to indicate drain failure in the MCC. Currently drain failures causes a prom alert, which isn't helpful for data aggregation in CI. - How to verify it On failing drain, in addition to logs, we should also see events emitted. !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "MCO-1646": {
                "summary": "Remove `replace` line in `go.mod` of origin repo",
                "description": "To simultaneously merge the V1 MCN API and updates to the MCN origin tests Once the simultaneous merges are complete and the API is properly bumped in the client-go repo (MCO-1644), the `replace` statements should be removed and the API and client-go versions should point to the merged commits in the openshift/origin repo. Done when: origin PR pointing to the correct API and client-go commits is created and merged",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "29717",
                        "type": "pullRequest",
                        "title": "MCO-1646: API & client-go bumps for MCN V1 API updates",
                        "body": "This bumps the API dependency to & the client-go dependency to Steps taken: - `go get github.com/openshift/api@9b80d67473bc15d85312ffda09dcde489c8e0545` - `go mod tidy` - `go get github.com/openshift/client-go@5f55ff6979a17ab49a12d181aa3c542372d6917d` - `go mod tidy` - `go mod vendor` - `go mod verify`"
                    }
                ]
            },
            "MCO-1590": {
                "summary": "Add explicit opt-out and Status field for ManagedBootImages API",
                "description": "Based on discussion on the enhancement, we have decided that we'd like to add an explicit opt-out option and a status field for the ManagedBootImages knob in the MachineConfiguration object. More context here:",
                "epic_key": "MCO-1361",
                "GITHUB": [
                    {
                        "id": "2223",
                        "type": "pullRequest",
                        "title": "MCO-1590: Add explicit opt-out & status field for boot image update configuration",
                        "body": "This PR adds: - a new \"None\" enum to specifically exclude machine resources from boot image updates - a status field for ManagedBootImages that will reflect the spec.ManagedBootImages. If unspecified, it will display the cluster defaults Scenario: No admin configuration and the current release does not opt-in by default: ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: None ``` Scenario: No admin configuration and the current release does opt-in by default: ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: All ``` Regardless of the default-on behavior of the release, if the admin were to add a configuration, the status must reflect that in the next update. ``` apiVersion: operator.openshift.io/v1 kind: MachineConfiguration spec: managedBootImages: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: Partial partial: machineResourceSelector: matchLabels: {} status: managedBootImagesStatus: machineManagers: - resource: machinesets apiGroup: machine.openshift.io selection: mode: Partial partial: machineResourceSelector: matchLabels: {} ```"
                    }
                ]
            },
            "MCO-1587": {
                "summary": "Add runbook for ExtremelyHighIndividualControlPlaneMemory alert",
                "description": "MCO will send an alert when a node for 45 minutes, when a control plane node has extremely high memory usage The alerts describes the following \"summary: - Extreme memory utilization per node within control plane nodes is extremely high, and could impact responsiveness and stability. description: - The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests especially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.\" It is possible that admin may not be able to interpret exact action to be taken after looking at the alert and the cluster state. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for ExtremelyHighIndividualControlPlaneMemory alert Created runbook link is accessible to cluster admin with ExtremelyHighIndividualControlPlaneMemory alert",
                "epic_key": "MCO-111",
                "GITHUB": [
                    {
                        "id": "4976",
                        "type": "pullRequest",
                        "title": "MCO-1587: Add runbook for ExtremelyHighIndividualControlPlaneMemory",
                        "body": "- What I did Added a runbook I wrote to the ExtremelyHighIndividualControlPlaneMemory Alert - How to verify it Trigger ExtremelyHighIndividualControlPlaneMemory on cluster. Alert should now display associated runbook. - Description for the changelog Added to alert as a runbook_url."
                    }
                ]
            },
            "MCO-1558": {
                "summary": "MOSB & Image Pruning",
                "description": "When user deletes a MOSB, delete the built image associated with it from the image registry.",
                "epic_key": "MCO-1552",
                "GITHUB": [
                    {
                        "id": "4975",
                        "type": "pullRequest",
                        "title": "MCO-1558, OCPBUGS-34745: Add support to rebuild and prune OCL images",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Closes Closes Please provide the following information: -- - What I did Add a check to ensure that we rebuild an image if the MOSB exists but the image was deleted from the registry. Also delete the image from the registry when a MOSB is deleted. If the image is currently applied to the node or desired by a node, then the image will not be deleted from the registry even if the MOSB is deleted. - How to verify it Create a MOSC and then create an MC to create another MOSB. Now delete the image from the first MOSB and then delete the MC, this should cause OCL to move back to the previous MOSB. But since the image doesn't exist for it anymore it will rebuild it. Do the same but where the image exists, for this case it won't rebuild and just reuse the existing image. Delete a MOSB and it should delete the image in the registry as well. Note: the renderedImagePushSecret should have enough permissions to be able to delete images for this to work. - Description for the changelog Add support to rebuild and prune OCL images. !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "MCO-1537": {
                "summary": "Add runbook for MCDRebootError alert",
                "description": "MCC sends drain alert when a node fails to reboot in a span of 5 minutes This is to make sure that admin takes appropriate action if required by looking at the pod logs. Alert contains the information on where to look for the logs. Example alert looks like: Reboot failed on \\ $labels.node , update may be blocked. For more details: oc logs -f -n \\ $labels.namespace \\ $labels.pod -c machine-config-daemon It is possible that admin may not be able to interpret exact action to be taken after looking at MCC pod logs. Adding runbook ( can help admin in better troubleshooting and taking appropriate action. Acceptance Criteria: Runbook doc is created for MCDRebootError alert Created runbook link is accessible to cluster admin with MCDRebootError alert",
                "epic_key": "MCO-111",
                "GITHUB": [
                    {
                        "id": "4895",
                        "type": "pullRequest",
                        "title": "MCO-1537: Add MCDRebootError runbook to prometheus rules",
                        "body": "Please provide the following information: - What I did Added a runbook I wrote to the MCDRebootError Alert - How to verify it Trigger MCDRebootError on cluster. Alert should now display associated runbook. - Description for the changelog Added alert as a runbook_url."
                    }
                ]
            },
            "MCO-1523": {
                "summary": "API 3/4 Create 5 tests in openshift/origin for GA readiness signal",
                "description": "The third step in GAing PinnedImageSets is creating at least five tests in openshift/origin. These tests will act as a GA readiness signal and build confidence in the feature\u2019s functionality. More information on the requirements for the tests can be found here| Done when: At least five tests are defined for PIS The tests meet the criteria specified in the API requirements Requires: MCO-1521 (Create GA PIS API) MCO-1522 (Adapt MCO code to use PIS\u2019s GA API)",
                "epic_key": "MCO-1258",
                "GITHUB": [
                    {
                        "id": "29599",
                        "type": "pullRequest",
                        "title": "MCO-1523: PinnedImageSet v1alpha1 Testing"
                    }
                ]
            },
            "MCO-1520": {
                "summary": "API 5/6 Create 5 tests in openshift/origin for GA readiness signal",
                "description": "The fifth step in GAing the MCN API is creating at least five tests in openshift/origin. These tests will act as a GA readiness signal and build confidence in the feature's functionality. More information on the requirements for the tests can be found here (Custom MCP support in MCN) Can be worked on in parallel to MCO-1519 (Adapt MCO code to use MCN\u2019s GA API)",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "29596",
                        "type": "pullRequest",
                        "title": "MCO-1520: Add MachineConfigNode e2e tests",
                        "body": "Contributors: @isabella-janssen @pablintino @djoshy Work Included - All current MCO team members added to the /machine_config owners file. - Adds tests for MCO's MachineConfigNode feature. - This includes a cherry pick of Notes - All tests should run successfully with a cluster built on `4.19.0-0.nightly-2025-03-21-030708` or later with tech preview enabled. - The tests for MCO-1595( & MCO-1596( will both run in the default serial suite since they can impact other tests through pool creation and updates. - The tests for MCO-1597( & MCO-1598( are both tagged `Slow`, as they often take longer than 5 minutes to run. To have these tests run automatically, we will need to create a suite at a later time (Slack ref( but the tests can still be run locally. - The tests for MCO-1599( will all run in the default parallel suite since they are not computationally expensive, simple, and should have no affects on other tests. MCO-1595( Validate MCN properties ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialShould have MCN properties matching associated node properties apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` MCO-1596( Validate MCN condition status transitions ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialShould properly transition through MCN conditions on node update apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/serial\" ``` MCO-1597( Validate MCN condition status on node degrade ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialSlowShould properly report MCN conditions on node degrade apigroup:machineconfiguration.openshift.io\" ``` MCO-1598( Validate MCN on node creation and deletion Note that this test is skipped for Single Node Openshift. ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes SerialSlowShould properly create and remove MCN on node creation and deletion apigroup:machineconfiguration.openshift.io\" ``` MCO-1599( Validate MCN object can only be accessible from its associated MCD Note that these tests are skipped for Single Node Openshift. ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly block MCN updates by impersonation of the MCD SA apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly block MCN updates from a MCD that is not the associated one apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ``` ``` ./openshift-tests run-test \"sig-mcoOCPFeatureGate:MachineConfigNodes Should properly block MCN updates by impersonation of the MCD SA apigroup:machineconfiguration.openshift.io Suite:openshift/conformance/parallel\" ```"
                    }
                ]
            },
            "MCO-1518": {
                "summary": "API 3/6 Create V1 MCN API",
                "description": "The third step in GAing the MCN API is creating a V1 API for the feature. A related API PR to use as a reference for the expected work can be found here| Done when: V1 API for MCN is finalized The MCN API fields are all properly documented All design decisions are appropriately documented Requires: MCO-1543 (Create finalized v1alpha1 MCN API)",
                "epic_key": "MCO-836",
                "GITHUB": [
                    {
                        "id": "2255",
                        "type": "pullRequest",
                        "title": "MCO-1518: Create V1 MachineConfigNodes API & deploy V1 MCN CRDs",
                        "body": "Merge Plan: This should merge simultaneously with - - Included work: - Copies over the v1alpha1 MCN api into the v1 folder. - Note that the tombstoned fields `Spec.PinnedImageSets` (tombstoned in 2256) and `Status.PinnedImageSets.LastFailedGenerationErrors` (tombstoned in 2201) are not referenced in this new v1 API. - Removes comments suggesting future changes and updates compatibility level. - Validation rule tweaks. References: - (WIP) Updated enhancement: - MCN origin tests:"
                    }
                ]
            },
            "MCO-1515": {
                "summary": "Pick up openshift/kubernetes 1.32 rebase updates",
                "description": "User or Developer story As a MCO developer, I want to pick up the openshift/kubernetes updates for the 1.32 k8s rebase to track the k8s version as rest of the OpenShift 1.32 cluster. Engineering Details Update the go.mod, go.sum and vendor dependencies pointing to the kube 1.32 libraries. This includes all direct kubernetes related libraries as well as openshift/api , openshift/client-go, openshift/library-go and openshift/runtime-utils Acceptance Criteria: All k8s.io related dependencies should be upgraded to 1.32. openshift/api , openshift/client-go, openshift/library-go and openshift/runtime-utils should be upgraded to latest commit from master branch All ci tests must be passing",
                "epic_key": "MCO-1488",
                "GITHUB": [
                    {
                        "id": "4797",
                        "type": "pullRequest",
                        "title": "MCO-1515: Kube bump to 1.32",
                        "body": "Bumped dependencies: ``` go get -u k8s.io/api go get -u k8s.io/code-generator go get -u k8s.io/component-base go get -u k8s.io/kube-proxy go get -u k8s.io/utils go get -u k8s.io/klog go get -u k8s.io/klog/v2 go get -u sigs.k8s.io/controller-runtime go get -u github.com/openshift/build-machinery-go go get -u sigs.k8s.io/controller-tools go get -u github.com/openshift/api go get -u github.com/openshift/client-go go get -u github.com/openshift/library-go go get -u k8s.io/cri-api go get -u k8s.io/kubectl go get -u k8s.io/kubelet make go-deps ``` operator.go: NewKubeRecorder now needs a Passive clock as an argument status.go: SetStatusConditions now needs a Passive clock as an argument Dockerfile: Multi arch builds for 1.32 golang is not yet supported, hence reverting"
                    }
                ]
            },
            "MCO-1470": {
                "summary": "Update helper binaries with latest changes",
                "description": "In we are migrating my helper binaries into the MCO repository. I had to make changes to several of my helpers in the original repository are incorporated into the MCO repository versions of the relevant helper binaries.",
                "epic_key": "MCO-531",
                "GITHUB": [
                    {
                        "id": "4753",
                        "type": "pullRequest",
                        "title": "MCO-1470: Incorporates upstream devex helper changes for OCL testing",
                        "body": "- What I did This takes @dkhater-redhat's work in and incorporates changes I had to make to my helpers repository in order to make the tests pass for This also repackages the helpers slightly into a `devex/` folder in the MCO repo's root and removes a few of the `hack/` scripts that the `devex/` helpers replace. This is intended to land after does. - How to verify it The unit test suite provides some test coverage for the devex helpers. Additionally, the verify target will compile the helpers in CI as a smoke test. - Description for the changelog Incorporates upstream devex helper changes for OCL testing"
                    }
                ]
            },
            "MCO-643": {
                "summary": "Implement a path in the controller to manage user-data secrets",
                "description": "The machinesets in the machine-api namespace reference a user-data secret (per pool and can be customized) which stores the initial ignition stub configuration pointing to the MCS, and the TLS cert. This today doesn't get updated after creation. The MCO now has the ability to manage some fields of the machineset object as part of the managed bootimage work. We should extend that to also sync in the updated user-data secrets for the ignition tls cert. The MCC should be able to parse both install-time-generated machinesets as well as user-created ones, so as to not break compatibility. One way users are using this today is to use a custom secret + machineset to do non-MCO compatible ignition fields, for example, to partition disks for different device types for nodes in the same pool. Extra care should be taken not to break this use case",
                "epic_key": "MCO-1208",
                "GITHUB": [
                    {
                        "id": "4735",
                        "type": "pullRequest",
                        "title": "MCO-643: MCO-645: Reintroduce MCS CA rotation",
                        "body": "This PR re-applies the work done in 4669, which was reverted in 4713. We should only land this after How to verify: In addition to passing existing e2es and verification done in 4669 , agent installations should succeed. The recently added test in the MCO repo which tests agent workflows(`ci/prow/e2e-agent-compact-ipv4`) should give us a good signal on this."
                    }
                ]
            },
            "MCO-466": {
                "summary": "Improve erroring where there is a bootstrap/in-cluster master config mismatch",
                "description": "Today the MCO bootstraps with a bootstrap MCC/MCS to generate and serve master configs. When the in-cluster MCC comes up, it then tries to regen the same MachineConfig via the in-cluster MCs at the time. This often causes a drift and for the install to fail. See and for more context. For the most recent occurrence of this, see: Early on this helped us see differences between bootstrap and in-cluster behaviour more easily, but we do have the bootstrap machineconfig on-disk on the masters. In theory, we should just be able to use that directly and attempt to consolidate the changes. In the case of a drift, instead of failing, we can consider doing an immediate update instead to the latest version.",
                "epic_key": "MCO-1298",
                "GITHUB": [
                    {
                        "id": "4723",
                        "type": "pullRequest",
                        "title": "MCO-466: Improve erroring where there is a bootstrap/in-cluster master config mismatch",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - Added better error collection for this failure mode. The diff between the configs are logged in the daemon and written to disk at `/etc/machine-config-daemon/bootstrapconfigdiff` to aid in debug for an MCO developer. - When the CO is currently degraded due to a `syncRequiredMachineConfigPools` failure, it does not populate the message for that case. I've updated the propagation of the condition messages to facilitate this. This shouldn't need QE testing and existing e2es should be sufficient. !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            },
            "MCO-113": {
                "summary": "Preserve previous boot MCD logs",
                "description": "In OCP 4.7 (?) and before, you were able to see the MCD logs of the previous container post upgrade. Now it seems that we no longer do in newer versions. I am not sure if this is a change in kube pod logging behaviour, how the pod gets shutdown and brought up, or something in the MCO. This however makes it relatively hard to debug in newer versions of the MCO, and in numerous bugs we could not pinpoint the source of the issue since we no longer have necessary logs. We should find a way to properly save the previous boot MCD logs if possible.",
                "epic_key": "MCO-1298",
                "GITHUB": [
                    {
                        "id": "4731",
                        "type": "pullRequest",
                        "title": "MCO-113: Preserve previous boot MCD log",
                        "body": "!-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did The daemon will begin to save its logs right before shutdown at `/etc/machine-config-daemon/previous-logs/`. I'm hoping that we can update the must gather tool so that it is able to scoop up this log file. I also wanted this to be a backportable solution so I tried to keep it out of the critical path, on encountering an error the daemon logs it, but it's not fatal. - How to verify it - Roll out an MC change that causes a node reboot. A cluster upgrade should also trigger this. - On reboot, debug into the node, and you should see the previous daemon logs saved at `/etc/machine-config-daemon`: ``` sh-5.1 ls /etc/machine-config-daemon/previous-logs/openshift-machine-config-operator_machine-config-daemon-qmgx4_a2b53ce6-ac08-4211-b93f-c1286d4e0554/machine-config-daemon/ 0.log 1.log ``` If you're lucky, you might also the see the daemon logs indicate successful save. Since this happens late in the daemon shutdown process, the line does get lost occasionally: ``` 2024-12-04T18:55:32.588709698+00:00 stderr F I1204 18:55:32.588667 12126 daemon.go:2930 Daemon logs from /var/log/pods/openshift-machine-config-operator_machine-config-daemon-qmgx4_a2b53ce6-ac08-4211-b93f-c1286d4e0554 preserved at /etc/machine-config-daemon/previous-logs/openshift-machine-config-operator_machine-config-daemon-qmgx4_a2b53ce6-ac08-4211-b93f-c1286d4e0554 2024-12-04T18:55:32.588804462+00:00 stderr F I1204 18:55:32.588787 12126 daemon.go:1400 Shutting down MachineConfigDaemon ``` !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            }
        },
        "epics": {
            "MCO-836": {
                "summary": "GA Machine Config Node",
                "description": "This epic describes the work required to GA a minimal viable version of the Machine Config Node feature to enable the subsequent GAing of the Pinned Image Sets feature. The GAing of status reporting as well as any further enhancements for the Machine Config Node feature will be tracked in MCO-1506. Related Items: Original MCN/State Reporting Enhancement Pinned Image Sets Enhancement 4.19 backport SBAR| Doc work tracked in OSDOCS-14404 Done when: MCN API is GAed MCN functionality is consistent across all MCPs (default & custom) and both clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing The team is confident that the state of MCN is robust enough to support the GAing of Pinned Image Sets"
            },
            "MCO-111": {
                "summary": "Actionable Error Messaging",
                "description": "The error propagation is generally speaking not 1-to-1. The operator status will generally capture the pool status, but the full error from Controller/Daemon does not fully bubble up to pool/operator, and the journal logs with error generally don\u2019t get bubbled up at all. This is very confusing for customers/admins working with the MCO without full understanding of the MCO\u2019s internal mechanics: The real error is hard to find The error message is often generic and ambiguous The solution/workaround is not clear at all Using \"unexpected on-disk state\" as an example, this can be caused by any amount of the following: An incomplete update happened, and something rebooted the node The node upgrade was successful until rpm-ostree, which failed and atomically rolled back The user modified something manually Another operator modified something manually Some other service/network manager overwrote something MCO writes Etc. etc. Since error use cases are wide and varied, there are many improvements we can perform for each individual error state. This epic aims to propose targeted improvements to error messaging and propagation specifically. The goals being: De-ambigufying different error cases with the same message Adding more error catching, including journal logs and rpm-ostree errors Propagating full error messages further up the stack, up to the operator status in a clear manner Adding actionable fix/information messages alongside the error message With a side objective of observability, including reporting all the way to the operator status items such as: Reporting the status of all pools Pointing out current status of update/upgrade per pool What the update/upgrade is blocking on How to unblock the upgrade Approaches can include: Better error messaging starting with common error cases De-ambigufying config mismatch Capturing rpm-ostree logs from previous boot, in case of osimageurl mismatch errors Capturing full daemon error message back to pool/operator status Adding a new field to the MCO operator spec, that attempts to suggest fixes or where to look next, when an error occurs Adding better alerting messages for MCO errors Options"
            },
            "MCO-1361": {
                "summary": "Opt-out updated bootimage for GCP and AWS",
                "description": "This epic will encompass work required to switch boot image updates on GCP to be opt-out."
            },
            "MCO-1234": {
                "summary": "Bump ignition to spec 3.5",
                "description": "Once ignition spec 3.5 stablizes, we should switch to using spec 3.5 as the default in the MCO to enable additional features in RHCOS. (example: needs 3.5)"
            },
            "MCO-1258": {
                "summary": "GA Pin and pre-load images",
                "description": "This epic describes the work required to GA the Pinned Image Sets feature. Related Documentation: Pinned Image Sets Enhancement Design Review Document Comment with helpful info on how to use PIS| Done when: Pinned Image Set API is GAed (here is the API in tech preview: ) Pinned Image Set functionality is consistent for clusters with and without OCL enabled Tests are created, encompassing of major functionality, and passing e2e testing: create PIS for custom pool and run garbage collection and check if PIS remain on node add PIS to custom pool and check if they have been successfully added add PIS to standard pool and check if they have been successfully added add invalid PIS and check if MCN has degraded in standard pool add invalid PIS and check if MCN has degraded in custom pool"
            },
            "MCO-1507": {
                "summary": "On Cluster Layering Disconnected Support",
                "description": "Post GA On Cluster Build Enhancement work"
            },
            "MCO-1208": {
                "summary": "Manage the MCS ignition-ca cert",
                "description": "Spun out of This aims to capture the work required to rotate the MCS-ignition CA + cert. Original description copied from MCO-668: Today in OCP there is a TLS certificate generated by the installer | which is called \"root-ca\" but is really \"the MCS CA\". A key derived from this is injected into the pointer Ignition configuration under the \"security.tls.certificateAuthorities\" section, and this is how the client verifies it's talking to the expected server. If this key expires (and by default the CA has a 10 year lifetime), newly scaled up nodes will fail in Ignition (and fail to join the cluster). The MCO should take over management of this cert, and the corresponding user-data secret field, to implement rotation. Reading: - There is a section in the customer facing documentation that touches on this: - There's a section in the customer facing documentation for this: that needs updating for clarification. - There's a pending PR to openshift/api: - Also see old (related) bug: - This is also separate to which describes the management of kubelet certs"
            },
            "MCO-1298": {
                "summary": "Tech debt 4.18",
                "description": "These are items that the team has prioritized to address in 4.18."
            },
            "MCO-1316": {
                "summary": "On-Cluster Layering - upgrades and integrations",
                "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
            },
            "MCO-828": {
                "summary": "On-Cluster Layering GA",
                "description": "This work describes the tech preview state of On Cluster Builds. Major interfaces should be agreed upon at the end of this state. As a cluster admin of user provided infrastructure, when I apply the machine config that opts a pool into On Cluster Layering, I want to also be able to remove that config and have the pool revert back to its non-layered state with the previously applied config. As a cluster admin using on cluster layering, when an image build has failed, I want it to retry 3 times automatically without my intervention and show me where to find the log of the failure. As a cluster admin, when I enable On Cluster Layering, I want to know that the builder image I am building with is stable and will not change unless I change it so that I keep the same API promises as we do elsewhere in the platform. To test: As a cluster admin using on cluster layering, when I try to upgrade my cluster and the Cluster Version Operator is not available, I want the upgrade operation to be blocked. As a cluster admin, when I use a disconnected environment, I want to still be able to use On Cluster Layering. As a cluster admin using On Cluster layering, When there has been config drift of any sort that degrades a node and I have resolved the issue, I want to it to resync without forcing a reboot. As a cluster admin using on cluster layering, when a pool is using on cluster layering and references an internal registry I want that registry available on the host network so that the pool can successfully scale up (MCO-770, MCO-578, MCO-574 ) As a cluster admin using on cluster layering, when a pool is using on cluster layering and I want to scale up nodes, the nodes should have the same config as the other nodes in the pool. Maybe: Entitlements: MCO-1097, MCO-1099 Not Likely: As a cluster admin using on cluster layering, when I try to upgrade my cluster, I want the upgrade operation to succeed at the same rate as non-OCL upgrades do."
            },
            "MCO-1552": {
                "summary": "On Cluster Layering: Address Image Pruning",
                "description": "Done When: We implement a solution that mitigates the node disruptions when applying Machine Configs. Notes: MVP: when a MachineOSBuild object is deleted, the corresponding image in the registry should also be deleted, whether internal or external Mid term: have good documentation around user-driven prunes, and what can/cannot be pruned Mid term: have an opt-in/opt-out mechanism for auto pruning, where builds not in use and x versions old are deleted automatically Nice to have: tag latest (and maybe latest-1) builds for pools, so the user can easily refer to the latest build and know what not to prune Nice to have: have user-pruned images automatically reflect back on the MachineOSBuild Long term: user defined prune logic"
            },
            "MCO-1488": {
                "summary": "Update MCO dependencies to Kubernetes 1.32",
                "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that MCO uses to v1.29 which will keep it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? Uncover any possible issues with the openshift/kubernetes rebase before it merges. MCO continues using the latest kubernetes/OpenShift libraries and the kubelet, kube-proxy components. MCO e2e CI jobs pass on each of the supported platform with the updated components. Acceptance Criteria All stories in this epic must be completed. Go version is upgraded for MCO components. CI is running successfully with the upgraded components against the 4.18/master branch. Dependencies (internal and external) ART team creating the go 1.31 image for upgrade to go 1.31. OpenShift/kubernetes repository downstream rebase PR merge. Open questions: Do we need a checklist for future upgrades as an outcome of this epic?- yes, updated below. Done Checklist Step 1 - Upgrade go version to match rest of the OpenShift and Kubernetes upgraded components. Step 2 - Upgrade Kubernetes client and controller-runtime dependencies (can be done in parallel with step 3) Step 3 - Upgrade OpenShift client and API dependencies Step 4 - Update kubelet and kube-proxy submodules in MCO repository Step 5 - CI is running successfully with the upgraded components and libraries against the master branch."
            },
            "MCO-531": {
                "summary": "General \"tech debt\" items that don't have a home yet",
                "description": "Background This is intended to be a place to capture general \"tech debt\" items so they don't get lost. I very much doubt that this will ever get completed as a feature, but that's okay, the desire is more that stores get pulled out of here and put with feature work \"opportunistically\" when it makes sense. Goal If you find a \"tech debt\" item, and it doesn't have an obvious home with something else (e.g. with MCO-1 if it's metrics and alerting) then put it here, and we can start splitting these out/marrying them up with other epics when it makes sense."
            }
        }
    },
    "OpenShift Image Registry": {
        "stories": {
            "IR-522": {
                "summary": "Allow registry to run in new regions without code changes",
                "description": "We want to allow the image registry to run in new AWS regions without requiring a manual intervention in the code every time a new region pops up. As we can see here| every time a new region is added we need to manually add it to the list of known regions as well. This is required because the upstream project uses an AWS client that isn't receiving these new regions automatically.",
                "epic_key": "IR-513",
                "GITHUB": [
                    {
                        "id": "425",
                        "type": "pullRequest",
                        "title": "IR-522: disable aws region check",
                        "body": "disable the artificial aws region check. we are doing this so we can support new aws regions without requiring patching the code every time. this is a test and should not land on main."
                    }
                ]
            }
        },
        "epics": {
            "IR-513": {
                "summary": "Support for new AWS regions without code changes",
                "description": "Epic Goal Streamline onboarding of new AWS regions in OCP managed services like ROSA or OSD by removing the need for code changes in the cluster image registry operator Why is this important? Every time AWS is launching a new region the deployment of OpenShift will fail with the Cluster Image Registry Operator failing to reconcile until the newly added region is hard coded into the operator's controller This blocks installer and QE teams from development, testing and rollout of new region support for OCP managed services on AWS Disabling the integrated registry is currently not supported by many of our managed services Acceptance Criteria A new AWS region should not require any code changes to CIRO for the operator to reconcile the request for required cloud infrastructure to run the image registry Backports to all currently supported OCP versions Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Hosted Control Plane": {
        "stories": {
            "HOSTEDCP-2262": {
                "summary": "Prototype having our own class instead of VAP for shared ownership",
                "description": "While VAP is ok for implementing shared ownership it has some drawbacks. e.g. it forces us to change the builtin CEL rules of the API for required fields which is a maintenance burden and error prone. Besides it doesn't gives control to future api pivots we might need to execute to satisfy business needs. E.g. expose a field for dual stream support which requires picking a different ami, e.g expose a field for kubeletconfig that let us include that in the payload generation... we should be ready to pivot to have our own class which exposes only a subset of the upstream and have a controller which just renders the upstream one",
                "epic_key": "CNTRLPLANE-414",
                "GITHUB": [
                    {
                        "id": "5600",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2262: Karpenter openshift ec2class",
                        "body": "What this PR does / why we need it: While VAP is ok for implementing shared ownership it has some drawbacks. e.g. it forces us to change the builtin CEL rules of the API for required fields which is a maintenance burden and error prone. Besides it doesn't gives control to future api pivots we might need to execute to satisfy business needs. E.g. expose a field for dual stream support which requires picking a different ami, e.g expose a field for kubeletconfig that let us include that in the payload generation. This PR introduces a new CRD `OpenshiftEC2NodeClass` to be the main interaction point for users with Karpenter instead of directly creating/modifying `EC2NodeClass` objects. Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2257": {
                "summary": "Vendor karpenter CRDs",
                "description": "User Story: As a (user persona), I want to be able to: create/update Karpenter resources directly without dealing with unstructured types so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Karpenter CRDs are vendored in Hypershift code Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CNTRLPLANE-414",
                "GITHUB": [
                    {
                        "id": "5522",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2257: Vendor karpenter CRDs",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2256": {
                "summary": "CPO Refactor: Components should be deleted when predicate changes to false",
                "description": "User Story: As a (user persona), I want to be able to: As an external dev I want to be able to add new components to the CPO easily As a core dev I want to feel safe when adding new components to the CPO As a core dev I want to add new components to the CPO with our copy/pasting big chunks of code introduced a new abstraction to be used by ControlPlane components. However, when a component or a sub-resources predicate changes to false, the resources are not removed from the cluster. All resources should be deleted from the cluster. docs:",
                "epic_key": "CNTRLPLANE-308",
                "GITHUB": [
                    {
                        "id": "5509",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2256: CPO Refactor Delete component and its resources when predicate changes to false",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2255": {
                "summary": "E2E coverage for custom tolerations",
                "description": "QE has testing for this which detected OCPBUGS-43357 but we should make our own test and verify this in our e2e as well",
                "GITHUB": [
                    {
                        "id": "5543",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2255: e2e: ensure custom tolerations on all HCP pods"
                    }
                ]
            },
            "HOSTEDCP-2234": {
                "summary": "Implement automated machine approval for karpenter instances",
                "description": "User Story: As a (user persona), I want to be able to: Instances created by karpenter can automatically become Nodes so that I can achieve Reduce operational burden. Acceptance Criteria: Description of criteria: For CAPI/MAPI driven machine management the cluster-machine-approver uses the machine.status.ips to match the CSRs. In karpenter there's no Machine resources We'll need to implement something similar. Some ideas: -- Explore using the nodeClaim resource info like status.providerID to match the CSRs -- Store the requesting IP when the ec2 instances query ignition and follow similar comparison criteria than machine approver to match CSRs -- Query AWS to get info and compare info to match CSRs (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "HOSTEDCP-2222",
                "GITHUB": [
                    {
                        "id": "5349",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2234: Karpenter auto machine approver",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2231": {
                "summary": "Replace Deprecated Cloud Connection with IBM Transit Gateway",
                "description": "User Story: As a (user persona), I want to be able to: Successfully create a PowerVS Hypershift cluster. The connection between VPC and PowerVS Instance should be through Transit Gateway. All cloud connection related checks should no longer be present. so that I can achieve Successfully transitioned from the deprecated IBM Cloud Connection to the IBM Transit Gateway, ensuring continued compatibility and improved network connectivity in the codebase. Acceptance Criteria: Description of criteria: Update the documentation replacing cloud connection with transit gateway. Do, mention the past usage of cloud connection and the reason of the update. Identify and refactor code dependencies on the deprecated IBM Cloud Connection. Ensure seamless integration with IBM Transit Gateway by conducting end-to-end testing. Collaborate with stakeholders to confirm compliance and resolve any issues during implementation.",
                "epic_key": "CNTRLPLANE-425",
                "GITHUB": [
                    {
                        "id": "5293",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2231: Remove cloud connection, Set Transit Gateway as default",
                        "body": "!-- - Please ensure code changes are split into a series of logically independent commits. - Every commit should have a subject/title (What) and a description/body (Why). - Every PR must have a description. - As an example you can use git commit -m\"What\" -m\"Why\" to achieve the requirements above. GitHub automatically recognises the commit description (-m\"Why\") in single commit PRs and adds it as the PR description. - Use the imperative mood( in the subject line for every commit. E.g `Mark infraID as required` instead of `This patch marks infraID as required` (This follows Git\u2019s own built-in conventions). See as an example. - See for more details. Delete this text before submitting the PR. -- What this PR does / why we need it: This PR updates the PowerVS documentation and code to reflect Transit Gateway as the default networking mode. IBM Cloud has removed support for cloud connection, necessitating this change. Removed references to cloud connection. Updated hypershift powervs commands to use Transit Gateway as the default networking mode. Revised the documentation to provide clear guidance on Transit Gateway usage. Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes 5062 Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - x This change includes docs. - x This PR was tested locally."
                    }
                ]
            },
            "HOSTEDCP-2120": {
                "summary": "e2e testing automation: Add a Mechanism to Label all Pods in the Control Plane Namespace",
                "epic_key": "HOSTEDCP-2004",
                "GITHUB": [
                    {
                        "id": "5549",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2120: Enable EnsureCustomLabels e2e test",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    },
                    {
                        "id": "5511",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2120: Enable EnsureCustomLabels e2e test",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-1971": {
                "summary": "test to capture HO updates causing nodePool rollouts",
                "description": "test to capture HO updates causing nodePool reboots",
                "epic_key": "CNTRLPLANE-398",
                "GITHUB": [
                    {
                        "id": "4999",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-1971: HyperShift operator upgrade test for rollout validation",
                        "body": "What this PR does / why we need it: This PR adds an upgrade test to validate that a HO upgrade won't cause any unnecessary node rollout. 1. The setup stage creates a HostedCluster and installs an initial HO image. 2. The test stage upgrades the HyprShift Operator to the version under test and validates that the upgrade did not cause a node rollout. Which issue(s) this PR fixes : Fixes HOSTEDCP-1971( How to test : 1. Build the e2e binary ```shell make e2e ``` 2. With your kubeconfig pointing to a fresh cluster run the upgrade-test stage ```shell ./bin/test-e2e -test.run ^TestUpgradeHyperShift$ -test.v -test.timeout 0 \\ --upgrade.run-tests \\ --e2e.hypershift-operator-initial-image \"quay.io/hypershift/hypershift-operator:initial\" --e2e.aws-credentials-file \"${AWS_CREDS}\" \\ --e2e.pull-secret-file \"${PULL_SECRET}\" \\ --e2e.aws-region \"${REGION}\" \\ --e2e.availability-zones \"${REGION}a,${REGION}b,${REGION}c\" \\ --e2e.aws-oidc-s3-bucket-name \"${BUCKET_NAME}\" \\ --e2e.base-domain \"${BASE_DOMAIN}\" \\ --e2e.aws-oidc-s3-credentials ${AWS_CREDS}\\ --e2e.external-dns-credentials ${AWS_CREDS}\\ --e2e.aws-private-credentials-file ${AWS_CREDS}\\ ``` Here's a successful upgrade test run !asciicast( Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - x This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2249": {
                "summary": "Move from karpenter nodepool to programatic generate userdata",
                "description": "Move from karpenter nodepool to programatic generate userdata",
                "epic_key": "CNTRLPLANE-414",
                "GITHUB": [
                    {
                        "id": "5439",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2249: Reconcile karpenter user-data secret programmatically instead of creating a mock nodePool",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2237": {
                "summary": "Implement auto approval for serving CSRs for Karpenter",
                "description": "User Story: As a (user persona), I want to be able to: Instances created by karpenter can automatically become Nodes so that I can achieve Reduce operational burden. Acceptance Criteria: Description of criteria: introduced a new controller to implement auto approval for kubelet client CSRs. We need to extend to also approve serving CSRs since they are not auto approved by the cluster-machine-approver Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "HOSTEDCP-2222",
                "GITHUB": [
                    {
                        "id": "5708",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2237: Auto approve Karpenter serving CSRs",
                        "body": "What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2075": {
                "summary": "Add missing API validation and docs for HostedCluster",
                "description": "User Story: As a (user persona), I want to be able to: Capability 1 Capability 2 Capability 3 so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CNTRLPLANE-424",
                "GITHUB": [
                    {
                        "id": "5171",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2075: Dns cel e2e",
                        "body": "!-- - Please ensure code changes are split into a series of logically independent commits. - Every commit should have a subject/title (What) and a description/body (Why). - Every PR must have a description. - As an example you can use git commit -m\"What\" -m\"Why\" to achieve the requirements above. GitHub automatically recognises the commit description (-m\"Why\") in single commit PRs and adds it as the PR description. - Use the imperative mood( in the subject line for every commit. E.g `Mark infraID as required` instead of `This patch marks infraID as required` (This follows Git\u2019s own built-in conventions). See as an example. - See for more details. Delete this text before submitting the PR. -- What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    },
                    {
                        "id": "5172",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2075: Nodeport address cel",
                        "body": "!-- - Please ensure code changes are split into a series of logically independent commits. - Every commit should have a subject/title (What) and a description/body (Why). - Every PR must have a description. - As an example you can use git commit -m\"What\" -m\"Why\" to achieve the requirements above. GitHub automatically recognises the commit description (-m\"Why\") in single commit PRs and adds it as the PR description. - Use the imperative mood( in the subject line for every commit. E.g `Mark infraID as required` instead of `This patch marks infraID as required` (This follows Git\u2019s own built-in conventions). See as an example. - See for more details. Delete this text before submitting the PR. -- What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            }
        },
        "epics": {
            "HOSTEDCP-2222": {
                "summary": "Implement automated machine approval for karpenter instances",
                "description": "Goal Instances created by karpenter can automatically become Nodes Why is this important? Reduce operational burden. Scenarios For CAPI/MAPI driven machine management the cluster-machine-approver uses the machine.status.ips to match the CSRs. In karpenter there's no Machine resources We'll need to implement something similar. Some ideas: - Explore using the nodeClaim resource info like status.providerID to match the CSRs - Store the requesting IP when the ec2 instances query ignition and follow similar comparison criteria than machine approver to match CSRs - Query AWS to get info and compare info to match CSRs ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "HOSTEDCP-2226": {
                "summary": "Implement shared ownership of karpenter CRs in guest cluster",
                "description": "Goal As a service provider I the cluster admin to only manipulate fields of the nodeclass API that won't impact the service ability to operate, e.g. userdata and ami can't be changed. As a service provider I want to be the solely authoritative source of truth to set input that impacts the ability to operate AutoNode. Why is this important? The way we implement this will have UX implications for cluster admin which has direct impact on customer satisfaction. Scenarios We decided to start by using validating admission policies to implement ownership of ec2NodeClass fields. So we can restrict fields crud to a particular service account. This has some caveats: - If a field that the service own is required in the API, we need to let the cluster admin to set it on creation even though we'll clobber it by reconciling a controller. To mitigate this we might want to change the upstream CEL validations of the ec2NodeClass API - The raw userdata is exposed to the cluster admin via ec2NodeClass.spec.userdata - Since we enforce the values for userdata and ami via controller reconciliation there's potential room for race conditions If using validating policies for this proves to be satisfactory we'll need to consider alternatives, e.g: - Having an additional dedicated CRD for openshiftnodeclass that translates into the ec2NodeClass and completely prevent the cluster admin from interacting with the latter via vap. - having our own class similar to eks so we can fully manage the operational input in the backend. ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "5395",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2226: Add ValidatingAdmissionPolicy for karpenter EC2NodeClass CRD",
                        "body": "What this PR does / why we need it: - Add ValidatingAdmissionPolicy for karpenter EC2NodeClass CRD to prevent updates to managed fields - Relax CEL validations for EC2NodeClass CRD - Default subnetSelectorTerms and securityGroupSelectorTerms if not set Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "HOSTEDCP-2004": {
                "summary": "Add a Mechanism to Label all Pods in the Control Plane Namespace",
                "description": "Goal Hypershift has a mechanism for Labeling Control Plane Pods Cluster service should be able to set the label for a given hosted cluster Why is this important? As part of being a first party Azure offering, ARO HCP needs to adhere to Microsoft secure supply chain software requirements. In order to do this, we require setting a label on all pods that run in the hosted cluster namespace. See Documentation: Scenarios Given a subscriptionID of \"1d3378d3-5a3f-4712-85a1-2485495dfc4b\", there needs to be the following label on all pods hosted on behalf of the customer: {code:yaml} kubernetes.azure.com/managedby: sub_1d3378d3-5a3f-4712-85a1-2485495dfc4b{code} Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "HOSTEDCP-2220": {
                "summary": "Build and merge a HCP + Karpenter feature gated prototype",
                "description": "Goal Codify and enable usage of a prototype for HCP working with karpetner management side. Why is this important? A first usable version is critical to democratize knowledge and develop internal feedback. Acceptance Criteria Deploying a cluster with --auto-node results in karpenter running management side, the CRDs and a default ec2NodeClass installed within the guest cluster ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "5279",
                        "type": "pullRequest",
                        "title": "HOSTEDCP-2220: Autonode karpenter",
                        "body": "What this PR does / why we need it: This introduces support for an autoNode feature via karpenter: - It introduces a self contained karpenter-operator that is built within the HO image, so its lifecycle is fully decouple from the CPO/OCP version. - Let the HO reconcile the karpenter-operator within the controlplane namespace - The karpenter operator manages karpenter deployment management side and manages CRDs, default nodeclass, and clobber \"service owned\" fields guest side. - Introduces a --auto-node flag for the CLI which also causes a new arn generation. - Expose autoNode with karpenter behind a feature gate within the HC API Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            }
        }
    },
    "OpenShift Etcd": {
        "stories": {
            "ETCD-717": {
                "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.19",
                "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.19",
                "epic_key": "ETCD-715",
                "GITHUB": [
                    {
                        "id": "315",
                        "type": "pullRequest",
                        "title": "ETCD-717: Rebase etcd 3.5.19 openshift 4.19",
                        "body": "This PR rebases etcd 3.5.19 into openshift-4.19. running `make` locally ``` SUCCESS: etcd_build (GOARCH=arm64) ./bin/etcd --version etcd Version: 3.5.19 Git SHA: 7ef93eff0 Go Version: go1.23.7 Go OS/Arch: darwin/arm64 ./bin/etcdctl version etcdctl version: 3.5.19 API version: 3.5 ./bin/etcdutl version etcdutl version: 3.5.19 API version: 3.5 ``` cc @openshift/openshift-team-etcd @sdodson"
                    }
                ]
            },
            "ETCD-709": {
                "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.18",
                "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.18",
                "epic_key": "ETCD-707",
                "GITHUB": [
                    {
                        "id": "308",
                        "type": "pullRequest",
                        "title": "ETCD-709: Rebase etcd 3.5.18 openshift 4.19",
                        "body": "This PR rebases etcd 3.5.18 into openshift-4.19. running `make` locally ``` SUCCESS: etcd_build (GOARCH=arm64) ./bin/etcd --version etcd Version: 3.5.18 Git SHA: 88bb55f64 Go Version: go1.22.11 Go OS/Arch: darwin/arm64 ./bin/etcdctl version etcdctl version: 3.5.18 API version: 3.5 ./bin/etcdutl version etcdutl version: 3.5.18 API version: 3.5 ``` cc @openshift/openshift-team-etcd"
                    }
                ]
            },
            "ETCD-726": {
                "summary": "Rebase openshift/etcd 4.19 to upstream etcd 3.5.21",
                "description": "Rebase openshift/etcd release-4.19 to upstream etcd 3.5.21",
                "epic_key": "ETCD-725",
                "GITHUB": [
                    {
                        "id": "324",
                        "type": "pullRequest",
                        "title": "ETCD-726: Rebase etcd 3.5.21 openshift 4.19",
                        "body": "This PR rebases etcd 3.5.21 into openshift-4.19. running `make` locally ``` SUCCESS: etcd_build (GOARCH=arm64) ./bin/etcd --version etcd Version: 3.5.21 Git SHA: 6e97e912b Go Version: go1.23.7 Go OS/Arch: darwin/arm64 ./bin/etcdctl version etcdctl version: 3.5.21 API version: 3.5 ./bin/etcdutl version etcdutl version: 3.5.21 API version: 3.5 ``` cc @openshift/openshift-team-etcd @sdodson"
                    }
                ]
            },
            "ETCD-677": {
                "summary": "Configure removal of etcd container from static pod manifest",
                "description": "For TNF we need to replace our currently running etcd after the installation with the one that is managed by pacemaker. This allows us to keep the following benefits: installation can stay the way it is upgrade paths are not blocked we can keep existing operator functionality around (cert rotation, defrags etc) pacemaker can control etcd without any other interference AC: on a (later specified) signal, we roll out a static pod revision without the etcd container",
                "epic_key": "ECOPROJECT-2053",
                "GITHUB": [
                    {
                        "id": "1352",
                        "type": "pullRequest",
                        "title": "ETCD-677: add unsupported config override for etcd container removal",
                        "body": "/hold for testing purposes in 2NO"
                    }
                ]
            }
        },
        "epics": {
            "ETCD-715": {
                "summary": "Rebase openshift/etcd to 3.5.19",
                "description": "rebase etcd to 3.5.19"
            },
            "ETCD-707": {
                "summary": "Rebase openshift/etcd to 3.5.18",
                "description": "rebase etcd to 3.5.18"
            },
            "ETCD-725": {
                "summary": "Rebase openshift/etcd to 3.5.21",
                "description": "rebase etcd to 3.5.21"
            }
        }
    },
    "OpenShift Installer": {
        "stories": {
            "CORS-3937": {
                "summary": "Static validations",
                "description": "system-assigned identity cannot be set on compute nodes user-assigned identity on compute nodes must == 1 assigned identity must match type assigned identity name is a valid guid",
                "epic_key": "CORS-3883"
            },
            "CORS-3919": {
                "summary": "Update Cluster Infra Manifest",
                "description": "User Story: As a (user persona), I want to be able to: Pass the custom endpoints to all cluster components Capability 2 Capability 3 so that I can achieve All cluster components should use the same api endpoints. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Fill out the API infra config with the custom endpoints when the user has supplied them via the install-config Bring in the API changes as a vendor update. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9518",
                        "type": "pullRequest",
                        "title": "CORS-3919: Add GCP service endpoints to the infrastructure manifest",
                        "body": "The manifest will contain the service endpoints originally in the install config. Validation for these service endpoint additions to the manifests will occur."
                    }
                ]
            },
            "CORS-3918": {
                "summary": "Pass custom endpoints to CAPG",
                "description": "User Story: As a (user persona), I want to be able to: Pass the custom endpoints to CAPG Capability 2 Capability 3 so that I can achieve Use the custom endpoints upstream Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Update the CAPG version that the installer uses Update the CRD for CAPG Pass the endpoints (when user entered the applicable ones) to CAPG through the Cluster Spec This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9528",
                        "type": "pullRequest",
                        "title": "CORS-3918: Send service endpoint overrides to CAPG through cluster spec",
                        "body": "Vendoring updates for CAPG upgrades to latest commit. Vendor updates for CAPG provider in cluster-api providers Match the required set in the CAPG go.mod file to the CAPG required section in their go.mod. This is due to some issues with the CEL library. Without the update the CEL library causes the build to fail on the previous locked version. Generated and added the infrastructure components yaml (CRD) for CAPG. Set the service endpoints in the CAPG cluster spec. Setting the values here will trigger CAPG to use overridden endpoints when creating the clients for gcp APIs."
                    }
                ]
            },
            "CORS-3917": {
                "summary": "Validate custom endpoints",
                "description": "User Story: As a (user persona), I want to be able to: Ensure that the custom endpoints are valid before use. Reach the endpoints provided. Capability 3 so that I can achieve Ensuring that the custom endpoint connectivity are not the reason for any installation issues. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9517",
                        "type": "pullRequest",
                        "title": "CORS-3917: Add install config validation for service endpoints",
                        "body": "check whether the user provided endpoints are reachable or not."
                    }
                ]
            },
            "CORS-3908": {
                "summary": "Use custom endpoints in cloud-provider-gcp",
                "description": "User Story: As a (user persona), I want to be able to: Use custom endpoints through out all cluster components Capability 2 Capability 3 so that I can achieve Using the same api endpoints through the cluster Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: {code:java} providers/gce/gce_fake.go: service, err := compute.NewService(context.Background(), option.WithoutAuthentication()) providers/gce/gce.go: service, err := compute.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: serviceBeta, err := computebeta.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: serviceAlpha, err := computealpha.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) providers/gce/gce.go: containerService, err := container.NewService(context.Background(), option.WithTokenSource(config.TokenSource)) {code} Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9600",
                        "type": "pullRequest",
                        "title": "CORS-3908: Update gcp cloud provider config map to include new endpoints",
                        "body": "manifests/gcp/cloudproviderconfig: The GCP cloud provider config will now be supplied with the api-endpoint and container-api-endpoint. The GCP Cloud provider (and upstream) can accept a config with the api-endpoint (compute) and container-api-endpoint (container) values to override the endpoints for the services. See and"
                    }
                ]
            },
            "CORS-3906": {
                "summary": "Use custom endpoints in MAPI for GCP",
                "description": "User Story: As a (user persona), I want to be able to: Use the custom endpoints in MAPI that were set in the installer. Override the endpoints for: compute tagging so that I can achieve Using the same custom endpoints for all of the services in the cluster. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: The services with endpoints to be overwritten can be found: pkg/cloud/gcp/actuators/services Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "111",
                        "type": "pullRequest",
                        "title": "CORS-3906: Update MAPI GCP Provider to use custom GCP endpoints",
                        "body": "Vendor update. Update openshift/api to use a version that contains the custom endpoint information. When creating the compute service, determine if there is a custom endpoint set in the infrastructure platform status. When creating the tagging service, determine fi there is a custom endpoint set in the infrastructure platform status. Add a function template to pass to the actuator and machine scope parameters for looking up the custom endpoints. During tests a Mock function is passed that will do nothing and return no error. During the actual execution the real lookup function is passed. When no function is set the default will be to use the function that will look through the infrastructure to find the custom endpoints. Update machine scope tests to set a mock function. Update actuator tests to set a mock function"
                    }
                ]
            },
            "CORS-3873": {
                "summary": "Place ingress LBs on specific subnets",
                "description": "User Story: The installer will apply the specified IngressControllerLB subnets to the default IngressController's spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.classicLoadBalancer.subnets or ...networkLoadBalancer.subnets field (based on platform.aws.lbType) in the manifest generated by the generateDefaultIngressController function. Acceptance Criteria: Description of criteria: Ingress LB is placed on specific subnet(s) as specified by user (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: For sanity checking, you can run openshift-install create manifests and check the cluster-ingress-.yaml manifests If no roles are specified in the installconfig, no subnets are supplied in the manifest (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440"
            },
            "CORS-3872": {
                "summary": "Place machines only in clusternode subnets (and bootstrap node in bootstrap subnet)",
                "description": "User Story: Machines should only be placed in subnets specified as clusternode. Bootstrap node should be on subnet with bootstrap node role. Acceptance Criteria: Description of criteria: See above (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: This needs to be specified in both the CAPI-created machines Better link for MAPI machines: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440"
            },
            "CORS-3869": {
                "summary": "Static validations (pkg/types)",
                "description": "User Story: Static validations (no API connection required) Acceptance Criteria: Description of criteria: -The subnet IDs must be valid:- -Start with subnet-- -Length is exactly 24 characters- The subnet IDs must not include duplicates Maximum of 10 IngressController Subnets Validation For a subnet that has defined roles Roles must be of supported types (i.e. from a set of defined roles) Roles must not be duplicate. This and check naturally validates that a subnet can only have max 5 roles EdgeNode cannot be combined with any other roles ClusterNode, IngressControllerLB, ControlPlaneExternalLB (if cluster is external), and ControlPlaneInternalLB must be assigned to at least 1 subnet A subnet cannot have both role ControlPlaneExternalLB and ControlPlaneInternalLB If the cluster is internal, ControlPlaneExternalLB must not be assigned to any subnets.| Some validations are extracted from API validation (i.e. the intstaller does not handle CEL at this time) and Xvalidation markers (i.e. defined in the enhancement proposal). (optional) Out of Scope: Validations that require access to the AWS API will go in pkg/asset (different card) Engineering Details: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440",
                "GITHUB": [
                    {
                        "id": "9505",
                        "type": "pullRequest",
                        "title": "CORS-3869: static validation for new vpc.subnets field",
                        "body": "This include static validations for the vpc.subnets field (no call to AWS API is performed). The validation criteria can be found at jira ticket 0 and enhancement proposal 1. Summary of validation criteria: - The subnet IDs must not include duplicates - There can only be maximum 10 subnets assigned IngressControllerLB role. - Either all subnets must have roles assigned or none at all. - For a subnet that has defined roles - Roles must be of supported types (i.e. from a set of defined roles) - Roles must not be duplicate. This and check naturally validates that a subnet can only have max 5 roles - EdgeNode cannot be combined with any other roles - ClusterNode, IngressControllerLB, ControlPlaneExternalLB (if cluster is external), and ControlPlaneInternalLB must be assigned to at least 1 subnet - A subnet cannot have both role ControlPlaneExternalLB and ControlPlaneInternalLB - If the cluster is internal, ControlPlaneExternalLB must not be assigned to any subnets. References: 0 1"
                    }
                ]
            },
            "CORS-3868": {
                "summary": "Deprecate installconfig.platform.aws.subnets",
                "description": "User Story: As an openshift-install user I want to be able to continue to use aws.subnets during deprecation (a warning will show) As an openshift developer, I only want a single code path for subnets (via upconversion) Acceptance Criteria: Description of criteria: Validation that both fields are not simultaneously specified When aws.subnets is specified, it's upconverted into aws.vpc.subnets Existing pkg/types/aws/Subnets type is renamed to DeprecatedSubnets Remove all (or as many possible) usages of DeprecatedSubnets, replaced with the new vpc.Subnets field We may need to keep usage of DeprecatedSubnets for certain validations Warning when using deprecated field (optional) Out of Scope: . Engineering Details: Conversion package: Review how subnets are used in and whether any changes/refactoring is needed",
                "epic_key": "CORS-3440"
            },
            "CORS-3864": {
                "summary": "Integrate CAPZ changes",
                "description": "Once CAPZ changes are integrated into upstream or our fork, we need to vendor those to the installer.",
                "epic_key": "CORS-3272"
            },
            "CORS-3861": {
                "summary": "Refactor resource group creation/reconciliation to be handled by CAPZ",
                "description": "Currently RG creation is handled by the installer SDK. It can and should be handled by CAPZ so that we have less code to maintain and do not need to handle separate configurations for Azure & Azure Stack",
                "epic_key": "CORS-3272"
            },
            "CORS-3855": {
                "summary": "Hybrid SRE: Remove ARO build-flag in openshift-installer",
                "description": "User Story: As a openshift developer, I want to be able to: Find all the places that the IsARO() function is used and either remove or rework the code so that it is no longer used. Figure out how ARO is using managed identities with IsARO() and rework the code so that it is not ARO specific. so that I can achieve Understanding of what work needs to be done. Acceptance Criteria: Description of criteria: Documentation of the required work.",
                "epic_key": "CORS-3489",
                "GITHUB": [
                    {
                        "id": "9124",
                        "type": "pullRequest",
                        "title": "CORS-3855: Remove ARO build flag from installer",
                        "body": "Removing the ARO build flag from the installer to achieve the goal of moving ARO installs closer to the installer. Ideally, these ARO specific changes to the installs/installer need to be done either in the wrapper or hive."
                    }
                ]
            },
            "CORS-3854": {
                "summary": "Hybrid SRE: Add support to enable boot diagnostics option at installation time in Azure",
                "description": "User Story: As a openshift developer, I want to be able to: Enable boot diagnostics on worker nodes. Determine if setting it enabled by default is okay. so that I can achieve Better debugging. Acceptance Criteria: Description of criteria: Documentation of the work that needs to be done.",
                "epic_key": "CORS-3490",
                "GITHUB": [
                    {
                        "id": "9125",
                        "type": "pullRequest",
                        "title": "CORS-3854: Enable option to set boot diagnostics",
                        "body": "Enabling the option for the users to set the type of boot diagnostics for the bootstrap and control plane machines for log collection."
                    }
                ]
            },
            "CORS-3843": {
                "summary": "Add Tech Preview Feature gate to Installer",
                "description": "User Story: As a (user persona), I want to be able to: Add the Tech Preview Feature Gate to the installer for custom endpoints Validate the custom endpoints feature gate in the installer Capability 3 so that I can achieve An installer feature gate to ensure users know that this feature is not yet slated for release Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9501",
                        "type": "pullRequest",
                        "title": "CORS-3843: Add Tech Preview Feature gate to Installer for GCP Custom API Endpoints",
                        "body": "Vendor updates: Add the api changes from Add the featuregate to the installer Add validation for the feature gate."
                    }
                ]
            },
            "CORS-3842": {
                "summary": "Add GCP Endpoint Tech Preview Feature to API",
                "description": "User Story: As a (user persona), I want to be able to: Add the feature to API Add tech preview tags for the feature Capability 3 so that I can achieve Protect installs using this feature. The feature will touch many aspects of openshift Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "2150",
                        "type": "pullRequest",
                        "title": "CORS-3842: Add API Updates for GCP Custom API Endpoints",
                        "body": "Add the Tech preview and No upgrade tags for the new feature GCP API Custom Endpoints. Add the ServiceEndpoint Structure that includes the api name and endpoint. Add the Service Endpoints to the GCP Spec and Status structs."
                    }
                ]
            },
            "CORS-3835": {
                "summary": "Add Endpoints to GCP Platform in Install Config",
                "description": "User Story: As a (user persona), I want to be able to: Enter the custom endpoints via the install config Capability 2 Capability 3 so that I can achieve Initiate an install where the custom endpoints for GCP APIs can be used. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: The user enters the data into the install-config. The data is validated. Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "9363",
                        "type": "pullRequest",
                        "title": "CORS-3835: Add gcp endpoints to the installer config",
                        "body": "Added the endpoints including the name and url to the install config. Added Specific endpoint names that able to be used/customized. Added validation for the GCP Service Endpoints."
                    }
                ]
            },
            "CORS-3637": {
                "summary": "OWNERS files for platform providers",
                "description": "Most of the platform subdirectories don't have OWNERS files we should add the aliases for everything that's missing backport to 4.16",
                "epic_key": "CORS-3623",
                "GITHUB": [
                    {
                        "id": "9407",
                        "type": "pullRequest",
                        "title": "CORS-3637: OWNERS files for platform provider subdirectories",
                        "body": "The files use aliases defined in OWNERS_ALIASES. Some files are only formatted (i.e. removing blank spaces & ending newline)."
                    }
                ]
            },
            "CORS-3960": {
                "summary": "Remove Terraform (!)",
                "description": "User Story: When Terraform is no longer used, I want to quit building any terraform artifacts, images, or ci jobs. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "GITHUB": [
                    {
                        "id": "9673",
                        "type": "pullRequest",
                        "title": "CORS-3960: Remove Terraform from Dockerfiles",
                        "body": "Removes all dependencies for the Terraform providers image from the Dockerfiles. We want to do this separately from removing all the build artifacts, so that the image can stop being built by ART without breaking anything, then remove the build artifacts."
                    }
                ]
            },
            "CORS-3959": {
                "summary": "Installer Hook Azure Stack Provisioning",
                "description": "User Story: All the installer hook (non-CAPZ) provisioning for Azure stack Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3272",
                "GITHUB": [
                    {
                        "id": "9666",
                        "type": "pullRequest",
                        "title": "CORS-3959, CORS-3864: CAPI AzureStack Support, Take II",
                        "body": "9645 accidentally broke baremetal installs and was reverted in 9655, per TRT policy. Accordingly, this PR reverts the revert and introduces the fix for baremetal IPI 25aebd22655411c7fe1dd8f0b41be6e303a39b5d"
                    },
                    {
                        "id": "9645",
                        "type": "pullRequest",
                        "title": "CORS-3959, CORS-3864: CAPI-based AzureStack Installs",
                        "body": "Adds support for installing to Azure Stack using CAPZ. Utilizes an openshift fork: openshift/cluster-api-provider-azurestack based on It is simple enough to build the controller from the fork, but in order to pick up the API changes I introduced the forked API (adds a single `ARMEnvironment` field to the ClusterClass) as a subpackage. See 7352698b375c2858ed3b21db3d35141a4126eccc and 3ab4bebcc01eca3e9c9aa8b00da472d7175749d4. The other commits handle the azure stack specifics and are described in the commit messages. Particularly, API Versions need to be updated to compatible versions, older SDKs need to be used to upload blobs, and we need a process for creating managed images, as Azure Stack does not support image galleries."
                    }
                ]
            },
            "CORS-3936": {
                "summary": "Support creating clusters with AWS Public IP",
                "description": "User Story: As a (user persona), I want to be able to: Create IPI AWS clusters with Public IP-only option (aka. NAT-less deployment) with managed-VPC so that I can achieve significant cost saving for OpenShift CI Currently, openshift-installer supports \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" environment, which will skip creating NAT Gateway. However, the above env will make openshift-installer skip the entire creation of VPC, which means, the VPC must be manually created beforehand. Our goal is to modify the behaviour of \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" to allow omitting \"subnets\" section in install config and let the openshift-installer to create the VPC and Internet Gateway (in replacement of NAT Gateway). Acceptance Criteria: Description of criteria: Passing \"OPENSHIFT_INSTALL_AWS_PUBLIC_ONLY\" without \"subnets\" section in install config will not report: {noformat} level=error msg=failed to fetch Master Machines: failed to load asset \"Install Config\": failed to create install config: platform.aws.subnets: Required value: subnets must be specified for public-only subnets clusters {noformat} (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "GITHUB": [
                    {
                        "id": "9544",
                        "type": "pullRequest",
                        "title": "CORS-3936: Add support for public-only networking",
                        "body": "Add support for public-only networking (i.e., a cluster deployed with managed-VPC while has only public subnets) Ref: Ref:"
                    }
                ]
            },
            "CORS-3911": {
                "summary": "Use custom endpoints in GCP PD CSI Driver",
                "description": "User Story: As a (user persona), I want to be able to: Use custom endpoints through out all cluster components Capability 2 Capability 3 so that I can achieve Using the same api endpoints through the cluster Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: {code:java} pkg/gce-cloud-provider/compute/gce.go: service, err := computebeta.NewService(ctx, computeOpts...) pkg/gce-cloud-provider/compute/gce.go: service, err := compute.NewService(ctx, computeOpts...) pkg/gce-cloud-provider/compute/gce_test.go: service, _ := compute.NewService(ctx, computeOpts...){code} Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "141",
                        "type": "pullRequest",
                        "title": "CORS-3911: Add custom endpoint args from infrastructure",
                        "body": "The infrastructure struct will contain the endpoint overrides (if they exist). The overrides should be passed along to the csi-driver. In this case upstream already has a flag for `compute-endpoint`. The operator will search through the list of endpoint overrides and if the compute endpoint override is found, then the data is passed along as an arg/flag."
                    }
                ]
            },
            "CORS-3907": {
                "summary": "Use custom endpoints in the cluster ingress operator",
                "description": "User Story: As a (user persona), I want to be able to: Use the same custom endpoints (when applicable) through out all cluster components Capability 2 Capability 3 so that I can achieve Allowing users to override api endpoints in the cluster components. The DNS api is found here. Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: Each time the service is created New, the the option for `withEndpoint` should be issued when applicable. pkg/dns/gcp/provider.go {code:java} dnsService, err := gdnsv1.NewService(context.TODO(), option.WithCredentialsJSON(config.CredentialsJSON), option.WithUserAgent(config.UserAgent)) {code} (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-2389",
                "GITHUB": [
                    {
                        "id": "1197",
                        "type": "pullRequest",
                        "title": "CORS-3907: Update ingress operator to with custom endpoints",
                        "body": "Enhancement link for the feature: Vendor update Set the endpoint information in the Config for GCP If an endpoint exists for the DNS service use the endpoint as an option when creating the new client."
                    }
                ]
            },
            "CORS-3871": {
                "summary": "Place API LBs in specified subnets",
                "description": "User Story: The installer will configure the ControlPlaneInternalLB subnets in the spec.controlPlaneLoadBalancer.subnets field of the CAPA AWSCluster object, while the ControlPlaneExternalLB subnets will be set in the spec.secondaryControlPlaneLoadBalancer.subnets field. Acceptance Criteria: Description of criteria: Loadbalancers are created in the subnets specified in the install config. (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: CAPA type: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440",
                "GITHUB": [
                    {
                        "id": "9633",
                        "type": "pullRequest",
                        "title": "CORS-3871 CORS-3872 CORS-3873: place the cluster components on the assigned byo subnets if any",
                        "body": "Descriptions Place the cluster components on the assigned subnets (i.e. via subnet roles). If managed vpc or subnets without roles, fall back to existing logics. Notes There are additional related changes: - Introduce type `SubnetsByZone` in `pkg/asset/installconfig/aws/subnet.go` to avoid confusion with `Subnets` (i.e. should be indexed by subnet ID). - Introduce `SubnetByID` in `pkg/asset/installconfig/aws/metadata.go` for resuable/easy retrieval of subnet metadata from ID. - Unit tests: I only added unit tests for the `default` ingress controller manifest. Other tests require a lot of refactoring + mock support for AWS metadata. To avoid making this PR too large, I defer it to another time and just locally tested manually."
                    }
                ]
            },
            "CORS-3870": {
                "summary": "AWS API Validations",
                "description": "User Story: I want to run pre-flight checks against the AWS API for the subnets I provided in the install config. Acceptance Criteria: Description of criteria: All Subnets Belong to the Same VPC Validation Consistent Cluster Scope with IngressControllerLB Subnets Validation Reject BYO VPC Installations that Contain Untagged Subnets Reject Duplicate AZs (optional) Out of Scope: Engineering Details: (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440",
                "GITHUB": [
                    {
                        "id": "9663",
                        "type": "pullRequest",
                        "title": "CORS-3870: validate duplicate AZs if no subnet roles at all or only for subnets of the same role",
                        "body": "We should only validate duplicate AZs for provided subnets if no roles are explicitly specified. This allows us to support the following user stories (see EP 0( - Dedicated Load Balancer Subnets User Story - Cluster Nodes on Different Private Subnets User Story However, we will still validation the duplicate AZs for subnets of the same roles."
                    },
                    {
                        "id": "9599",
                        "type": "pullRequest",
                        "title": "CORS-3870: add validations for subnets field with AWS API",
                        "body": "Description This include validations for the vpc.subnets field with AWS API to conform to the specifications for subnet role assignment in the case of BYO subnets. The validation criteria can be found at jira ticket 0 and enhancement proposal 1. For automatic role selection (i.e. no subnets have roles assigned), the installer rejects BYO VPC with untagged subnets (i.e. those without tag `kubernetes.io/cluster/cluster-id`) and suggests users to add a tag `kubernetes.io/cluster/unmanaged` to those subnets. Other changes Other related changes include: - Refactored AWS API helper to be reusable without different query input (i.e. by vpcID, or subnetIDs) and migrated them to sdk v2 if possible. - Refactored and improved test structures. References 0 1"
                    }
                ]
            },
            "CORS-3867": {
                "summary": "Allow users to specify subnets with roles in install config",
                "description": "User Story: As an openshift-instlal user I want to be able to specify AWS subnets with roles. Acceptance Criteria: Description of criteria: Installconfig has installconfig.platform.aws.vpc.subnets field vpc.subnets conforms to API defined in the enhancement godoc/oc explain text is written and generated (see explain docs| (optional) Out of Scope: Validations will be handled in a different card. Engineering Details: API for installconfig field is defined in the type for subnets is: id - string roles - slice of SubnetRoles the list of subnet roles is defined in the enhancement. they include ClusterNode, EdgeNode, ControlPlaneExternalLBSubnetRole, etc... (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3440",
                "GITHUB": [
                    {
                        "id": "9655",
                        "type": "pullRequest",
                        "title": "CORS-3867: fix CRD descriptor to mention required BootstrapNode role",
                        "body": "This is a follow-up to In manual role selection, BootstrapNode role is required. This should be reflected in the CRD descriptor (i.e. `openshift-install explain`)."
                    },
                    {
                        "id": "9443",
                        "type": "pullRequest",
                        "title": "CORS-3867 CORS-3868: deprecate platform.subnets and introduce platform.vpc.subnets field in install-config",
                        "body": "Following proposal for selecting LB subnets, the field platform.vpc.subnets will be introduced for more flexible configurations. This enhancement proposal is available reference 0. There are some adjustments to the API markers and descriptions in comparison to the proposal. - Organize field description for easier read. - Correct kubebuilder:validation:MaxItems on array field This field is dropped in place of the deprecated `platform.subnets` field. Part of CORS-3440( References: 0"
                    }
                ]
            },
            "CORS-3825": {
                "summary": "Add azure disk nvme controller support",
                "description": "User Story: As a (user persona), I want to be able to: Capability 1 Capability 2 Capability 3 so that I can achieve Outcome 1 Outcome 2 Outcome 3 Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "CORS-3771"
            },
            "CORS-2508": {
                "summary": "Unused AWS action DescribeAutoScalingGroups",
                "description": "The OpenShift installer includes a autoscaling:DescribeAutoScalingGroups IAM permission which I believe its not used and carried over from something that may have existed in Hive a long time ago or maybe not at all. Reference installer commit they too don't see it. Done criteria: - Test creating a cluster with ROSA (Hive) - Create enabling auto scaling - Destroy cluster and ensure everything was cleaned up - Validate with CloudTrail that the IAM call was not used",
                "GITHUB": [
                    {
                        "id": "9223",
                        "type": "pullRequest",
                        "title": "CORS-2508: aws: remove unused autoscaling:DescribeAutoScalingGroups perm",
                        "body": "With the analysis from the permission is required by the installer but never used."
                    }
                ]
            }
        },
        "epics": {
            "CORS-3883": {
                "summary": "Azure: Remove Automatic Identity Creation",
                "description": "OCP/Telco Definition of Done Epic Goal Remove automatic (opinionated) creation (and attachment) of identities to Azure nodes Allow API to configure identities for nodes Why is this important? Creating and attaching identities to nodes requires elevated permissions The identities are no longer required (or used) so we can reduce the required permissions Scenarios Users want to do a default ipi install that just works without the User Access Admin role Users want to BYO user-assigned identity (requires some permissions) Users want to use a system assigned identity Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "133",
                        "type": "pullRequest",
                        "title": "CORS-3883: Allow managed identity from other resource groups",
                        "body": "Prior to this commit, MAPI only supports specifying a managed identity by name, and assumes that identity exists in the same resource group as the machine. This commit relaxes that requirement to allow a fully-specified id, to allow a managed identity from any resource group. For background context openshift/installer9538 removes the default managed-identity creation the installer has been creating since the beginning to authenticate cloud-provider-azure. With the move to out-of-tree, we have found that identity is no longer required as cloud-provider-azure is authenticated by credentials requests. Removing this automatic & unnecessary identity creation is a win for users, because they no longer need the Service Account Admin permission, which is a powerful permission. On the other hand, it is possible that there may be a use case for having identities attached to a machine, so openshift/installer9538 also exposes CAPZ's API to allow fuller control of VM identities, in case users need to manage it. Users can bring their own identity to attach to the VMs. We planned on limiting this to control-plane nodes due to the limitation in MAPI addressed by this PR, but @jinyunma pointed out( that control-plane nodes also depend on MAPI for the control-plane machineset operator. So I want to float this PR to see if we can expand the set of values accepted by MAPI. If this PR is unacceptable, we may consider other alternatives: - currently the installer requires that any existing resource group is empty before using it for install. This is to protect against unintended deletion of resources when using `destroy cluster`. We could rework that validation to allow a managed identity to exist in that resource group AND enforce a user-assigned identity belong to the cluster resource group. - we could also just not expose an identity API and simply remove the installer-created identity, but I think that leaves users with worse options, in the case they do need an identity This PR would be my first choice though, as it is most beneficial to users."
                    },
                    {
                        "id": "9625",
                        "type": "pullRequest",
                        "title": "CORS-3883: Remove user-assigned identity from ARM template",
                        "body": "For UPI, user-assigned identity attached on each node is also not required, remove it."
                    },
                    {
                        "id": "9538",
                        "type": "pullRequest",
                        "title": "CORS-3883, CORS-3861, CORS-3937: Azure Machine Identity API",
                        "body": "Most significantly, this PR removes the automatic creation of the cluster identity which was attached to control-plane and compute nodes. This identity was originally created in order to authenticate the in-tree cloud-provider-azure. When cloud-provider-azure moved out-of-tree and under the management of the CCMO, it began to use credentials requests to authenticate. The identity we create for that purpose is now unused. By removing the creation of the identity, we can reduce the permissions required to run an Azure install, particularly the User Access Administrator role, which is a significant role. This PR also extends CAPZ's identity API into the install config machine pool to allow users to customize the identities attached to VMs. The identity will default to `None`, but we also allow users to BYO `UserAssigned` identities or to have Azure create a `SystemAssigned` identity. For example: ```yaml controlPlane: platform: azure: identity: type: UserAssigned userAssignedIdentities: - name: test-capi-id resourceGroup: os4-common subscription: 433715e6-37fe-4328-af75-3661e13b15fc ``` uses an existing identity called `test-capi-id` from the `os4-common` resource group. ```yaml controlPlane: platform: azure: identity: type: SystemAssigned ``` Will create a system-assigned identity and attach it to the VM. It defaults to a contributor role. The system-assigned identity role can also be customized: ```yaml controlPlane: platform: azure: identity: type: SystemAssigned systemAssignedIdentityRole: definitionID: /providers/Microsoft.Authorization/roleDefinitions/8e3af657-a8ff-443c-a75c-2fe8c4bcb635 ``` Will give the system-assigned identity the Owner role (the value for `definitionID` corresponds to the owner role). This will also work for custom roles. 008926c614933a1eb6e6b25af7ce6785ff729d32 removes any identity from being attached to compute nodes. To configure an identity in MAPI, MAPI expects a user-assigned identity in the machine's resource group. That would only be possible when installing to an existing resource group, but the installer enforces that resource group to be empty. This can be addressed in future work, particularly the MAPI-CAPI transition will allow us to extend this API directly to compute nodes."
                    }
                ]
            },
            "CORS-3927": {
                "summary": "GCP - Add support to deploy Confidential VMs using Intel TDX",
                "description": "Epic Goal Add support to deploy Confidential VMs on GCP using Intel TDX technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using Intel TDX technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3923": {
                "summary": "GCP - Add support to deploy Confidential VMs using AMD SEV-SNP",
                "description": "Epic Goal Add support to deploy Confidential VMs on GCP using AMD SEV-SNP technology Why is this important? As part of the Zero Trust initiative we want to enable OpenShift to support data in use protection using confidential computing technologies Scenarios As a user I want all my OpenShift Nodes to be deployed as Confidential VMs on Google Cloud using SEV-SNP technology Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Previous Work (Optional): We enabled Confidential VMs for GCP using SEV technology already - OCPSTRAT-690 Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "9395",
                        "type": "pullRequest",
                        "title": "CORS-3923, CORS-3927: Support confidential cluster installation on SEV-SNP and TDX nodes on GCP",
                        "body": "This patch series aims to support installing a cluster on AMD SEV SNP or TDX confidential nodes on GCP. Previously, only AMD SEV nodes were supported, which were configured through the `confidentialCompute: Enabled` configuration flag. Now that GCP supports specifying the confidential instance type, we are letting users specify which node type (SEV/SEV-SNP/TDX) they would like to deploy the cluster on. This can be done by using any of the new available values in `condidentialCompute` such as `AMDEncryptedVirtualization` (similar to `Enabled`), `AMDEncryptedVirtualizationNestedPaging` (AMD SEV-SNP) or `IntelTrustedDomainExtensions` (Intel TDX). This series depend on the following patches: - - - - Which have been merged now and and vendored in this patch. A new cluster-api-provider-gcp release containing the changes I submitted isn't available yet. I created the `data/data/cluster-api/gcp-infrastructure-components.yaml` myself. I'm not sure if that's a valid approach, or if we should rather wait until a new version is out."
                    }
                ]
            },
            "CORS-2389": {
                "summary": "OpenShift Installer to support Private Google Access to GCP endpoints",
                "description": "Feature Overview Add support to custom GCP API endpoints (private and restricted) while deploying OpenShift on GCP Goals Enable OpenShift to support private and restricted GCP API endpoints while deploying the platform on GCP as we do for AWS already Requirements This Section: A list of specific needs or objectives that a Feature must deliver to satisfy the Feature.. Some requirements will be flagged as MVP. If an MVP gets shifted, the feature shifts. If a non MVP requirement slips, it does not shift the feature. RequirementNotesisMvp? This is a requirement for ALL features. Provide necessary release enablement details and documents. Use Cases This Section: As a user I want to be able to use GCP Private API endpoints while deploying OpenShift so I can be complaint with my company security policies As a user I want to be able to use GCP Restricted API endpoints while deploying OpenShift so I can be complaint with my company security policies Background, and strategic fit For users with strict regulatory policies, Private Service Connect allows private consumption of services across VPC networks that belong to different groups, teams, projects, or organizations. Supporting OpenShift to consume these private endpoints is key for these customers to be able to deploy the platform on GCP and be complaint with their regulatory policies. Documentation Considerations Questions to be addressed: What educational or reference material (docs) is required to support this product feature? For users/admins? Other functions (security officers, etc)? Does this feature have doc impact? New Content, Updates to existing content, Release Note, or No Doc Impact If unsure and no Technical Writer is available, please contact Content Strategy. What concepts do customers need to understand to be successful in action? How do we expect customers will use the feature? For what purpose(s)? What reference material might a customer want/need to complete action? Is there source material that can be used as reference for the Technical Writer in writing the content? If yes, please link if available. What is the doc impact (New Content, Updates to existing content, or Release Note)?"
            },
            "CORS-3440": {
                "summary": "Add ability to choose ingress controller subnets at installation",
                "description": "OCP/Telco Definition of Done Epic Goal Add the ability to choose subnets for IngressControllers with LoadBalancer-type Services for AWS in the Installer. This install config should be applies to the default IngressController and all future IngressControllers (the design is similar to installconfig.platform.aws.lbtype Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3272": {
                "summary": "Provision Azure Stack Infra with CAPI",
                "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3489": {
                "summary": "Hybrid SRE: Remove ARO build-flag in openshift-installer",
                "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3490": {
                "summary": "Hybrid SRE: Add support to enable boot diagnostics option at installation time in Azure",
                "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3623": {
                "summary": "Technical debt for 4.19",
                "description": "Epic Goal This epic includes tasks the team would like to tackle to improve our process, QOL, CI. It may include tasks like updating the RHEL base image and vendored assisted-service. Why is this important? We need a place to add tasks that are not feature oriented. Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORS-3278": {
                "summary": "Replace Terraform with CAPI Provider for IBM Cloud",
                "description": "Epic Goal Replace Terraform infrastructure and machine (bootstrap, control plane) provisioning with CAPI-based approach.",
                "GITHUB": [
                    {
                        "id": "9649",
                        "type": "pullRequest",
                        "title": "CORS-3278: IBMCloud: Remove IBM Cloud TF provider",
                        "body": "Now that IBM Cloud CAPI is the default for IPI, remove the IBM Cloud TF provider to prevent it being built, as it is no longer being used."
                    },
                    {
                        "id": "9636",
                        "type": "pullRequest",
                        "title": "CORS-3278: IBMCloud: Add DestroyBootstrap for leftover CAPI resources",
                        "body": "Add DestroyBootstrap support for IBM Cloud VPC, to cleanup the remaining resources used during bootstrapping during CAPI deployment. Attach a floating IP to bootstrap node for public clusters during post provision."
                    },
                    {
                        "id": "9652",
                        "type": "pullRequest",
                        "title": "CORS-3278: Switch IBMCloud to CAPI",
                        "body": "Sets IBMCloud to default to using CAPI--not Terraform for installs."
                    },
                    {
                        "id": "9523",
                        "type": "pullRequest",
                        "title": "CORS-3278: Bump IBM Cloud CAPI to v0.10.0",
                        "body": "Bump IBM Cloud CAPI to v0.10.0 to pick up fixes for Infrastructure."
                    }
                ]
            },
            "CORS-3771": {
                "summary": "Azure - Add support for Dxv6 machine series",
                "description": "Epic Goal Dlsv6 Dsv6 Why is this important? ARO will need to support Dxv6 instance types supported. These are currently in preview Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Core Networking": {
        "stories": {
            "CORENET-5786": {
                "summary": "ux improvement: provide decent error msg if the provided physical network name is invalid",
                "description": "We want to throw a decent error msg to the user (seen when describing the pod) in case the network to which to pod is being attached features an invalid (missing) bridge mapping name - i.e. the physical network name points to a mapping that doesn't exist.",
                "epic_key": "CORENET-5358",
                "GITHUB": [
                    {
                        "id": "2521",
                        "type": "pullRequest",
                        "title": "CORENET-5786: Downstream merge 2025-04-25",
                        "body": "No conflicts. @jluhrsen @kyrtapz"
                    }
                ]
            },
            "CORENET-5751": {
                "summary": "origin: use l2bridge binding for virt suite",
                "description": "We're still using the passt binding, which prevents us from checking the TCP connection persistence. We should start using the correct binding - l2bridge - which will allow us to QE the real binding our customers are using, and will also allow us to improve our test coverage, since we would be able to align w/ our upstream tests.",
                "epic_key": "CORENET-5649"
            },
            "CORENET-5587": {
                "summary": "API Port CUDN e2e tests to openshift/origin",
                "description": "Port e2e tests of CUDN introduced on U/S by",
                "epic_key": "CORENET-4931"
            },
            "CORENET-5575": {
                "summary": "Add placeholder GA tests for persistent IPs feature on openshift conformance tests"
            },
            "CORENET-5481": {
                "summary": "Fix security issues with CNO IPSec certificate signing",
                "description": "In CNO we have an approver that signs certs automatically, without checking any identity information. We should modify this to require that the certificate request contains the kubelet certificate (issued separately) to ensure the identity of the client is an openshift node. We should not just hand out certificates to anyone who asks for them.",
                "epic_key": "CORENET-5361"
            },
            "CORENET-5479": {
                "summary": "UDN API Make IPAM options for explicit in the API; cater to common use cases",
                "description": "See the UDN Sync Meeting notes: In our current UDN API, subnets field is mandatory always for primary role and optional for secondary role. This is because users are allowed to have a pure L2 without subnets for secondary networks. However, in the future if we want to add egress support on secondary networks, we might need subnets... CNV has many different use cases: For UDPNs, we always need subnets for L2 and L3 why not make them optional and let users get default values? - drawback is loosing visibility and this podsubnet now conflicting with other internal subnets and customer in their ignorange have the oopsy stage, we have seen this in plenty with joinsubnets already For UDSNs, we may or maynot have the need for IPAM, today this subnets field is optional, but then when we do need subnets we cannot set default values here so its icky. This card tracks the design changes to the API and the code changes needed to implement this. See for details.",
                "epic_key": "CORENET-4931"
            },
            "CORENET-5389": {
                "summary": "VM/Pod using OVN localnet network unable to access its own host IP/Ingress",
                "description": "Description of problem: VM or Pod utilizing the OVN localnet network is unable to access its own host IP or the ingress service when both are hosted on the same node. However, the VM/Pod can successfully ping other compute nodes and communicate with external networks. Observed Results: VM/Pod = VM/Pod's Host: Not working VM/Pod = Ingress (same host): Not working VM/Pod = Other Hosts: Works VM/Pod = Ingress (different host): Works External = VM/Pod: Works VM/Pod = External Networks: Works While the issue involves the inability to access the host from the VM, our goal is to access the ingress of the hosting cluster. Version-Release number of selected component (if applicable): 4.16 How reproducible: 100% Steps to Reproduce: 1. Create nncp object with localnet: apiVersion: nmstate.io/v1 kind: NodeNetworkConfigurationPolicy metadata: annotations: description: localnet1 network is mapped to the br-ex bridge (OVN) name: localnet1-nncp spec: desiredState: ovn: bridge-mappings: - bridge: br-ex localnet: localnet1 state: present nodeSelector: node-role.kubernetes.io/worker: '' 2. Create NetworkAttachmentDefinition object apiVersion: k8s.cni.cncf.io/v1 kind: NetworkAttachmentDefinition metadata: name: localnet1-network spec: config: - \\{ \"name\": \"localnet1-network\", \"mac\": \"XX:XX:XX:XX:88:05\", \"ips\": \"XX.XX.XX.11/24\" } labels: app: test-pod spec: containers: - name: test-pod image: docker.io/library/alpine:latest command: \"/bin/sleep\", \"10000\" ~~~ 4. Attempt to access the host IP or ingress service located on the same host node. Hypervisor (hosting Pod) IP: XX.XX.XX.70 Hypervisor (Another worker in cluster) IP: XX.XX.XX.71 Ping to own host fails ping XX.XX.XX.70 PING XX.XX.XX.70 (XX.XX.XX.70): 56 data bytes ^C --- XX.XX.XX.70 ping statistics --- 8 packets transmitted, 0 packets received, 100% packet loss Ping to other worker in environment works / ping XX.XX.XX.71 -c 1 PING XX.XX.XX.71 (XX.XX.XX.71): 56 data bytes 64 bytes from XX.XX.XX.71: seq=0 ttl=42 time=0.199 ms Actual results: VM/Pod using OVN localnet network unable to access its own host IP/Ingress Expected results: VM/Pod using OVN localnet network should be able to access Ingress Additional info: Affected Platforms: Is it an internal RedHat testing failure.",
                "epic_key": "CORENET-5878",
                "GITHUB": [
                    {
                        "id": "2516",
                        "type": "pullRequest",
                        "title": "CORENET-5389,OCPBUGS-51040,OCPBUGS-54577: DownstreamMerge 2025-04-16",
                        "body": "!-- Please make sure you've read and understood our contributing guidelines; Make sure all your commits include a signature generated with `git commit -s` All changes must adhere to this template to make it easy for reviewers and preserve rationale/history behind every change -- \ud83d\udcd1 Description !-- Add a brief description of the pr -- This D/S sync brings along primary UDN for virtualization fixes for IPv6. !-- Automatically closes linked issue when PR is merged. Usage: `Fixes issue number`, or `Fixes (paste link of issue)`. _If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_ -- Fixes Additional Information for reviewers !-- What exactly did you change - you may also defer to information contained in commit messages. At a bare minimum it's worth highlighting which areas of the code were changed as it's easier to assign reviewers -- Clean merge, no conflicts. \u2705 Checks !-- Make sure your pr passes the CI checks and do check the following fields as needed - -- - My code requires changes to the documentation - if so, I have updated the documentation as required - My code requires tests - if so, I have added and/or updated the tests as required - All the tests have passed in the CI !-- If not leave a comment as to why the CI is red and if you need help understanding what's wrong -- How to verify it !-- Did you include unit tests? or end-to-end tests? How can I manually verify that this patch achieves its objective --"
                    }
                ]
            },
            "CORENET-5374": {
                "summary": "openshift/origin: only provision workloads when network creation has started"
            },
            "CORENET-5371": {
                "summary": "Evaluate improvements to ovs-monitor-ipsec/libreswan",
                "description": "If opportunistic IPSec is a dead-end we need to investigate whether or not further improvements to what we have is possible. Some ideas are: Improve ovs-monitor-ipsec logic to reconcile. Ilya is already working on this: Check if there is a native API solution where we can talk to libreswan, instead of forking bash commands. strongswan enables dynamic config file watching, consult with IPSec team if it is possible for that same capability to be added to libreswan. Then we eliminate the need to manage ipsec connections in ovs-monitor-ipsec, and can simply update the config file. Examine using a different ipsec control plane (maybe strongswan) instead of libreswan.",
                "epic_key": "CORENET-5361",
                "GITHUB": [
                    {
                        "id": "2472",
                        "type": "pullRequest",
                        "title": "CORENET-5371: Dockerfile: Bump OVS version to 3.5.0-0.9",
                        "body": "This new version provides configuration flexibility for `openvswitch-ipsec` systemd service which enables OCP to run `ovs-monitor-ipsec` as a service on host instead of running it inside a container. This PR as such only bumps OVS version to `3.5.0-0.9`, so it doesn't have any harm in merging it. The changes in the PRs get `openvswitch-ipsec` systemd service running on the host and OVN uses it to configure IPsec for east west traffic. Until these two PRs are merged, CNO would just continue to use `ovs-monitor-ipsec` process running in the `ovn-ipsec-host` pod container."
                    }
                ]
            },
            "CORENET-5241": {
                "summary": "Enable support to enable OVN-Kubernetes BGP in CNO",
                "description": "CNO should deploy the new RouteAdvertisements OVN-K CRD. When the OCP API flag to enable BGP support in the cluster is set, CNO should enable support on OVN-K through a CLI arg.",
                "epic_key": "CORENET-4947"
            },
            "CORENET-5080": {
                "summary": "Make CNO to react for Machine Config Pool status",
                "description": "The CNO rolls out ipsec mc plugin for rolling out IPsec for the cluster, but it doesn't really check master and work role machine config pools status to confirm if that's successfully installed in the cluster nodes. Hence CNO should be made to listen for MachineConfigPool status object updates and set network operator condition accordingly based on ipsec mc plugin rollout status.",
                "epic_key": "CORENET-5361"
            },
            "CORENET-4974": {
                "summary": "L2 NetworkPolicy Support NetworkPolicies on Primary UDNs",
                "description": "We want to do Network Policies not MultiNetwork POlicies",
                "epic_key": "CORENET-4931"
            },
            "CORENET-4056": {
                "summary": "Improve ipsec tests",
                "epic_key": "CORENET-5361"
            },
            "CORENET-821": {
                "summary": "Whereabouts Downstream Merge",
                "description": "DS merge for fast range fixes and dep bumps"
            },
            "CORENET-710": {
                "summary": "Whereabouts fast ranges: Get fast ranges working in the CNO",
                "description": "The CNO should deploy the fast ranges CRDs We need to add this CRD And it'll be deployed with the CNO in here: Goal: Enable the perf/scale team to use Whereabouts fast ranges (as well as other interested parties, such as telco)",
                "epic_key": "CORENET-664"
            },
            "CORENET-5914": {
                "summary": "API Replicate CRD changes to CNO",
                "description": "Following extensions of CUDN CRD to support Localnet topology, the CRD changes should be replicated to CNO in order to make them available on OCP.",
                "epic_key": "CORENET-5358",
                "GITHUB": [
                    {
                        "id": "2678",
                        "type": "pullRequest",
                        "title": "CORENET-5914: bindata,ovn-k: Update CUDN CRD following localnet support",
                        "body": "The ClusterUserDefinedNetwork CRD has been extended and now support creating localnet topology networks."
                    }
                ]
            },
            "CORENET-5856": {
                "summary": "Update RA API with new universal network selector",
                "epic_key": "CORENET-5350",
                "GITHUB": [
                    {
                        "id": "2685",
                        "type": "pullRequest",
                        "title": "CORENET-5856: Update RouteAdvertisements schema"
                    }
                ]
            },
            "CORENET-5743": {
                "summary": "CNCC 1.32 Kube Rebase",
                "epic_key": "CORENET-5635"
            },
            "CORENET-5721": {
                "summary": "SGW Add support for Layer-2 UDNs",
                "description": "The main difficulty of supporting L2 UDN is not having a node-specific pod network subnet to advertise with that node as next hop: L2 UDNs subnet is cluster wide. One of the ideas was to advertise /32 pod specific routes but there are concerns on the scalability of that. The other idea is to advertise the whole L2 UDN subnet with the selected nodes as next hop and let multi-path take care of the rest. With this alternative there is acceptance that this might not always route the traffic on the most optimum path. This effort entails adding support for it in cluster manager route advertisement controller, and mimic the existing support of L3 zone and node network controllers for L2 as well. This includes upstream testcases that should basically mimic L3 existing test cases which add some level of dependency with SDN-5712.",
                "epic_key": "CORENET-5350"
            },
            "CORENET-5711": {
                "summary": "CNO 1.32 Kube rebase",
                "epic_key": "CORENET-5635"
            },
            "CORENET-5678": {
                "summary": "CNO: update MNP CRD",
                "epic_key": "CORENET-5645"
            },
            "CORENET-5666": {
                "summary": "Downstream Merge of OVN-Kubernetes 1.32 rebase",
                "epic_key": "CORENET-5635",
                "GITHUB": [
                    {
                        "id": "2505",
                        "type": "pullRequest",
                        "title": "CORENET-5666,OCPBUGS-43004,OCPBUGS-54199: Downstream Merge 2025-04-02",
                        "body": "cc @ricky-rav"
                    }
                ]
            },
            "CORENET-5581": {
                "summary": "Add ipsec upgrade ci job as mandatory lane",
                "description": "The e2e-aws-ovn-ipsec-upgrade job is currently an optional job and always_run: false because the job not reliable and success rate is so low. This must be made as mandatory CI lane after fixing its relevant issues.",
                "epic_key": "CORENET-5361"
            },
            "CORENET-5524": {
                "summary": "Revert libreswan-4.6 change in openshift/os repo",
                "description": "Revert libreswan-4.6 change in openshift/os repo made via once final fixes in libreswan for bug| is ready to be consumed in openshift.",
                "epic_key": "CORENET-5361",
                "GITHUB": [
                    {
                        "id": "4959",
                        "type": "pullRequest",
                        "title": "CORENET-5524: Modify regex for ipsec showstates command",
                        "body": "The Libreswan 5.2 uses a slightly updated string for displaying established child SAs for an IPsec connection. So this PR updates ipsec showstates command with right regex value. The modified command is harmless and can also work well with Libreswan 4.x version."
                    }
                ]
            },
            "CORENET-5387": {
                "summary": "Set the NetworkSegmentation FG on persistent IPs conformance tests using primary UDNs"
            },
            "CORENET-5382": {
                "summary": "Restore default libreswan version in ovnk image",
                "description": "The SDN-5480 gets libreswan4.6 into ovnk image to use stable libreswan version which fixes connection/timeout/crash issues. But we must restore libreswan to default version in ovnk image once these issues are addressed in any of forthcoming RHEL release.",
                "epic_key": "CORENET-5361",
                "GITHUB": [
                    {
                        "id": "2498",
                        "type": "pullRequest",
                        "title": "CORENET-5382: unpin libreswan version",
                        "body": "This PR unpins the ovnk libreswan-4.6 version which was introduced through OCPBUGS-41823( It allows to consume the latest libreswan 5 version from FDP repo which includes recent improvement and enhancement from upstream libreswan, that would otherwise only be made available until RHEL-10. This PR depends on OS( update which was already merged."
                    }
                ]
            },
            "CORENET-5253": {
                "summary": "Downstream e2e CI tests for PodNetwork Advertisement",
                "epic_key": "CORENET-5350"
            },
            "CORENET-5115": {
                "summary": "monitoringL2/L3 Open default network ports on UDN pods via users's request through pod annotations",
                "epic_key": "CORENET-4931"
            },
            "CORENET-4481": {
                "summary": "Whereabouts perf scale: Re-enable opt-in for the node slice controller.",
                "description": "We made a compromise to automatically enable this for now in order to move forward with perf/scale testing as a short term item. While this shouldn't have a large impact on cluster performance in general (even when installed without use), it should still be optional to use. See also:",
                "epic_key": "CORENET-664"
            },
            "CORENET-454": {
                "summary": "temp logging change for debugging release",
                "description": "temp for bot"
            },
            "CORENET-365": {
                "summary": "Whereabouts Downstream Merge"
            }
        },
        "epics": {
            "CORENET-5358": {
                "summary": "Universal connectivity: Localnet 4.19",
                "description": "Template: Networking Definition of Planned Epic Goal Provide quality user experience for customers connecting their Pods and VMs to the underlying physical network through OVN Kubernetes localnet. Why is this important? This is a continuation to It covers the UDN API for localnet and other improvements Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (/) Priority+ is set by engineering - (/) +Epic must be Linked to a +Parent Feature+ - (/) Target version+ must be set - (/) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated This must be done downstream too Release Technical Enablement - Provide necessary release enablement details and documents. OVN Kubernetes secondary networks with the localnet topology can be created through ClusterUserDefinedNetworks When possible, user input is validated and any configuration issue is shown on the UDN. Alternatively some issues can be shown on CNI ADD events on Pod -Definition of these networks can be changed even if there are Pods connected to them. When that happens, the UDN is marked as degraded until all the \"old\" pods are gone. The mutable fields should be: MTU, VLAN, physnet name- For cases where a user incorrectly set their MTU, VLAN, or physnet name, there is a clear and foolproof flow describing how to correct this mistake. A single \"bridge-mappings\" \"localnet\" can be referenced from multiple different UDNs The default MTU set for localnet is 1500 Pod requesting UDN without a VLAN is able to connect to services running on the host's network ({-}stretch) The \"physnet\" mapping is a \"supported API\" and available to users - so they can connect to the machine network without a need to configure a custom bridge-mapping{-} we should just always request user to configure the mapping themselves, until we understand all the implications of non-NORMAL mode on br-ex and how it works with local access / bondings / ... (stretch) Scheduling is managed by the platform - if a UDN requests a localnet (as in bridge-mappins.localnet), the Pod requesting this UDN will be only scheduled on a node with this resource available. This can use the same mechanism as the SR-IOV operator - combination of device plugins and \"k8s.v1.cni.cncf.io/resourceName\" annotation ... IPAM is not in the scope of this epic. See RFE-6947. Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORENET-5878": {
                "summary": "Universal connectivity: Localnet 4.20",
                "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORENET-4947": {
                "summary": "4.18 OVN Kubernetes support for BGP as a routing protocol",
                "description": "Epic Goal OVN Kubernetes support for BGP as a routing protocol. Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORENET-664": {
                "summary": "Tech Preview Whereabouts: Performance and scale considerations",
                "description": "Epic Goal Address performance and scale issues in Whereabouts IPAM CNI Why is this important? Whereabouts is becoming increasingly more popular for use on workloads that operate at scale. Whereabouts was originally built as a convenience function for a handful of IPs, however, more and more customers want to use whereabouts in scale sitatuions. Notably, for telco and ai/ml scenarios. Some ai/ml scenarios launch a large number of pods that need to use secondary networks for related traffic. Supporting Documents Upstream collaboration outline| Acceptance Criteria Both original allocation mode, and fast_ranges mode work without user intervention (e.g. backwards compatible) Reconciler still works with fast_ranges fast_ranges: 50% reduction in allocation time over original allocation method"
            },
            "CORENET-5350": {
                "summary": "4.20 GA OVN Kubernetes support for BGP as a routing protocol: On-Prem",
                "description": "Epic Goal Left over from 4.18 (potentially BGP+UDN, egress IP) perf/scale UX fixes (ovnk specific API) Enabling subset of nodes selected for BGP advertisement with pod network (requirement from customers) Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORENET-5635": {
                "summary": "Rebase Kube version to 1.32 in repos maintained by the SDN team",
                "description": "Template: Networking Definition of Planned Epic Goal Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "CORENET-5645": {
                "summary": "Support EndPort in MultiNetworkPolicy",
                "description": "Template: Networking Definition of Planned Epic Goal Add support for endPort field in multinetworkpolicy. The API change is merged upstream time to make d/s update. The support tracked in this epic is only for ovn-kubernetes, multi-netpol implementation will catch up later. It is known that MNP lacks some validations that networkpolicy has due to being a core API. It needs additional discussion (and potentially breaking changes) to decide whether MNP should also introduce similar validation. Opened a bug Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn't have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Virtualization": {
        "stories": {
            "CNV-59853": {
                "summary": "Update old console classes",
                "description": "Followup to: Also Catalog - Template catalog is visually broken in kubevirt-ui",
                "GITHUB": [
                    {
                        "id": "231",
                        "type": "pullRequest",
                        "title": "CNV-59853: remove co-m-table-grid and dropdown-kebab-pf classes",
                        "body": "- replaces `co-m-table-grid` and related classes with PatternFly `Table` - removes `dropdown-kebab-pf` classes Before: img width=\"1429\" alt=\"Screenshot 2025-04-14 at 11 38 33\" src=\" / img width=\"1429\" alt=\"Screenshot 2025-04-14 at 11 38 22\" src=\" / After: img width=\"1450\" alt=\"Screenshot 2025-04-14 at 10 51 15\" src=\" / img width=\"1429\" alt=\"Screenshot 2025-04-14 at 11 18 18\" src=\" /"
                    }
                ]
            },
            "CNV-58647": {
                "summary": "DEV: Networking: PatternFly Modal deprecated in PF6",
                "description": "update PF modal",
                "epic_key": "CNV-57714",
                "GITHUB": [
                    {
                        "id": "228",
                        "type": "pullRequest",
                        "title": "CNV-58647: update PatternFly Modal to new version",
                        "body": "- Updates PF Modal to new version. - replaces `variant=\"type\"` with `SomeVariant.type` - removes unnecessary `type=\"button\"` on PF Buttons"
                    }
                ]
            },
            "CNV-57648": {
                "summary": "upstream documentation for descheduler integration with hcp nodepools",
                "description": "In the hypershift upstream documentation, outline how the de-scheduler can be used to continually redistribute VMs in a nodepool when clumping of VMs occur after live migration.",
                "epic_key": "CNV-41958",
                "GITHUB": [
                    {
                        "id": "5779",
                        "type": "pullRequest",
                        "title": "CNV-57648: document how to configure descheduler",
                        "body": "What this PR does / why we need it: Document hot to configure the cluster-kube-descheduler operator to configure the descheduler according to the `SoftTopologyAndDuplicates` profile in order to satisfy a topologySpreadConstraint to continuously spread VMs for a NodePool out across multiple underlying nodes. Which issue(s) this PR fixes: Fixes CNV-57648( Checklist - X Subject and description added to both, commit and PR. - X Relevant issues have been referenced. - X This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "CNV-55987": {
                "summary": "DEV: Aggregate the alerts in the Alerts page by alert name and severity",
                "description": "Goal Description As an Admin I want to have an easy view of the alerts that are firing in my cluster. If I have the same alert that fires many time it is very hard to identify the issues. We can simplify the existing Alerts page to make it much clearer by the following quick fix: 1. Aggregate the alerts by alert name and severity 2. For each aggregated line add the \"Total alerts number\" 3. When pressing on the aggregated line it can be expanded with the list of alerts 4.Optional - Add the namespace label to the expanded list of alerts, where each alert can have a different namespace. Note: Not all alerts have this label. Initial mockup by ~fkargbo: Acceptance Criteria Given we have several alerts of the same name and severity When we go to the Alerts page and view the alerts Then We would see a single line for each \"alert name\" and \"severity\", the number of times its shown and I can click on the line to expend the line and get the full list of alerts of that name and severity. User Stories High-Level goal-based user story, with context. \"As a VM owner/cluster administrator, I want to Achieve Some Goal, so that Some Reason/Context.\" another user story Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes Any additional details or decisions made/needed",
                "epic_key": "CNV-54399",
                "GITHUB": [
                    {
                        "id": "348",
                        "type": "pullRequest",
                        "title": "CNV-55987: Aggregated alerts",
                        "body": "Demo Fix for inner table sticky header After some discussion and Foday redesign img width=\"1915\" alt=\"Screenshot 2025-03-10 at 09 38 16\" src=\" /"
                    }
                ]
            },
            "CNV-58320": {
                "summary": "DEV: Remove Console classes in Networking UI",
                "epic_key": "CNV-58313",
                "GITHUB": [
                    {
                        "id": "227",
                        "type": "pullRequest",
                        "title": "CNV-58320: remove console classes which changed styling",
                        "body": "Removes `co-m-pane__body`, `co-m-nav-title`, replaces heading elements with `Title`"
                    }
                ]
            },
            "CNV-56305": {
                "summary": "DEV: PatternFly6 Upgrade NETWORK UI",
                "description": "Goal Upgrade CNV make use of PatternFly6 User Stories Notes Any additional details or decisions made/needed",
                "epic_key": "CNV-56300",
                "GITHUB": [
                    {
                        "id": "221",
                        "type": "pullRequest",
                        "title": "CNV-56305: upgrade Network UI to PatternFly 6",
                        "body": "Updates Networking console plugin to PatternFly 6. Found an unused file: `/src/views/udns/list/components/ProjectSelector.tsx` so I deleted it"
                    }
                ]
            }
        },
        "epics": {
            "CNV-57714": {
                "summary": "PatternFly Modal deprecated in PF6",
                "description": "PatternFly Modal was deprecated in PF6, we should replace this component with a new Modal. This issue is a followup to updating to PF6:"
            },
            "CNV-41958": {
                "summary": "HCP KubeVirt VM Enhanced Topology Spread",
                "description": "Feature Overview (aka. Goal Summary) Today VMs for a single nodepool can \"clump\" together on a single node after the infra cluster is updated. This is due to live migration shuffling around the VMs in ways that can result in VMs from the same nodepool being placed next to each other. Through a combination of TopologySpreadConstraints and the De-Scheduler, it should be possible to continually redistributed VMs in a nodepool (via live migration) when clumping occurs. This will provide stronger HA guarantees for nodepools Goals (aka. expected user outcomes) VMs within a nodepool should re-distribute via live migration in order to best satisfy topology spread constraints."
            },
            "CNV-54399": {
                "summary": "Aggregate the alerts in the Alerts page by alert name and severity",
                "description": "Goal Description As an Admin I want to have an easy view of the alerts that are firing in my cluster. If I have the same alert that fires many time it is very hard to identify the issues. We can simplify the existing Alerts page to make it much clearer by the following quick fix: 1. Aggregate the alerts by alert name and severity 2. For each aggregated line add the \"Total alerts number\" 3. When pressing on the aggregated line it can be expanded with the list of alerts 4.Optional - Add the namespace label to the expanded list of alerts, where each alert can have a different namespace. Note: Not all alerts have this label. Initial mockup by ~fkargbo: Acceptance Criteria Given we have several alerts of the same name and severity When we go to the Alerts page and view the alerts Then We would see a single line for each \"alert name\" and \"severity\", the number of times its shown and I can click on the line to expend the line and get the full list of alerts of that name and severity. User Stories High-Level goal-based user story, with context. \"As a VM owner/cluster administrator, I want to Achieve Some Goal, so that Some Reason/Context.\" another user story Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes Any additional details or decisions made/needed"
            },
            "CNV-46603": {
                "summary": "UI for OVN Kubernetes: Primary user-defined networks",
                "description": "Goal Primary used-defined networks can be managed from the UI and the user flow is seamless. User Stories As a cluster admin, I want to use the UI to define a ClusterUserDefinedNetwork, assigned with a namespace selector. As a project admin, I want to use the UI to define a UserDefinedNetwork in my namespace. As a project admin, I want to be queried to create a UserDefinedNetwork before I create any Pods/VMs in my new project. As a project admin running VMs in a namespace with UDN defined, I expect the \"pod network\" to be called \"user-defined primary network\", and I expect that when using it, the proper network binding is used. As a project admin, I want to use the UI to request a specific IP for my VM connected to UDN. UX doc Non-Requirements List of things not included in this epic, to alleviate any doubt raised during the grooming process. Notes The user-defined networks design, including the API, is available here:",
                "GITHUB": [
                    {
                        "id": "150",
                        "type": "pullRequest",
                        "title": "CNV-46603: select projects with multitypeahead",
                        "body": "For user is difficult to use matchLabels. Use Project selection with `matchExpressions` use `matchExpressions` also to add projects to a ClusterUDN in the project creation modal Before img width=\"1915\" alt=\"Screenshot 2024-12-09 at 17 07 44\" src=\" After"
                    },
                    {
                        "id": "142",
                        "type": "pullRequest",
                        "title": "CNV-46603: search project by name and fix select height",
                        "body": "Before img width=\"1920\" alt=\"Screenshot 2024-11-28 at 16 03 56\" src=\" After img width=\"1920\" alt=\"Screenshot 2024-11-28 at 16 02 19\" src=\""
                    },
                    {
                        "id": "146",
                        "type": "pullRequest",
                        "title": "CNV-46603: Create project modal with ClusterUDN creation",
                        "body": "Select ClusterUDN Using this option we'll add the needed labels to the project to respect the ClusterUDN matchLabels. (We can't edit the project directly but if we patch the namespace, the project will get the labels) img width=\"1920\" alt=\"Screenshot 2024-11-29 at 17 49 36\" src=\" img width=\"1920\" alt=\"Screenshot 2024-11-29 at 17 49 39\" src=\" img width=\"1920\" alt=\"Screenshot 2024-11-29 at 17 51 59\" src=\""
                    },
                    {
                        "id": "145",
                        "type": "pullRequest",
                        "title": "CNV-46603: Cluster UDN modal creation",
                        "body": "Create Cluster UDN let the user choose the matchlabels and show the selected projects img width=\"1920\" alt=\"Screenshot 2024-12-02 at 10 46 38\" src=\" img width=\"1920\" alt=\"Screenshot 2024-12-02 at 10 46 54\" src=\" img width=\"1920\" alt=\"Screenshot 2024-12-02 at 10 55 53\" src=\""
                    },
                    {
                        "id": "137",
                        "type": "pullRequest",
                        "title": "CNV-46603: add CUDN kind",
                        "body": "Followup on to add `ClusterUserDefinedNetwork` to list / forms !image( !image( !image("
                    }
                ]
            },
            "CNV-58313": {
                "summary": "Remove Console classes (prefixed with co-, ocs-, odc-) in plugins",
                "description": "Remove Console classes in Kubevirt, Network and NMState UI plugins. - remove those, which have / will have their styling removed Details also on Slack:"
            },
            "CNV-56300": {
                "summary": "PatternFly6 Upgrade NETWORK UI",
                "description": "Goal Upgrade NETWORK UI make use of PatternFly6 User Stories Notes Any additional details or decisions made/needed"
            }
        }
    },
    "Cluster Integration and Delivery": {
        "stories": {
            "CLID-321": {
                "summary": "As a user of a disconnected cluster, I would like to identify easily the custom resources generated by oc-mirror applied to the cluster",
                "GITHUB": [
                    {
                        "id": "1086",
                        "type": "pullRequest",
                        "title": "CLID-321: Add annotations to resources generated by oc-mirror v2",
                        "body": "Description This PR helps add an annotation to all cluster resources generated by oc-mirror. Right now the annotation is `author: oc-mirror` , but let's discuss what is the right annotation to use. This way, on a cluster, when users need to remove/update/view IDMS/ITMS/CatalogSource generated by oc-mirror they could use: ```bash oc get idms -o jsonpath='{.items?(@.metadata.annotations.createdBy==\"oc-mirror v2\").metadata.name}' ``` Github / Jira issue: CLID-321( Type of change Please delete options that are not relevant. - Bug fix (non-breaking change which fixes an issue) - x New feature (non-breaking change which adds functionality) - Breaking change (fix or feature that would cause existing functionality to not work as expected) - Code Improvements (Refactoring, Performance, CI upgrades, etc) - Internal repo assets (diagrams / docs on github repo) - This change requires a documentation update on openshift docs How Has This Been Tested? I tested this only for additionalImages. ```yaml kind: ImageSetConfiguration apiVersion: mirror.openshift.io/v2alpha1 mirror: additionalImages: - name: registry.redhat.io/ubi9/ubi-micro:latest ``` Expected Outcome ```yaml --- apiVersion: config.openshift.io/v1 kind: ImageTagMirrorSet metadata: annotations: createdAt: Thursday, 05-Jun-25 10:00:50 UTC createdBy: oc-mirror v2 oc-mirror_version: v0.2.0-alpha.1-407-g0d8b76a name: itms-generic-0 spec: imageTagMirrors: - mirrors: - sherinefedora:5000/ubi9 source: registry.redhat.io/ubi9 status: {} ```"
                    }
                ]
            },
            "CLID-310": {
                "summary": "As an oc-mirror user, I want to be able to skip signature mirroring",
                "description": "For users that might not have their policy.json and/or registries.d correctly configured, one might want to skip signature verification and mirroring completely. This story doesn' t provide a granular way (per image) way to skip signature mirroring. This story only provides a way to enable/disable signature mirroring as a whole. We need to also verify the behavior behind the existing command line arg secure-policy We need to at least ask PM if other parameters related to signature configuration found in skopeo/podman should also be available in oc-mirror. Ex: {code:java} // This is what skopeo uses to not verify signatures --insecure-policy run the tool without any policy check // This is what skopeo uses to set different locations for policy.json and registries.d --policy string Path to a trust policy file --registries.d DIR use registry configuration files in DIR (e.g. for container signature storage) // This is what skopeo uses to stop copying signatures --remove-signatures Do not copy signatures from SOURCE-IMAGE // these shouldn't be needed. --sign-by FINGERPRINT Sign the image using a GPG key with the specified FINGERPRINT --sign-by-sigstore PATH Sign the image using a sigstore parameter file at PATH --sign-by-sigstore-private-key PATH Sign the image using a sigstore private key at PATH --sign-identity string Identity of signed image, must be a fully specified docker reference. Defaults to the target docker reference. --sign-passphrase-file PATH Read a passphrase for signing an image from PATH {code}",
                "epic_key": "CLID-289"
            },
            "CLID-309": {
                "summary": "As an oc-mirror user, I want cosign signature tags to be mirrored alongside images during disk to mirror workflow",
                "description": "Acceptance criteria When mirroring from disk to mirror, with an imageSetConfig containing an additional signed image, from an archive that was previously verified to contain the signatures corresponding to that image, the mirror registry should contain all signature tags corresponding to the mirored image",
                "epic_key": "CLID-289"
            },
            "CLID-308": {
                "summary": "As an oc-mirror user, I want cosign signature tags to be incrementally saved to archives during mirror to disk workflow",
                "description": "Acceptance criteria when performing a mirror to disk with an empty working-dir, and an imagesetconfig containing a signed additional image, the archive generated contains the signature manifest AND the blobs corresponding to that manifest when performing a mirror to disk with an existing working-dir, and the same imagesetconfig as a previous run from a previous day, the archive doesn't contain signatures that were included in a previous archive",
                "epic_key": "CLID-289"
            },
            "CLID-303": {
                "summary": "Removal of the selected bundles feature",
                "epic_key": "CLID-302",
                "GITHUB": [
                    {
                        "id": "1045",
                        "type": "pullRequest",
                        "title": "CLID-303: disables the selected bundle feature",
                        "body": "Description The selected bundle feature was released in the Tech Preview of oc-mirror v2. This PR disables this feature since it could cause issues when selecting a bundle which does not contains everything necessary to run the operator. Github / Jira issue: CLID-303( Type of change Please delete options that are not relevant. - Bug fix (non-breaking change which fixes an issue) - New feature (non-breaking change which adds functionality) - x Breaking change (fix or feature that would cause existing functionality to not work as expected) - Code Improvements (Refactoring, Performance, CI upgrades, etc) - Internal repo assets (diagrams / docs on github repo) - This change requires a documentation update on openshift docs How Has This Been Tested? Ran the following workflows: `m2d/d2m` `m2m` `delete --generate` `delete` (with and without `--force-cache-delete` flag) ImageSetConfiguration with and without the field `bundles` Expected Outcome All the flows above should successfully pass, except the one without the field `bundles`."
                    }
                ]
            },
            "CLID-246": {
                "summary": "Helm chart signature support",
                "description": "Currently helm chart support in v2 does not mirror and verify signatures, this user story is to implement the mirroring of the signatures and the verification of them.",
                "epic_key": "CLID-289"
            },
            "CLID-347": {
                "summary": "Create defaults configs for signature mirroring/verification",
                "description": "In operating systems (OS) where the registries.d and policy.json does not include our internal registries and the field use-sigstore-attachment: true, it is necessary to have a default embedded in oc-mirror. For oc-mirror cache: {code:java} docker: localhost:55000: use-sigstore-attachments: true{code} For customer regitry (only an example of a registry running on localhost:6000 below) {code:java} docker: localhost:6000: use-sigstore-attachments: true{code} For the release images: {code:java} docker: quay.io: use-sigstore-attachments: true{code} For operator catalog and bundles: {code:java} docker: registry.access.redhat.com: use-sigstore-attachments: true lookaside: {code:java} docker: registry.redhat.io: use-sigstore-attachments: true lookaside: Reference about containers/image policy.json/registries.d:",
                "epic_key": "CLID-289"
            },
            "CLID-307": {
                "summary": "As an oc-mirror user, I want cosign signature tags to be mirrored alongside images during mirror to mirror workflow",
                "description": "Acceptance criteria Signed additional images have all corresponding signature tags available in the mirror registry -Signed catalog images have all corresponding signature tags available in the mirror registry- Signed Operator images (and related) have all corresponding signature tags available in the mirror registry Signed Release images have all corresponding signature tags available in the mirror registry Signed Release payload images have all corresponding signature tags available in the mirror registry A manifest list image shall have a signature tag for the manifest list, and 1 signature tag for each manifest included in the manifest list.",
                "epic_key": "CLID-289",
                "GITHUB": [
                    {
                        "id": "1119",
                        "type": "pullRequest",
                        "title": "CLID-307,CLID-308,CLID-309,CLID-347,CLID-246,CLID-310: feat/adds signatures to the archive",
                        "body": "Description This PR implements the cosign tag-based signature mirroring epic CLID-289( The code changes on this PR allows to mirror all cosign signatures associated with a container image together with the image when copying from source to destination. Github / Jira issue: CLID-307( CLID-308( CLID-309( CLID-347( CLID-246( CLID-310( Type of change Please delete options that are not relevant. - Bug fix (non-breaking change which fixes an issue) - x New feature (non-breaking change which adds functionality) - Breaking change (fix or feature that would cause existing functionality to not work as expected) - x Code Improvements (Refactoring, Performance, CI upgrades, etc) - Internal repo assets (diagrams / docs on github repo) - x This change requires a documentation update on openshift docs How Has This Been Tested? With the following ImageSetConfiguration: ``` kind: ImageSetConfiguration apiVersion: mirror.openshift.io/v2alpha1 mirror: platform: channels: - name: stable-4.18 minVersion: 4.18.1 maxVersion: 4.18.1 graph: true operators: - catalog: registry.redhat.io/redhat/redhat-operator-index:v4.18 packages: - name: aws-load-balancer-operator additionalImages: - name: quay.io/rh_ee_aguidi/multi-platform-container:latest helm: repositories: - name: cosigned url: charts: - name: cosigned version: 0.1.23 ``` Run all the flows (`m2d`/`d2m and `m2m`) Expected Outcome In the destination registry, all the signatures for releases and additional images should have a new tag for the signature with its blobs."
                    }
                ]
            },
            "CLID-301": {
                "summary": "cli: add option to change location of the cache",
                "description": "Currently the location of the cache directory can be set via the environment variable `OC_MIRROR_CACHE`. The only problem is that the env var is not easily discoverable by users. It would be better if we had a command line option (e.g `--cache-dir dir`) which is discoverable via `--help`.",
                "GITHUB": [
                    {
                        "id": "1041",
                        "type": "pullRequest",
                        "title": "CLID-301: add --cache-dir to delete command",
                        "body": "Description This is a fallout of 41c0161728e1b0ed1a63172f36bcc53424dbb09c Github / Jira issue: Type of change Please delete options that are not relevant. - x Bug fix (non-breaking change which fixes an issue) - New feature (non-breaking change which adds functionality) - Breaking change (fix or feature that would cause existing functionality to not work as expected) - Code Improvements (Refactoring, Performance, CI upgrades, etc) - Internal repo assets (diagrams / docs on github repo) - This change requires a documentation update on openshift docs How Has This Been Tested? ``` $ ./bin/oc-mirror --v2 delete --generate --cache-dir /tmp/ -c ~/Downloads/disc.yaml --workspace file://~/Downloads/mirror-v2-data docker://localhost:5000 2025/01/24 12:30:47 INFO : \ud83d\udc4b Hello, welcome to oc-mirror 2025/01/24 12:30:47 INFO : \u2699 setting up the environment for you... 2025/01/24 12:30:47 INFO : \ud83d\udd00 workflow mode: diskToMirror / delete 2025/01/24 12:30:47 INFO : \ud83d\udd75 going to discover the necessary images... 2025/01/24 12:30:47 INFO : \ud83d\udd0d collecting release images... ... ``` Expected Outcome Please describe the outcome expected from the tests."
                    }
                ]
            }
        },
        "epics": {
            "CLID-289": {
                "summary": "As a user I would like to mirror the signatures of the container images",
                "description": "-Open Questions:- -Verifying Third-Party Image Signatures: Support verifying the authenticity and integrity of the non-Red Hat (third-party) image signatures using the public keys.- -Question 1: How complex would it be to allow users to specify the location of their public keys in the configuration file or pass them as arguments?- -Question 2: Is it oc-mirror going to copy the certificate/public key as a resource to the cluster resources folder and ask the customer to apply them?- -Question 3: How about certificates?- Catalog images signatures: scenario when we rebuild the catalog Question 1: The signature of the catalog rebuilt is not like the original one since we changed the image completely, how is it going to work? Is the cluster going to fail because the signature is not the one expected? Support the future OCI 1.1 referrer-based approach: Question 1: Is the container image prioritizing this implementation on their side? Do we already have the Jira issue about this implementation?"
            },
            "CLID-302": {
                "summary": "Bundles feature removal",
                "description": "There was a selected bundle feature on v2 that needs to be removed in 4.18 because of the its risk. An alternative solution is required to unblock one of our customers."
            }
        }
    },
    "OpenShift CFE": {
        "stories": {
            "CFE-1167": {
                "summary": "As a developer, I want to add a new field to openshift/api",
                "description": "Provide a new field to the CPMS that allows to define a Machine name prefix",
                "epic_key": "OAPE-16",
                "GITHUB": [
                    {
                        "id": "2086",
                        "type": "pullRequest",
                        "title": "CFE-1167: Feature-gated MachineNamePrefix field for CPMS",
                        "body": "This PR - Introduces `MachineNamePrefix` field in `ControlPlaneMachineSet` allowing custom prefixes to be used for Control Plane Machine names. - This feature is gated behind the `CPMSMachineNamePrefix` feature gate. - Feature gate PR : - This PR fixed the feature-gate naming typo. - Implements - Part of CFE-1167("
                    }
                ]
            },
            "CFE-1168": {
                "summary": "As a developer, I want to add a new feature gate in openshift/api",
                "description": "Define a new feature gate in openshift/api for this feature so that all the implementation can be safe guarded behind this gate.",
                "epic_key": "OAPE-16",
                "GITHUB": [
                    {
                        "id": "2094",
                        "type": "pullRequest",
                        "title": "CFE-1168: Add CMPSMachineNamePrefix feature-gate",
                        "body": "Add new `CMPSMachineNamePrefix` feature-gate to guard the implementation changes done to support customized control plane machine names. - Part of CFE-1168( - Implements"
                    }
                ]
            }
        }
    },
    "OpenShift Cloud Credential Operator": {
        "stories": {
            "CCO-647": {
                "summary": "Enable readOnlyRootFilesystem on all pods",
                "description": "Enable readOnlyRootFilesystem on all of the cloud-credential-operator pods. This will require reverting prior changes that caused the tls-ca-bundler.pem to be mounted in a temporary location and then moved to the default location as part of the cloud-credential-operator pod's command.",
                "epic_key": "CCO-385",
                "GITHUB": [
                    {
                        "id": "819",
                        "type": "pullRequest",
                        "title": "CCO-647: Enable readOnlyRootFilesystem on all containers",
                        "body": "The readOnlyRootFilesystem is now explicitly set to True on all containers. In order to for this to work on the cloud-credential-operator container, the tls-ca-bundle.pem mount being reverted to the default location."
                    }
                ]
            },
            "CCO-631": {
                "summary": "Upgrade to Kubernetes 1.32",
                "description": "As a developer, I want to upgrade the Kubernetes dependencies to 1.32 to ensure compatibility with the OpenShift cluster",
                "epic_key": "CCO-627",
                "GITHUB": [
                    {
                        "id": "814",
                        "type": "pullRequest",
                        "title": "CCO-631: Upgrade to k8s v0.32.0"
                    }
                ]
            },
            "CCO-629": {
                "summary": "Update vendor dependencies to latest",
                "description": "As a developer, I want to update all go dependencies: to reduce the risk of security vulnerabilities to reduce the risk of incompatibility when handling urgent updates such as CVEs Note: As this is the first time we are doing this, we should provide best-effort here by upgrading the dependencies that have no conflict, and creating cards for the ones that need more effort. Those cards can be linked to the 4.20 Regular Maintenance epic if/when created so we can better plan for the additional effort.",
                "epic_key": "CCO-628",
                "GITHUB": [
                    {
                        "id": "817",
                        "type": "pullRequest",
                        "title": "CCO-629: Update modules to latest"
                    }
                ]
            },
            "CCO-626": {
                "summary": "Log diff on CredentialsRequest status change",
                "description": "When the CCO updates a CredentialsRequest's status, the current logs are not clear on what's changing: {code:none} time=\"2024-12-05T21:44:49Z\" level=info msg=\"status has changed, updating\" controller=credreq cr=openshift-cloud-credential-operator/aws-ebs-csi-driver-operator secret=openshift-cluster-csi-drivers/ebs-cloud-credentials {code} We should make it possible to get the CCO to log the diff it's trying to push, even if that requires bumping the operator's log level to debug. That would make it easier to understand hotloops like OCPBUGS-47505.",
                "GITHUB": [
                    {
                        "id": "811",
                        "type": "pullRequest",
                        "title": "CCO-626: pkg/operator/utils: Log diff on CredentialsRequest status change",
                        "body": "`status has changed, updating` shows that something is changing. But without a diff, it's hard to figure out what. For example in this recent CI run1: ```console $ curl -s :"
                    }
                ]
            }
        },
        "epics": {
            "CCO-385": {
                "summary": "readOnlyRootFilesystem should be explicitly to true and if required to false for security reason",
                "description": "_1. Proposed title of this feature request_ openshift-cloud-credential-operator - readOnlyRootFilesystem should be explicitly to true and if required to false for security reason _2. What is the nature and description of the request?_ According to security best practice, it's recommended to set readOnlyRootFilesystem: true for all containers running on kubernetes. Given that openshift-cloud-credential-operator does not set that explicitly, it's requested that this is being evaluated and if possible set to readOnlyRootFilesystem: true or otherwise to readOnlyRootFilesystem: false with a potential explanation why the file-system needs to be write-able. _3. Why does the customer need this? (List the business requirements here)_ Extensive security audits are run on OpenShift Container Platform 4 and are highlighting that many vendor specific container is missing to set readOnlyRootFilesystem: true or else justify why readOnlyRootFilesystem: false is set. _4. List any affected packages or components._ openshift-cloud-credential-operator"
            },
            "CCO-627": {
                "summary": "Upgrade to Kubernetes 1.32",
                "description": "Epic Goal The goal of this epic is to upgrade all OpenShift and Kubernetes components that CCO uses to v1.32 which keeps it on par with rest of the OpenShift components and the underlying cluster version. Why is this important? To make sure that Hive imports of other OpenShift components do not break when those rebase To avoid breaking other OpenShift components importing from CCO. To pick up upstream improvements Acceptance Criteria CI - MUST be running successfully with tests automated Dependencies (internal and external) Kubernetes 1.32 is released Previous Work (Optional): Similar previous epic CCO-595 Done Checklist CI - CI is running, tests are automated and merged."
            },
            "CCO-628": {
                "summary": "4.19 Regular Maintenance",
                "description": "Epic Goal Update all golang dependencies Ensure periodic CI jobs for new version Sunset periodic CI jobs for version(s) no longer supported Why is this important? To ensure we are using latest vendor code To reduce security vulnerabilities in vendor code To ensure we are regularly testing the latest version To reduce costs from testing old, unsupported versions. Scenarios ... Acceptance Criteria ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Autoscaling": {
        "stories": {
            "AUTOSCALE-193": {
                "summary": "use aws-karpenter-provider-aws image in HCP autonode",
                "description": "Currently, the HCP autonode implementation uses an upstream aws image for karpenter-aws-provider: We need to be able to propagate the current karpenter-provider-aws image associated with the release payload into the HCP autonode karpenter deployment once we lifecycle the image. The hardcoded image \"public.ecr.aws/karpenter/controller:1.0.7\" is at version 1.0.7, while our fork was forked off a few minor versions after that. We also need to resolve any bugs/breaking changes that arise when using the new image with autonode.",
                "epic_key": "AUTOSCALE-36",
                "GITHUB": [
                    {
                        "id": "6029",
                        "type": "pullRequest",
                        "title": "AUTOSCALE-193: Use payload karpenter image if present, otherwise don't",
                        "body": "What this PR does / why we need it: This PR syncs karpenter libs with our downstream fork, and makes the hypershift karpenter operator check the payload for an `aws-karpenter-provider-aws` image to use to deploy karpenter Why: - hypershift is currently hard-coded to use upstream karpenter-provider aws 1.07 - We want to ship with our downstream `aws-karpenter-provider-aws` version that will be packed into the payload (which is approximately ~1.3.2~ 1.2.3, and is broken, but we are fixing it: h - It's not in the payload yet but we're working on it, so we can't hard cut to always looking in the payload - We want to be able to run hypershift tests over in How: - adds a replace directive to go.mod to point the karpenter-aws-libraries to our openshift fork and bumps the required deps - syncs the CRDs with our downstream fork and (ours are newer than 1.0.7/1.0.8, fields were missing) and run `adjust-cel.sh` against them - adds logic to retrieve the karpenter provider from the `aws-karpenter-provider-aws` image in the payload image if present, and if not it falls back to the upstream image - adds plumbing to the karpenter-operator so it accepts a karpenter image argument Notes: - I did test this, and it passed the smoke test, I was able to deploy karpenter and scale. We'll see what we get in CI :smile: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "AUTOSCALE-163": {
                "summary": "The most basic e2e to gate upcoming work",
                "description": "Set up a mininum presubmit gate",
                "epic_key": "AUTOSCALE-28"
            },
            "AUTOSCALE-159": {
                "summary": "Implement HCP karpenter deletion logic",
                "description": "As an OpenShift HCP management cluster admin, when I delete a HCP cluster with autonode on, I would also want to make sure any provisioned nodes are removed from the infrastructure. We need to make sure Karpenter related objects are not blocking the deletion or would result in resource leakage. This story should capture the flow described in and implements cascading deletion login in the HCP Karpenter operator. This should also include e2e test(s) and relevant unit tests which make sure a HCP cluster with autonode on gets successfully deleted.",
                "epic_key": "AUTOSCALE-31"
            },
            "AUTOSCALE-127": {
                "summary": "create release job to exercise e2e tests for karpenter-provider-aws",
                "epic_key": "AUTOSCALE-36",
                "GITHUB": [
                    {
                        "id": "6062",
                        "type": "pullRequest",
                        "title": "AUTOSCALE-127: fix e2e autonode drift test",
                        "body": "Fixes the test by grabbing the correct part of the string from node.Status.NodeInfo.OSImage that now points to the rhcos version. See as to why we need this. !-- - Please ensure code changes are split into a series of logically independent commits. - Every commit should have a subject/title (What) and a description/body (Why). - Every PR must have a description. - As an example you can use git commit -m\"What\" -m\"Why\" to achieve the requirements above. GitHub automatically recognises the commit description (-m\"Why\") in single commit PRs and adds it as the PR description. - Use the imperative mood( in the subject line for every commit. E.g `Mark infraID as required` instead of `This patch marks infraID as required` (This follows Git\u2019s own built-in conventions). See as an example. - See for more details. Delete this text before submitting the PR. -- What this PR does / why we need it: Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes Checklist - Subject and description added to both, commit and PR. - Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    },
                    {
                        "id": "5999",
                        "type": "pullRequest",
                        "title": "AUTOSCALE-127: allow karpenter-provider-aws image to be overriden by hcp annotation",
                        "body": "Allows the karpenter-provider-aws image to be overriden by an HCP annotation. ~We need this in order to test/e2e our OpenShift karpenter-provider-aws image since it's not in the release payload yet.~ This is a helpful quality of life change to allow devs to test different aws-karpenter-provider versions with HyperShift autonode. Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            },
            "AUTOSCALE-112": {
                "summary": "add test for basic scale out",
                "description": "if possible we might be able to reuse upstream tests, based on decisions from PODAUTO-323.",
                "epic_key": "AUTOSCALE-28"
            },
            "AUTOSCALE-44": {
                "summary": "e2e testing automation: Implement AutoNode upgrades via karpenter drift/consolidation",
                "description": "As a developer I want to make sure the Hypershift hosted cluster upgrades induce karpenter drift and the RHCOS version and Kubernetes version of the node match the versions of the release payload that we were upgrading the hosted cluster to. I also want to make sure the workloads on the old node are moved onto the new node. We've figured out that karpenter drift with hypershift cluster upgrades works already as part of AUTOSCALE-1, now we have to e2e ci test it. Since this is a feature that tests node upgrades inside a guest cluster, we can only test it from the hypershift side. We would probably stick the test here: We need to create a hypershift e2e test (or if possible, integrate it into the existing test) that follows some flow like this: creates a hosted cluster that points to n-1 release version (by e2eutil.NewHypershiftTest) creates and scales a workload that starts a karpenter node and schedules the workload take note of the kubernetes version and the rhcos version of the node upgrades the hosted cluster release image to version n wait until the new nodeclaim and node are created and schedulable make sure the k8s versions and rhcos version match the expected version of release image n wait until the previous workload is scheduled to the new node, fail if it doesn't within a timeout",
                "epic_key": "AUTOSCALE-23",
                "GITHUB": [
                    {
                        "id": "5870",
                        "type": "pullRequest",
                        "title": "AUTOSCALE-44: add autonode karpenter drift hcp upgrade e2e test",
                        "body": "What this PR does / why we need it: This PR adds a new test to the karpenter e2e suite which upgrades the hcp and validates that karpenter nodes are drift and upgraded to the control-plane's new release image version. Also validates that any pods that were already scheduled to the old karpenter node, will be rescheduled to the new karpenter node. Also this PR lowers the CPU of the tested workload to avoid them being unschedulable because of the small CPU allocation of `t3.large` instances. Which issue(s) this PR fixes (optional, use `fixes issue_number(, fixes issue_number, ...)` format, where issue_number might be a GitHub issue, or a Jira story: Fixes: Checklist - x Subject and description added to both, commit and PR. - x Relevant issues have been referenced. - This change includes docs. - This change includes unit tests."
                    }
                ]
            }
        },
        "epics": {
            "AUTOSCALE-36": {
                "summary": "Mirror karpenter-aws repo and include it in OCP Payload",
                "description": "Goal Mirror the karpenter aws source code within openshift GH org Agree and automate a rebase cadence Include the image build within the OCP payload. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "AUTOSCALE-28": {
                "summary": "Setup e2e testing for techpreview API and add e2e for autoNode via karpenter",
                "description": "Goal Have a CI pipeline that runs HO with --tech-preview Add e2e test for autoNode via karpenter that validates: all manifest are created as expected management and guest side. Karpenter is able to autoprovision and remove compute. Why is this important? ... Scenarios ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "AUTOSCALE-31": {
                "summary": "Implement HCP karpenter deletion",
                "description": "Goal The goal of this epic is to support deletion of Karpenter provisioned nodes and their corresponding instances, when a HyperShift HostedCluster is torn down. The end goal is that all infrastructure backed instances are automatically fully removed from their infrastructure when deletion of the HostedCluster is finished. A subgoal includes allowing metrics, alerts, and events to be emitted during teardown. This epic is a part of the strategic feature work for OpenShift AutoNode: Why is this important? This is important because a user will expect all related resources corresponding to a HostedCluster is deleted when it is torn down. We need to specially care for Karpenter instances since they are being provisioned outside of the cluster's environment and being registered with the cluster afterwards. That means we will need to delete them from the infrastructure during teardown, without potentially leaking resources. It is also important that deletion deadlocks are minmized so that users are not stuck during deletion for an excessive amount of time. Additionally, metrics, events, and alerts will allow cluster-admins to diagnose any potential problems related to Karpenter/AutoNode during the tear down phase, and allow them to safely deprovision the cluster. Scenarios A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster. A cluster admin creates a HostedCluster with AutoNode enabled, creates some workloads on the cluster which initiate Karpenter provisioning of nodes, and then deletes the cluster, but the deletion is timed out due to some issue in the deletion process. Acceptance Criteria Dev - Deletion implementation has been merged, and metrics, alerts, events, etc. have been added. Dev - Upstream docs are merged that include document the deletion process, and steps to debug a stuck/failed deletion CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented (created hypershift-hosted cluster with AutoNode on, create some workloads, delete hosted cluster, make sure karpenter provisioned instances are deleted from infrastructure) Release Technical Enablement - Must have TE slides Dependencies (internal and external) None Previous Work (Optional): None Open questions: None for now. Some questions were covered by this spike: Done Checklist CI - CI is running, tests are automated and merged. link to tests in openshift/release Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: N/A"
            },
            "AUTOSCALE-23": {
                "summary": "Implement AutoNode upgrades via karpenter drift/consolidation",
                "description": "Goal Define upgrade criteria for Karpenter Nodes (E.g. follow the control plane, this can be configurable at the HC level so the services can make their choice) Implement it relying on native Drift and Consolidation. Why is this important? Reduce operational burden Scenarios With AutoNode via karpenter the Service is authoritative to manage upgrades of karpenter Nodes. We need to agree on 1..N criteria/strategies. Possibly expose them in the HC API and let them be driven via Drift/Consolidation. Known caveats: in the current prototype everytime the ignition token is rotated would cause drift as a side effect. We'll need to either make it configurable or somehow transparent for drift ... Acceptance Criteria Dev - Has a valid enhancement if necessary CI - MUST be running successfully with tests automated QE - covered in Polarion test plan and tests implemented Release Technical Enablement - Must have TE slides ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Technical Enablement link to Feature Enablement Presentation DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Enhancement merged: link to meaningful PR or GitHub Issue QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift Authentication": {
        "stories": {
            "AUTH-541": {
                "summary": "Structured authentication configuration for the KAS pods",
                "description": "The CAO and KAS-o both need to work and enable structured authentication configuration for the KAS static pods. CAO: - a controller tracks the auth CR for auth type OIDC - generates structured auth config object and serializes it into a configmap - syncs the configmap into openshift-config KAS-o: - a config observer tracks the auth CR for type OIDC - syncs the auth configmap from openshift-config into openshift-kube-apiserver and enables the `--authentication-config` CLI arg for the KAS pods - the auth-metadata and webhook-authenticator config observers remove their resources and CLI args accordingly - a revision controller syncs that configmap into a static file",
                "epic_key": "CNTRLPLANE-80",
                "GITHUB": [
                    {
                        "id": "713",
                        "type": "pullRequest",
                        "title": "AUTH-541: OIDC structured auth config",
                        "body": "This PR adds a controller behind the `ExternalOIDC` feature gate that tracks the auth CR, and when auth type is configured to be OIDC, it: - creates a structured auth config object based on the auth CR and validates it - serializes it into JSON and stores it into a configmap - syncs that configmap into `openshift-config-managed`, where it will be picked up by the KAS-o and synced into a static file and passed on to the KAS pods KAS-o counterpart PR: Enhancement:"
                    },
                    {
                        "id": "1760",
                        "type": "pullRequest",
                        "title": "AUTH-541: OIDC structured auth config",
                        "body": "This PR does the following: - adds unit tests for the `AuthMetadata` config observer - adds a new config observer for direct OIDC; the config observer: - detects auth type `OIDC` and the structured auth config CM in `openshift-config-managed` (created by the CAO in cluster-authentication-operator713( - copies it to `openshift-kube-apiserver` where it will be used as a revisioned configmap and synced to a static file on all KAS nodes - enables OIDC config via the `--authentication-config` CLI flag of the KAS pods - modifies the `AuthMetadata` and `WebhookTokenAuthenticator` config observers to delete their respective resources when auth type `OIDC` is detected Enhancement:"
                    }
                ]
            },
            "AUTH-555": {
                "summary": "Merge kube-rbac-proxy v0.18.2 into downstream",
                "description": "What Merge upstream kube-rbac-proxy v0.18.2 into downstream. Why We don't want to log the tokens even if the verbosity is set to high",
                "epic_key": "CNTRLPLANE-882",
                "GITHUB": [
                    {
                        "id": "114",
                        "type": "pullRequest",
                        "title": "AUTH-555: Merge upstream v0.18.2 release",
                        "body": "WHAT Releasing token changes to mask them even with the high verbosity in logs Dependency bumps WHY We don't want to log the tokens even if the verbosity is set to high"
                    }
                ]
            }
        }
    },
    "Agent-based Installer for OpenShift": {
        "stories": {
            "AGENT-1159": {
                "summary": "Internal docs update",
                "description": "Update the internal installer documentation to reflect the changes on the agent services",
                "epic_key": "AGENT-408",
                "GITHUB": [
                    {
                        "id": "9627",
                        "type": "pullRequest",
                        "title": "AGENT-1159: agent docs update",
                        "body": "This patch describes the new interactive workflow and updated accordingly the existing ones"
                    }
                ]
            },
            "AGENT-1151": {
                "summary": "Use internal appliance registry",
                "description": "Recently the appliance allowed using an internal registry (see Modify the script to use that (instead of the external one), and test the installation workflow.",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "58",
                        "type": "pullRequest",
                        "title": "AGENT-1151: Use internal appliance registry",
                        "body": "Appliance allows using an internal registry (see Use that (instead of the external one)."
                    }
                ]
            },
            "AGENT-1150": {
                "summary": "Move setup-agent-tui.sh into ignition file",
                "description": "Currently the builder script embeds the agent-setup-tui.service in the ignition files, but the script directly in the ISO. For consistency, also the script should be placed inside the ISO ignition",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "59",
                        "type": "pullRequest",
                        "title": "AGENT-1150: Move setup-agent-tui.sh into ignition file",
                        "body": "Move setup-agent-tui.sh into ignition file. Also, log time taken to generate the ISO."
                    }
                ]
            },
            "AGENT-1137": {
                "summary": "Allow using nightlies/CI release payloads for testing",
                "description": "The current builder script must be able to ingest a nightly/CI release payload for local and CI testing. This also means that the appliance tool should be able to mirror the images from a nightly/CI payload (except maybe for the operators)",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "49",
                        "type": "pullRequest",
                        "title": "AGENT-1137: Support both --ocp-version and --release-image-url",
                        "body": "Added support to specify OCP version via --ocp-version or extract version from --release-image-url. - --release-image-url is mainly for testing/debugging non-released version - Updated input validation to ensure only one of --ocp-version or --release-image-url is provided. - Adds channel and cpuArchitecture in appliance-config only if --ocp-version is provided. - Modified appliance config creation to dynamically include the OCP version based on inputs. - Improved error handling and default architecture handling. - Enhanced usage instructions to reflect new input options and examples."
                    }
                ]
            },
            "AGENT-1119": {
                "summary": "Add assisted UI to ove builder script",
                "description": "The ove builder script must include any required assisted UI artifact into the generated ISO",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "48",
                        "type": "pullRequest",
                        "title": "AGENT-1119: agent-setup-tui.service typo fix"
                    }
                ]
            },
            "AGENT-1118": {
                "summary": "Add agent TUI to ove builder script",
                "description": "The ove builder script should include all the necessary agent TUI artifacts within the generated ISO",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "57",
                        "type": "pullRequest",
                        "title": "AGENT-1118: Ensure the correct selinux labels are applied to agent tui files",
                        "body": "This also avoids modifying the original /usr/local/bin labels, since it could have an impact in a later stage during the installation"
                    }
                ]
            },
            "AGENT-464": {
                "summary": "Expand TUI with a form that asks whether the node should be node0 or not",
                "description": "User Story: As an admin, I want to be able to: Have an interactive generic installation image that I can use for all nodes. Since it is a single image for all the nodes, I need to be able to select on boot whether the node is node0 (and future master) or a regular node. Have the TUI checks take into account whether the node is node0 or not to perform additional checks (like connectivity check to the rendenzvous IP) so that I can achieve Interactive installation with a single image Acceptance Criteria: Description of criteria: A dialog is presented on boot asking whether this node should be the one that controls the installation (node0) On regular nodes additional connectivity checks are performed towards rendezvous IP TUI writes Node0 configuration so the blocked node0 services can proceed (after network configuration and registry checks) Engineering Details: There is a PoC of this dialog in Final dialog in use with ABI: (?) This does not require a design proposal. (?) This does not require a feature gate.",
                "epic_key": "AGENT-387",
                "GITHUB": [
                    {
                        "id": "42",
                        "type": "pullRequest",
                        "title": "AGENT-464: Refactor UI for Rendezvous node IP entry",
                        "body": "Added confirmation screens after the Rendezvous node IP is saved. Instead of a checkbox to \"randomly\" choose of the Node's IP as the Rendezvous node IP, a list of all of this node's IP addresses is presented which allows the user to choose the exact one they want. Added additional descriptions on the form to make the process more understandable. Depends-on:"
                    },
                    {
                        "id": "38",
                        "type": "pullRequest",
                        "title": "AGENT-464: Adds form to agent-tui to enter rendezvous IP address",
                        "body": "This change supports a new user scenario whereby the rendezvous IP address may not be written into the agent ISO. In this case, the agent-tui should detect if the NODE_ZERO_IP is specified in /etc/assisted/rendezvous-host.env. If it is unspecified, the tui should display a form to allow the user to enter the rendezvous host IP. There will be a supporting change in the installer repo to pass the NODE_ZERO_IP from /etc/assisted/rendezvous-host.env to the agent-interactive-console.service which executes agent-tui during boot. If NODE_ZERO_IP is not set in the environment, agent_tui reads /etc/assisted/rendezvous_host.env for the value."
                    }
                ]
            },
            "AGENT-1188": {
                "summary": "Make ISO USB-bootable",
                "description": "Since the ISO containing the entire release image is {_}HUGE{_}, many users will likely want to copy it to a USB drive rather than mounting it as virtualmedia through the BMC. USB mass storage devices require a master boot record to be bootable, unlike optical drives which use the El Torito ISO9660 extension to make them bootable. To turn add an MBR to an El Torito ISO to make it bootable in either mode, we can do: {code:bash} isohybrid --uefi agent.x86_64.iso {code} Based on how CoreOS does this when building the RHCOS live ISO, it's possible that this is only available on x86-64:",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "72",
                        "type": "pullRequest",
                        "title": "AGENT-1188: Add MBR to El Torito ISO to support USB boot alongside VM",
                        "body": "The release ISO includes the entire release image and is often large, so many users could prefer writing it to a USB drive instead of mounting it via BMC virtual media. While optical drives can boot El Torito ISOs directly, USB mass storage devices require a Master Boot Record (MBR) to be bootable. This change makes the ISO hybrid-bootable by appending an MBR, allowing it to boot via both USB and virtual media without requiring separate ISO builds or formats."
                    }
                ]
            },
            "AGENT-1154": {
                "summary": "Internal documentation",
                "description": "Add a README file in the builder script folder to document internally its usage",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "74",
                        "type": "pullRequest",
                        "title": "AGENT-1154: Dev docs for ISOBuilder"
                    }
                ]
            },
            "AGENT-1114": {
                "summary": "TUI checks connectivity to rendezvous node",
                "description": "User Story: As a (user persona), I want to be able to: enter the rendezvous host ip in the agent_tui so that I can achieve have the agent_tui check the current host has connectivity to the rendezvous host an error dialog is displayed if there is no connectivity Acceptance Criteria: Description of criteria: Upstream documentation Point 1 Point 2 Point 3 (optional) Out of Scope: Detail about what is specifically not being delivered in the story Engineering Details: (optional) (optional) Engineering detail 1 Engineering detail 2 (?) This requires/does not require a design proposal. (?) This requires/does not require a feature gate.",
                "epic_key": "AGENT-387",
                "GITHUB": [
                    {
                        "id": "64",
                        "type": "pullRequest",
                        "title": "AGENT-1114: Check connectivity to rendezvous node and improvements",
                        "body": "Adds connectivity checks from current host to rendezvous node. Add a blank line between IP addresses and Back and Configure Network buttons. Update the select IP address list navigation, so that the blank line is not selectable. Made invalid_ip_address_modal generic so that it can be reused in multiple situations."
                    }
                ]
            },
            "AGENT-1113": {
                "summary": "Add create interactive-disconnected-ignition sub command to openshift-installer",
                "description": "This sub-command will be used to generate the ignition file based on the interactive disconnected workflow. This command will be invoked by the builder script (currently within the appliance tool) for supporting generating the ISO. It will also consume, in future, the eventual (portion of) install configuration that the user will provide via the connected UI (above the sea level)",
                "epic_key": "AGENT-1086",
                "GITHUB": [
                    {
                        "id": "9565",
                        "type": "pullRequest",
                        "title": "AGENT-1113: additional refinement for loading the agent UI",
                        "body": "This patch is refiniment followup on"
                    },
                    {
                        "id": "9529",
                        "type": "pullRequest",
                        "title": "AGENT-1113: enhance the unconfigured-ignition command to support the interactive workflow",
                        "body": "This patch adds a new flag `--interactive` to the `unconfigured-ignition` command to support the interactive disconnected workflow. It is possible to specify the rendezvous IP via the agent-config.yaml file. The interactive workflow spins up the assisted-ui after the assisted-service (and it's mutually exclusive with the automatic workflow services). Required by: Related to:"
                    }
                ]
            },
            "AGENT-537": {
                "summary": "OpenShift installer on demand generation of just the certificates and credentails for installation",
                "description": "In order to perform interactive installation, we need to expose the OpenShift installer generation of certificates, Kubeadmin password kubeconfig This can either be done by exposing it as some subcommand of openshift-install or exposing the relevant functions in some of the installer golang packages. This needs to be called by a new endpoint in assisted service that REST can call to trigger the generation",
                "epic_key": "AGENT-408",
                "GITHUB": [
                    {
                        "id": "9574",
                        "type": "pullRequest",
                        "title": "AGENT-537: Run agent installer command to create certificates",
                        "body": "If the agent tls certificates have not been created yet, i.e. when using the agent UI, use the installer command to create the certs prior to running the assisted-service. Requires"
                    },
                    {
                        "id": "9557",
                        "type": "pullRequest",
                        "title": "AGENT-537: Add agent command to generate certificates",
                        "body": "Add a new hidden agent-based-installer command to generate tls certificates. This will be used by the agent UI. ``` $ ./bin/openshift-install agent create certificates INFO Certificates created in: tls $ ls tls admin-kubeconfig-signer.crt kube-apiserver-lb-signer.crt kube-apiserver-localhost-signer.crt kube-apiserver-service-network-signer.crt admin-kubeconfig-signer.key kube-apiserver-lb-signer.key kube-apiserver-localhost-signer.key kube-apiserver-service-network-signer.key ```"
                    }
                ]
            },
            "AGENT-467": {
                "summary": "TUI shows WebUI URLs once the assisted service is available",
                "description": "User Story: As an admin, I want to be able to: See the WebUI available URLs (if there are multiple addresses, it could be multiple URLs) in the TUI once all the checks have passed and the backend services are running so that I can achieve Successful connection to the WebUI to continue with the interactive installation Acceptance Criteria: Description of criteria: The WebUI dialog updates to show the URLs in a prominent place for the user to be aware that they can already connect to proceed with interactive installation (the URLs, if multipe, should probably be sorted putting the addresses that have default gateway first) Engineering Details: (?) This does not require a design proposal. (?) This does not require a feature gate.",
                "epic_key": "AGENT-408",
                "GITHUB": [
                    {
                        "id": "9584",
                        "type": "pullRequest",
                        "title": "AGENT-467: show the agent ui url when available",
                        "body": "This patch shows the UI url in the rendezvous node console when available."
                    }
                ]
            }
        },
        "epics": {
            "AGENT-408": {
                "summary": "GUI backend services",
                "description": "Epic Goal Have a friendly graphical user to perform interactive installation that runs on node0 Why is this important? Allows the WebUI to run in _Agent based installation_ where we can only count on node0 to run it Provides a familiar (close to SaaS) interface to walk through the first cluster installation Interactive installation takes us closer to having generated images that serve multiple first cluster installations Scenarios As an admin, I want to generate an ISO that I can send to the field to perform a friendly, interactive installation Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) Assisted-Service WebUI needs an _Agent based installation_ wizard Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "AGENT-1086": {
                "summary": "OVE release image generation",
                "description": "Epic Goal Setup a workflow to generate an ISO that will contain all the relevant pieces to install an OVE cluster Why is this important? As per OCPSTRAT-1874, the user must be able to install into a disconnected environment an OVE cluster, with the help of a UI, and without requiring explicitly to setup an external registry Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Previous work: Dependencies (internal and external) ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "AGENT-387": {
                "summary": "Interactively configure the rendezvous address",
                "description": "Epic Goal Allow the user to select a host to be Node 0 interactively after the booting the ISO. On each host the user would be presented with a choice between two options: Select this host as the rendezvous host (it will become part of the control plane) The IP address of the rendezvous host is: Enter IP (If the former option is selected, the IP address should be displayed so that it can be entered in the other hosts.) Why is this important? Currently, when using DHCP the user must determine which IP address is assigned to at least one of the hosts prior to generating the ISO. (OpenShift requires infinite DHCP leases anyway, so no extra configuration is required but it does mean trying to manually match data with an external system.) AGENT-385 would extend a similar problem to static IPs that the user is planning to configure interactively, since in that case we won't have the network config to infer them from. We should permit the user to delay collecting this information until after the hosts are booted and we can discover it for them. Scenarios In a DHCP network, the user creates the agent ISO without knowing which IP addresses are assigned to the hosts, then selects one to act as the rendezvous host after booting. Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) AGENT-7 Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            },
            "AGENT-587": {
                "summary": "Add Nutanix platform integration",
                "description": "OCP/Telco Definition of Done Epic Goal ... Why is this important? ... Scenarios ... Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) ... Previous Work (Optional): ... Open questions: ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR",
                "GITHUB": [
                    {
                        "id": "9537",
                        "type": "pullRequest",
                        "title": "AGENT-587: Support Nutanix in Agent-based Installer",
                        "body": "AGENT-587( Add Nutanix platform integration OCPSTRAT-367( Support Nutanix in Agent-based Installer"
                    }
                ]
            }
        }
    },
    "On Prem Networking": {
        "stories": {
            "OPNET-629": {
                "summary": "Improve HAProxy monitor robustness",
                "description": "This is a followup to to make the monitor resilient in all configurations, including things like 5 node control planes. Instead of relying on a longer fall time, we can just let HAProxy report its own ability to reach any backend, which means under ordinary circumstances this check will never fail. There will no longer be any issue with pathologically bad call chains where we happen to hit backends that are down but haven't been detected yet.",
                "epic_key": "OPNET-579",
                "GITHUB": [
                    {
                        "id": "343",
                        "type": "pullRequest",
                        "title": "OPNET-629: Use HAProxy monitor endpoint instead of API",
                        "body": "Same as 336 which was reverted because it merged before a dependency in MCO."
                    },
                    {
                        "id": "342",
                        "type": "pullRequest",
                        "title": "Revert \"OPNET-629: Use HAProxy monitor endpoint instead of API\"",
                        "body": "Reverts openshift/baremetal-runtimecfg336"
                    },
                    {
                        "id": "336",
                        "type": "pullRequest",
                        "title": "OPNET-629: Use HAProxy monitor endpoint instead of API",
                        "body": "This is the runtimecfg change corresponding to which switches the monitor call to the HAProxy endpoing rather than call through to the API. I also included a commit to bump the Dockerfile image to golang 1.22 to match the current state of the code."
                    },
                    {
                        "id": "4767",
                        "type": "pullRequest",
                        "title": "OPNET-629: Mark haproxy unhealthy if no healthy backends",
                        "body": "Previously we avoided doing this because of potential issues in unhealthy clusters where backends were flapping and we didn't want to trigger failovers. However, given the nature of the firewall rule monitor check that approach was not effective anyway and allowing HAProxy to report its own status to the monitor is much more robust than relying on API calls being routed correctly when API rollouts are happening. This is being implemented as a separate monitor endpoint because we don't want the Kubelet liveness probes to fail just because there are no backends (which is an expected state in early cluster deployment). That would trigger unnecessary crash loops. !-- If this is a bug fix, make sure your description includes \"Fixes: xxxx\", or \"Closes: xxxx\" Please provide the following information: -- - What I did - How to verify it - Description for the changelog !-- Write a short (one line) summary that describes the changes in this pull request for inclusion in the changelog: --"
                    }
                ]
            }
        },
        "epics": {
            "OPNET-579": {
                "summary": "Unplanned work for 4.18",
                "description": "Template: Networking Definition of Planned Epic Goal Track work that needs to happen in 4.18 but was not part of the original planning. Why is this important? Planning Done Checklist The following items must be completed on the Epic prior to moving the Epic from Planning to the ToDo status - (-) Priority+ is set by engineering - (-) +Epic must be Linked to a +Parent Feature+ - (-) Target version+ must be set - (-) Assignee+ must be set - (-) (Enhancement Proposal is Implementable - (-) (No outstanding questions about major work breakdown - (-) (Are all Stakeholders known? +Have they all been notified about this item?+ - (-) +Does this epic affect SD?+ {}Have they been notified\\{+}? (View plan definition for current suggested assignee) Please use the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox to facilitate the conversation with SD Architects. The SD architecture team monitors this checkbox which should then spur the conversation between SD and epic stakeholders. Once the conversation has occurred, uncheck the \"Discussion Needed: Service Delivery Architecture Overview\" checkbox and record the outcome of the discussion in the epic description here. The guidance here is that unless it is very clear that your epic doesn\u2019t have any managed services impact, default to use the Discussion Needed checkbox to facilitate that conversation. Additional information on each of the above items can be found here: Networking Definition of Planned| Acceptance Criteria CI - MUST be running successfully with tests automated Release Technical Enablement - Provide necessary release enablement details and documents. ... Dependencies (internal and external) 1. ... Previous Work (Optional): 1. ... Open questions: 1. ... Done Checklist CI - CI is running, tests are automated and merged. Release Enablement link to Feature Enablement Presentation DEV - Upstream code and tests merged: link to meaningful PR or GitHub Issue DEV - Upstream documentation merged: link to meaningful PR or GitHub Issue DEV - Downstream build attached to advisory: link to errata QE - Test plans in Polarion: link or reference to Polarion QE - Automated tests merged: link or reference to automated tests DOC - Downstream documentation merged: link to meaningful PR"
            }
        }
    },
    "OpenShift BuildConfig": {
        "stories": {
            "OCPBUILD-174": {
                "summary": "Bring openshift/builder Contributor Docs Up to Date",
                "description": "User Story As a developer looking to contribute to OCP BuildConfig I want contribution guidelines that make it easy for me to build and test all the components. Background Much of the contributor documentation for openshift/builder is either extremely out of date or buggy. This hinders the ability for newcomers to contribute. Approach Document dependencies needed to build openshift/builder from source. Update \"dev\" container image for openshift/builder so teams can experiment locally. Provide instructions on how to test \"WIP Pull Request\" process \"Disable operators\" mode. Red Hatter instructions: using cluster-bot Acceptance Criteria New contributors can compile openshift/builder from GitHub instructions New contributors can test their code changes on an OpenShift instance Red Hatters can test their code changes with cluster-bot.",
                "GITHUB": [
                    {
                        "id": "414",
                        "type": "pullRequest",
                        "title": "OCPBUILD-174: Clean Up Containerfiles",
                        "body": "Remove previous \"dev\"/rhel7 Dockerfiles, and replace them with an equivalent `Containerfile` based on ubi9. The `Containerfile` uses the suffix `.ubi` as opposed to a specific UBI/RHEL version number to support future upgrades to ubi10 and beyond. The Containerfile uses UBI images from `registry.access.redhat.com`, which does not require a pull secret. This makes it easy for any contributor to pick up and build. Created a separate Containerfile for OCP builds (`.ocp`), copying the current contents of `Dockerfile.rhel8`. This `Containerfile` uses images from the OpenShift CI system, which are only available to Red Hat employees. The `.rhel8` suffix was removed because it is a source of confusion for new contributors; OCP is currently based on RHEL9 images, and will switch to ubi10 at some point in the future."
                    },
                    {
                        "id": "415",
                        "type": "pullRequest",
                        "title": "OCPBUILD-174: Always Build with `-mod vendor`",
                        "body": "We no longer need to test if we are building with go modules and the `-mod vendor` mode; always build with `-mod vendor` because vendoring dependencies is an OpenShift standard practice."
                    },
                    {
                        "id": "416",
                        "type": "pullRequest",
                        "title": "OCPBUILD-174: Refresh CONTRIBUTING Guide",
                        "body": "Clean up the README and contributing guide so it is easier to comprehend and provide fuller context to contributors. The original README was a bare document that was mostly untouched after OpenShift 4.0 was released. Most of the contributors at the time were OpenShift veterans and had deep \"head knowledge\" of how OpenShift worked; a fully detailed README or contributor guide was not a top priority. This update provides fuller context to contributors, linking out to the openshift-controller-manager repository and Red Hat's official docs for Builds with BuildConfig. The contributor guide is restructured with a more natural developer progression (set up, clone, compile, test, deploy). Deploy instructions are updated to reference cluster-bot for Red Hat employees, and self- managed install modes for the general public. To encourage cluster-bot use, contributors are encouraged to submit pull requests with only unit tests passing; testing code changes end to end is not required prior to PR submission."
                    }
                ]
            }
        }
    },
    "OpenShift API Server": {
        "epics": {
            "API-1689": {
                "summary": "Create TLS artifacts registry",
                "description": "In order to keep track of existing certs/CA bundles and ensure that they adhere to requirements we need to have a TLS artifact registry setup. The registry would: have a test which automatically collects existing certs/CA bundles from secrets/configmaps/files on disk have a test which collects necessary metedata from them (from cert contents or annotations) ensure that new certs match expected metadata and have necessary annotations on when a new cert is added Ref: API-1622",
                "GITHUB": [
                    {
                        "id": "1977",
                        "type": "pullRequest",
                        "title": "API-1689: features: add ShortCertRotation",
                        "body": "Enhancement proposal:"
                    },
                    {
                        "id": "1763",
                        "type": "pullRequest",
                        "title": "API-1689: TLS registry: add description",
                        "body": "This adds description to certificates managed by cluster-kube-apiserver-operator. These are taken from"
                    }
                ]
            }
        }
    }
}